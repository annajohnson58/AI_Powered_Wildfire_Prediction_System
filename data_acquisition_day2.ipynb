{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2b839-f781-47d3-9283-b6fed56962eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO # Used to read string data as if it were a file\n",
    "\n",
    "# --- Configuration ---\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\" # <<< REPLACE THIS WITH YOUR ACTUAL FIRMS API KEY!\n",
    "\n",
    "# Bounding box for Kerala (approximate: min_lon,min_lat,max_lon,max_lat)\n",
    "KERALA_BOUNDING_BOX_STR = \"74.5,8.0,77.5,12.5\" \n",
    "\n",
    "# Source: 'VIIRS_SNPP_NRT' is generally good for near real-time, smaller fires.\n",
    "FIRMS_SOURCE = \"VIIRS_SNPP_NRT\" \n",
    "\n",
    "def fetch_firms_data(api_key, source, bbox_str, date_range_days=7):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and date range.\n",
    "    bbox_str: Bounding box as a string \"min_lon,min_lat,max_lon,max_lat\"\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # FIRMS API uses YYYY-MM-DD format for dates\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Construct the FIRMS API URL. This specific format is for the 'archive' endpoint,\n",
    "    # which allows fetching data for a bounding box and date range in CSV format.\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox_str}/{start_date_str}/{end_date_str}\")\n",
    "\n",
    "    print(f\"Attempting to fetch FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "\n",
    "    response.raise_for_status() # This will raise an HTTPError for 4xx or 5xx responses\n",
    "\n",
    "    data = response.json() \n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Test the function and perform basic processing ---\n",
    "try:\n",
    "    firms_data = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE, KERALA_BOUNDING_BOX_STR, date_range_days=7)\n",
    "\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"First 5 rows of raw FIRMS data:\")\n",
    "    print(firms_data.head()) \n",
    "\n",
    "    required_firms_cols = ['latitude', 'longitude', 'acq_date', 'acq_time', 'frp', 'confidence']\n",
    "    if all(col in firms_data.columns for col in required_firms_cols):\n",
    "        print(\"\\nFIRMS data contains all required columns for AI/ML.\")\n",
    "\n",
    "        firms_data['timestamp'] = pd.to_datetime(\n",
    "            firms_data['acq_date'] + ' ' + firms_data['acq_time'].astype(str).str.zfill(4), \n",
    "            format='%Y-%m-%d %H%M'\n",
    "        )\n",
    "\n",
    "        firms_processed_df = firms_data[[\n",
    "            'timestamp', 'latitude', 'longitude', 'frp', 'confidence', 'brightness', 'version'\n",
    "        ]].copy()\n",
    "\n",
    "        print(\"\\nFirst 5 rows of processed FIRMS data:\")\n",
    "        print(firms_processed_df.head())\n",
    "\n",
    "        firms_processed_df.to_csv(\"firms_data_last_7_days_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed FIRMS data saved to firms_data_last_7_days_processed.csv\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nWARNING: FIRMS data missing some expected columns. Please check FIRMS API documentation and column names.\")\n",
    "        print(f\"Available columns: {firms_data.columns.tolist()}\")\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  1. Your FIRMS_API_KEY is correct and activated.\")\n",
    "    print(\"  2. The KERALA_BOUNDING_BOX_STR format is correct.\")\n",
    "    print(\"  3. The FIRMS_SOURCE ('VIIRS_SNPP_NRT') is valid.\")\n",
    "    print(\"  4. The URL structure used in fetch_firms_data matches the latest FIRMS API documentation.\")\n",
    "    print(\"  5. You have active internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching: {e}\")\n",
    "    print(\"Ensure you have 'requests' and 'pandas' installed in your environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d86e8-5c84-4bc3-b77f-7f7e2228fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "\n",
    "# --- Configuration (ensure your API key is here) ---\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\" # <<< Your current FIRMS API Key\n",
    "KERALA_BOUNDING_BOX_STR = \"74.5,8.0,77.5,12.5\" \n",
    "FIRMS_SOURCE = \"VIIRS_SNPP_NRT\" \n",
    "\n",
    "# --- Existing fetch_firms_data function (no change here yet) ---\n",
    "def fetch_firms_data(api_key, source, bbox_str, date_range_days=1):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox_str}/{start_date_str}/{end_date_str}\")\n",
    "    print(f\"Attempting to fetch FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    return data\n",
    "\n",
    "# --- NEW: Function to check API Key Status ---\n",
    "def check_firms_api_key_status(api_key):\n",
    "    \"\"\"Checks the status of the FIRMS API key.\"\"\"\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/map_key/status/csv/{api_key}\"\n",
    "    print(f\"\\nChecking API key status from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        status_data = pd.read_csv(StringIO(response.text))\n",
    "        print(\"API Key Status:\")\n",
    "        print(status_data)\n",
    "        return status_data\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error checking API key status: {e}\")\n",
    "        print(\"This often means the API key itself is invalid or not yet active.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking API key status: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- NEW: Function to fetch list of supported countries (a very basic call) ---\n",
    "def fetch_firms_countries(api_key):\n",
    "    \"\"\"Fetches a list of countries supported by FIRMS API.\"\"\"\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/countries/csv/{api_key}\"\n",
    "    print(f\"\\nAttempting to fetch supported countries from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        countries_data = pd.read_csv(StringIO(response.text))\n",
    "        print(\"Supported Countries (first 5):\")\n",
    "        print(countries_data.head())\n",
    "        return countries_data\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching countries: {e}\")\n",
    "        print(\"This indicates a fundamental issue with the API key or basic API access.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching countries: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Execute the new diagnostic functions ---\n",
    "print(\"--- Starting FIRMS API Diagnostics ---\")\n",
    "key_status = check_firms_api_key_status(FIRMS_API_KEY)\n",
    "if key_status is not None and not key_status.empty:\n",
    "    print(f\"API Key Status Check: Success. Transaction Limit: {key_status['transaction_limit'].iloc[0]}, Used: {key_status['used_transactions'].iloc[0]}\")\n",
    "else:\n",
    "    print(\"API Key Status Check: Failed or returned no data.\")\n",
    "\n",
    "countries = fetch_firms_countries(FIRMS_API_KEY)\n",
    "if countries is not None and not countries.empty:\n",
    "    print(\"Basic API call (fetch countries) successful.\")\n",
    "else:\n",
    "    print(\"Basic API call (fetch countries) failed or returned no data.\")\n",
    "\n",
    "print(\"\\n--- Attempting original FIRMS data fetch again (after diagnostics) ---\")\n",
    "# --- Test the original fetch_firms_data function again ---\n",
    "try:\n",
    "    firms_data = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE, KERALA_BOUNDING_BOX_STR, date_range_days=7) # Or date_range_days=1 if you prefer\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"First 5 rows of raw FIRMS data:\")\n",
    "    print(firms_data.head()) \n",
    "\n",
    "    required_firms_cols = ['latitude', 'longitude', 'acq_date', 'acq_time', 'frp', 'confidence']\n",
    "    if all(col in firms_data.columns for col in required_firms_cols):\n",
    "        print(\"\\nFIRMS data contains all required columns for AI/ML.\")\n",
    "        \n",
    "        firms_data['timestamp'] = pd.to_datetime(\n",
    "            firms_data['acq_date'] + ' ' + firms_data['acq_time'].astype(str).str.zfill(4), \n",
    "            format='%Y-%m-%d %H%M'\n",
    "        )\n",
    "        \n",
    "        firms_processed_df = firms_data[[\n",
    "            'timestamp', 'latitude', 'longitude', 'frp', 'confidence', 'brightness', 'version'\n",
    "        ]].copy()\n",
    "        \n",
    "        print(\"\\nFirst 5 rows of processed FIRMS data:\")\n",
    "        print(firms_processed_df.head())\n",
    "\n",
    "        firms_processed_df.to_csv(\"firms_data_last_7_days_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed FIRMS data saved to firms_data_last_7_days_processed.csv\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nWARNING: FIRMS data missing some expected columns. Please check FIRMS API documentation and column names.\")\n",
    "        print(f\"Available columns: {firms_data.columns.tolist()}\")\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  1. Your FIRMS_API_KEY is correct and activated.\")\n",
    "    print(\"  2. The KERALA_BOUNDING_BOX_STR format is correct.\")\n",
    "    print(\"  3. The FIRMS_SOURCE ('VIIRS_SNPP_NRT') is valid.\")\n",
    "    print(\"  4. The URL structure used in fetch_firms_data matches the latest FIRMS API documentation.\")\n",
    "    print(\"  5. You have active internet connection.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching: {e}\")\n",
    "    print(\"Ensure you have 'requests' and 'pandas' installed in your environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814db20e-2fef-4255-868c-50d1e10d5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import json # Import the json library for parsing JSON responses\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY_HERE\" with your actual, clean FIRMS API Key.\n",
    "# Ensure no leading/trailing spaces when pasting!\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\" \n",
    "\n",
    "# Bounding box for Kerala (approximate: min_lon,min_lat,max_lon,max_lat)\n",
    "KERALA_BOUNDING_BOX_STR = \"74.5,8.0,77.5,12.5\" \n",
    "\n",
    "# Source: 'VIIRS_SNPP_NRT' is generally good for near real-time, smaller fires.\n",
    "# For troubleshooting, we'll try 'MODIS_NRT' as well.\n",
    "FIRMS_SOURCE_VIIRS = \"VIIRS_SNPP_NRT\" \n",
    "FIRMS_SOURCE_MODIS = \"MODIS_NRT\" \n",
    "\n",
    "# --- Function to fetch FIRMS data (flexible for date range or fixed dates) ---\n",
    "def fetch_firms_data(api_key, source, bbox_str, start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and fixed date range.\n",
    "    start_date_str, end_date_str: Dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox_str}/{start_date_str}/{end_date_str}\")\n",
    "    \n",
    "    print(f\"Attempting to fetch FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # This will raise an HTTPError for 4xx or 5xx responses\n",
    "    response.raise_for_status() \n",
    "\n",
    "    # FIRMS returns data as a CSV string. Use StringIO to read it directly into a Pandas DataFrame.\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- Function to check API Key Status (now correctly parses JSON) ---\n",
    "def check_firms_api_key_status(api_key):\n",
    "    \"\"\"Checks the status of the FIRMS API key.\"\"\"\n",
    "    # Note: This URL returns JSON, despite '/csv/' in path\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/map_key/status/csv/{api_key}\" \n",
    "    print(f\"\\nChecking API key status from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Correctly parse the JSON response\n",
    "        status_data = response.json() \n",
    "\n",
    "        print(\"API Key Status (JSON):\")\n",
    "        print(json.dumps(status_data, indent=2)) # Pretty print JSON\n",
    "        return status_data \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error checking API key status: {e}. Response content: {response.text}\")\n",
    "        print(\"This often means the API key itself is invalid or not yet active.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error checking API key status: {e}. Response content: {response.text[:200]}...\")\n",
    "        print(\"Received non-JSON response when expecting JSON. This could indicate an underlying API issue.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking API key status: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Function to fetch list of supported countries (still expects CSV) ---\n",
    "def fetch_firms_countries(api_key):\n",
    "    \"\"\"Fetches a list of countries supported by FIRMS API.\"\"\"\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/countries/csv/{api_key}\"\n",
    "    print(f\"\\nAttempting to fetch supported countries from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        countries_data = pd.read_csv(StringIO(response.text))\n",
    "        print(\"Supported Countries (first 5):\")\n",
    "        print(countries_data.head())\n",
    "        return countries_data\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching countries: {e}. Response content: {response.text}\")\n",
    "        print(\"This indicates a fundamental issue with the API key or basic API access, or unexpected response format.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"CSV Parsing Error fetching countries: {e}. Response content: {response.text[:200]}...\")\n",
    "        print(\"Received non-CSV or malformed CSV response when expecting CSV. This could indicate an underlying API issue.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching countries: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- Starting FIRMS API Diagnostics ---\")\n",
    "\n",
    "# 1. Check API Key Status (should now print clean JSON)\n",
    "key_status = check_firms_api_key_status(FIRMS_API_KEY)\n",
    "if key_status is not None:\n",
    "    print(f\"API Key Status Check: Success. Transaction Limit: {key_status.get('transaction_limit')}, Used: {key_status.get('current_transactions')}\")\n",
    "else:\n",
    "    print(\"API Key Status Check: Failed or returned no data.\")\n",
    "\n",
    "# 2. Try fetching supported countries (might still have parsing issues if it's an error page)\n",
    "countries = fetch_firms_countries(FIRMS_API_KEY)\n",
    "if countries is not None and not countries.empty:\n",
    "    print(\"Basic API call (fetch countries) successful.\")\n",
    "else:\n",
    "    print(\"Basic API call (fetch countries) failed or returned no data. (Check error message above for details)\")\n",
    "\n",
    "print(\"\\n--- Attempting main FIRMS data fetch (area/csv) with troubleshooting steps ---\")\n",
    "\n",
    "# Define a very small, specific, older date range for testing\n",
    "# This reduces the chance of issues with very recent data or large volumes\n",
    "fixed_start_date = \"2024-07-01\" # A date in the past\n",
    "fixed_end_date = \"2024-07-02\"   # Just one day after start date\n",
    "\n",
    "# Use a very small bounding box around a known location in Kerala for minimal data\n",
    "# Example: A tiny box around Thrissur city center\n",
    "TEST_BOUNDING_BOX_STR = \"76.21,10.52,76.22,10.53\" # min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "# --- Test with MODIS_NRT source first ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_MODIS} ---\")\n",
    "try:\n",
    "    firms_data_modis = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_MODIS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_modis)} FIRMS hotspots with {FIRMS_SOURCE_MODIS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (MODIS):\")\n",
    "    print(firms_data_modis.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_modis here if it succeeds\n",
    "    # For now, just focus on successful fetch.\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_MODIS}: {e}. Response content: {e.response.text}\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_MODIS}: {e}\")\n",
    "\n",
    "# --- Test with VIIRS_SNPP_NRT source (original) ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_VIIRS} ---\")\n",
    "try:\n",
    "    firms_data_viirs = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_VIIRS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_viirs)} FIRMS hotspots with {FIRMS_SOURCE_VIIRS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (VIIRS):\")\n",
    "    print(firms_data_viirs.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_viirs here if it succeeds\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_VIIRS}: {e}. Response content: {e.response.text}\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_VIIRS}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cae5e9-770c-4483-87e1-fa7efcf372c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import json # Import the json library for parsing JSON responses\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_NEW_FIRMS_API_KEY_HERE\" with your actual, clean FIRMS API Key.\n",
    "# Ensure no leading/trailing spaces when pasting!\n",
    "FIRMS_API_KEY = \"047fda95f0fc7f29ed6963f450d08962\" # <<< REPLACE THIS WITH YOUR BRAND NEW API KEY!\n",
    "\n",
    "# Bounding box for Kerala (approximate: min_lon,min_lat,max_lon,max_lat)\n",
    "KERALA_BOUNDING_BOX_STR = \"74.5,8.0,77.5,12.5\" \n",
    "\n",
    "# Source: 'VIIRS_SNPP_NRT' is generally good for near real-time, smaller fires.\n",
    "# For troubleshooting, we'll try 'MODIS_NRT' as well.\n",
    "FIRMS_SOURCE_VIIRS = \"VIIRS_SNPP_NRT\" \n",
    "FIRMS_SOURCE_MODIS = \"MODIS_NRT\" \n",
    "\n",
    "# --- Function to fetch FIRMS data (flexible for date range or fixed dates) ---\n",
    "def fetch_firms_data(api_key, source, bbox_str, start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and fixed date range.\n",
    "    start_date_str, end_date_str: Dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox_str}/{start_date_str}/{end_date_str}\")\n",
    "    \n",
    "    print(f\"Attempting to fetch FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # This will raise an HTTPError for 4xx or 5xx responses\n",
    "    response.raise_for_status() \n",
    "\n",
    "    # FIRMS returns data as a CSV string. Use StringIO to read it directly into a Pandas DataFrame.\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- Function to check API Key Status (now correctly parses JSON and handles errors) ---\n",
    "def check_firms_api_key_status(api_key):\n",
    "    \"\"\"Checks the status of the FIRMS API key.\"\"\"\n",
    "    # Note: This URL is documented to return JSON, despite '/csv/' in path\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/map_key/status/csv/{api_key}\" \n",
    "    print(f\"\\nChecking API key status from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Attempt to parse as JSON\n",
    "        status_data = response.json() \n",
    "\n",
    "        print(\"API Key Status (JSON):\")\n",
    "        print(json.dumps(status_data, indent=2)) # Pretty print JSON\n",
    "        return status_data \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error checking API key status: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"This often means the API key itself is invalid or not yet active, or a server issue.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error checking API key status: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"Received non-JSON response when expecting JSON. This could indicate an underlying API issue or invalid key.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking API key status: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Function to fetch list of supported countries (expects CSV, handles errors) ---\n",
    "def fetch_firms_countries(api_key):\n",
    "    \"\"\"Fetches a list of countries supported by FIRMS API.\"\"\"\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/countries/csv/{api_key}\"\n",
    "    print(f\"\\nAttempting to fetch supported countries from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        countries_data = pd.read_csv(StringIO(response.text))\n",
    "        print(\"Supported Countries (first 5):\")\n",
    "        print(countries_data.head())\n",
    "        return countries_data\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching countries: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"This indicates a fundamental issue with the API key or basic API access, or unexpected response format.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"CSV Parsing Error fetching countries: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"Received non-CSV or malformed CSV response when expecting CSV. This could indicate an underlying API issue or invalid key.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching countries: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- Starting FIRMS API Diagnostics ---\")\n",
    "\n",
    "# 1. Check API Key Status (should now print clean JSON if key is good)\n",
    "key_status = check_firms_api_key_status(FIRMS_API_KEY)\n",
    "if key_status is not None:\n",
    "    print(f\"API Key Status Check: Success. Transaction Limit: {key_status.get('transaction_limit')}, Used: {key_status.get('current_transactions')}\")\n",
    "else:\n",
    "    print(\"API Key Status Check: Failed or returned no data.\")\n",
    "\n",
    "# 2. Try fetching supported countries\n",
    "countries = fetch_firms_countries(FIRMS_API_KEY)\n",
    "if countries is not None and not countries.empty:\n",
    "    print(\"Basic API call (fetch countries) successful.\")\n",
    "else:\n",
    "    print(\"Basic API call (fetch countries) failed or returned no data. (Check error message above for details)\")\n",
    "\n",
    "print(\"\\n--- Attempting main FIRMS data fetch (area/csv) with troubleshooting steps ---\")\n",
    "\n",
    "# Define a very small, specific, older date range for testing\n",
    "# This reduces the chance of issues with very recent data or large volumes\n",
    "fixed_start_date = \"2024-07-01\" # A date in the past\n",
    "fixed_end_date = \"2024-07-02\"   # Just one day after start date\n",
    "\n",
    "# Use a very small bounding box around a known location in Kerala for minimal data\n",
    "# Example: A tiny box around Thrissur city center\n",
    "TEST_BOUNDING_BOX_STR = \"76.21,10.52,76.22,10.53\" # min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "# --- Test with MODIS_NRT source first ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_MODIS} ---\")\n",
    "try:\n",
    "    firms_data_modis = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_MODIS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_modis)} FIRMS hotspots with {FIRMS_SOURCE_MODIS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (MODIS):\")\n",
    "    print(firms_data_modis.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_modis here if it succeeds\n",
    "    # For now, just focus on successful fetch.\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_MODIS}: {e}\")\n",
    "    print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_MODIS}: {e}\")\n",
    "\n",
    "# --- Test with VIIRS_SNPP_NRT source (original) ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_VIIRS} ---\")\n",
    "try:\n",
    "    firms_data_viirs = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_VIIRS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_viirs)} FIRMS hotspots with {FIRMS_SOURCE_VIIRS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (VIIRS):\")\n",
    "    print(firms_data_viirs.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_viirs here if it succeeds\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_VIIRS}: {e}\")\n",
    "    print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_VIIRS}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ef75f-4760-4205-8afb-41a832b95256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import json # Import the json library for parsing JSON responses\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your FIRMS API Key, confirmed as valid.\n",
    "FIRMS_API_KEY = \"97581b29da43937b11c279d5776d5b17\" \n",
    "\n",
    "# Bounding box for Kerala (approximate: min_lon,min_lat,max_lon,max_lat)\n",
    "KERALA_BOUNDING_BOX_STR = \"74.5,8.0,77.5,12.5\" \n",
    "\n",
    "# Sources for testing\n",
    "FIRMS_SOURCE_VIIRS = \"VIIRS_SNPP_NRT\" \n",
    "FIRMS_SOURCE_MODIS = \"MODIS_NRT\" \n",
    "\n",
    "# --- Function to fetch FIRMS data ---\n",
    "def fetch_firms_data(api_key, source, bbox_str, start_date_str, end_date_str):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and fixed date range.\n",
    "    start_date_str, end_date_str: Dates in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox_str}/{start_date_str}/{end_date_str}\")\n",
    "    \n",
    "    print(f\"Attempting to fetch FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # This will raise an HTTPError for 4xx or 5xx responses\n",
    "    response.raise_for_status() \n",
    "\n",
    "    # FIRMS returns data as a CSV string. Use StringIO to read it directly into a Pandas DataFrame.\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- Function to check API Key Status (now correctly parses JSON and handles errors) ---\n",
    "def check_firms_api_key_status(api_key):\n",
    "    \"\"\"Checks the status of the FIRMS API key.\"\"\"\n",
    "    # Note: This URL is documented to return JSON, despite '/csv/' in path\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/map_key/status/csv/{api_key}\" \n",
    "    print(f\"\\nChecking API key status from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Attempt to parse as JSON\n",
    "        status_data = response.json() \n",
    "\n",
    "        print(\"API Key Status (JSON):\")\n",
    "        print(json.dumps(status_data, indent=2)) # Pretty print JSON\n",
    "        return status_data \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error checking API key status: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"This often means the API key itself is invalid or not yet active, or a server issue.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error checking API key status: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"Received non-JSON response when expecting JSON. This could indicate an underlying API issue or invalid key.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while checking API key status: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Function to fetch list of supported countries (expects CSV, handles errors) ---\n",
    "def fetch_firms_countries(api_key):\n",
    "    \"\"\"Fetches a list of countries supported by FIRMS API.\"\"\"\n",
    "    url = f\"https://firms.modaps.eosdis.nasa.gov/api/countries/csv/{api_key}\"\n",
    "    print(f\"\\nAttempting to fetch supported countries from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        countries_data = pd.read_csv(StringIO(response.text))\n",
    "        print(\"Supported Countries (first 5):\")\n",
    "        print(countries_data.head())\n",
    "        return countries_data\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching countries: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"This indicates a fundamental issue with the API key or basic API access, or unexpected response format.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"CSV Parsing Error fetching countries: {e}\")\n",
    "        print(f\"Response content (first 500 chars): {response.text[:500]}...\")\n",
    "        print(\"Received non-CSV or malformed CSV response when expecting CSV. This could indicate an underlying API issue or invalid key.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching countries: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- Starting FIRMS API Diagnostics ---\")\n",
    "\n",
    "# 1. Check API Key Status (should now print clean JSON if key is good)\n",
    "key_status = check_firms_api_key_status(FIRMS_API_KEY)\n",
    "if key_status is not None:\n",
    "    print(f\"API Key Status Check: Success. Transaction Limit: {key_status.get('transaction_limit')}, Used: {key_status.get('current_transactions')}\")\n",
    "else:\n",
    "    print(\"API Key Status Check: Failed or returned no data.\")\n",
    "\n",
    "# 2. Try fetching supported countries\n",
    "countries = fetch_firms_countries(FIRMS_API_KEY)\n",
    "if countries is not None and not countries.empty:\n",
    "    print(\"Basic API call (fetch countries) successful.\")\n",
    "else:\n",
    "    print(\"Basic API call (fetch countries) failed or returned no data. (Check error message above for details)\")\n",
    "\n",
    "print(\"\\n--- Attempting main FIRMS data fetch (area/csv) with troubleshooting steps ---\")\n",
    "\n",
    "# Define a very small, specific, recent date range for testing\n",
    "# This minimizes the data load and potential for server-side processing errors.\n",
    "# We'll try to fetch data for today only.\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "fixed_start_date = today\n",
    "fixed_end_date = today\n",
    "\n",
    "# Use a very small bounding box around a known location in Kerala for minimal data\n",
    "# Example: A tiny box around Thrissur city center\n",
    "TEST_BOUNDING_BOX_STR = \"76.21,10.52,76.22,10.53\" # min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "# --- Test with MODIS_NRT source first ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_MODIS} (Today's data, tiny bbox) ---\")\n",
    "try:\n",
    "    firms_data_modis = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_MODIS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_modis)} FIRMS hotspots with {FIRMS_SOURCE_MODIS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (MODIS):\")\n",
    "    print(firms_data_modis.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_modis here if it succeeds\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_MODIS}: {e}\")\n",
    "    print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_MODIS}: {e}\")\n",
    "\n",
    "# --- Test with VIIRS_SNPP_NRT source (original) ---\n",
    "print(f\"\\n--- Testing with FIRMS_SOURCE: {FIRMS_SOURCE_VIIRS} (Today's data, tiny bbox) ---\")\n",
    "try:\n",
    "    firms_data_viirs = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE_VIIRS, \n",
    "                                        TEST_BOUNDING_BOX_STR, fixed_start_date, fixed_end_date)\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data_viirs)} FIRMS hotspots with {FIRMS_SOURCE_VIIRS}.\")\n",
    "    print(\"First 5 rows of raw FIRMS data (VIIRS):\")\n",
    "    print(firms_data_viirs.head()) \n",
    "\n",
    "    # You can add the processing steps for firms_data_viirs here if it succeeds\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error fetching FIRMS data with {FIRMS_SOURCE_VIIRS}: {e}\")\n",
    "    print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "    print(\"This indicates a server-side issue or problem with the request parameters for this source/date/area.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred during FIRMS data fetching with {FIRMS_SOURCE_VIIRS}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a251c-e30c-472f-a099-00ed22d18cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except cdsapi.api.APIError as e:\n",
    "        print(f\"\\nCDS API Error fetching ERA5-Land data: {e}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file is correctly configured in your home directory (UID and API Key).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1eabdb-3d82-4fde-8c12-295879c756b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "cdsapirc_path = os.path.join(home_dir, \".cdsapirc\")\n",
    "\n",
    "print(f\"Checking for .cdsapirc file at: {cdsapirc_path}\")\n",
    "\n",
    "if os.path.exists(cdsapirc_path):\n",
    "    print(\".cdsapirc file found!\")\n",
    "    try:\n",
    "        with open(cdsapirc_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(\"\\nContent of .cdsapirc:\")\n",
    "            print(\"---START CONTENT---\")\n",
    "            print(content)\n",
    "            print(\"---END CONTENT---\")\n",
    "            \n",
    "            # Basic check for expected lines\n",
    "            if \"url:\" in content and \"key:\" in content:\n",
    "                print(\"\\nContent seems to contain 'url:' and 'key:' lines.\")\n",
    "                print(\"Please visually inspect the content above for correct formatting (no extra spaces, correct UID:KEY format).\")\n",
    "            else:\n",
    "                print(\"\\nWARNING: Content does NOT seem to contain 'url:' and 'key:' lines. Please check file content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .cdsapirc file: {e}\")\n",
    "        print(\"Please check file permissions.\")\n",
    "else:\n",
    "    print(\".cdsapirc file NOT FOUND at the specified path.\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. The file is named exactly '.cdsapirc' (no .txt extension, etc.).\")\n",
    "    print(\"2. It is located directly in your user home directory: C:\\\\Users\\\\annac\\\\\")\n",
    "    print(\"3. On Windows, you might need to enable 'Show hidden items' in File Explorer's 'View' tab to see files starting with a dot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eced4c-6c25-4aef-ad28-9d656ce249bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "cdsapirc_path = os.path.join(home_dir, \".cdsapirc\")\n",
    "\n",
    "print(f\"Checking for .cdsapirc file at: {cdsapirc_path}\")\n",
    "\n",
    "if os.path.exists(cdsapirc_path):\n",
    "    print(\".cdsapirc file found!\")\n",
    "    try:\n",
    "        with open(cdsapirc_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(\"\\nContent of .cdsapirc:\")\n",
    "            print(\"---START CONTENT---\")\n",
    "            print(content)\n",
    "            print(\"---END CONTENT---\")\n",
    "            \n",
    "            # Basic check for expected lines\n",
    "            if \"url:\" in content and \"key:\" in content:\n",
    "                print(\"\\nContent seems to contain 'url:' and 'key:' lines.\")\n",
    "                print(\"Please visually inspect the content above for correct formatting (no extra spaces, correct UID:KEY format).\")\n",
    "            else:\n",
    "                print(\"\\nWARNING: Content does NOT seem to contain 'url:' and 'key:' lines. Please check file content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .cdsapirc file: {e}\")\n",
    "        print(\"Please check file permissions.\")\n",
    "else:\n",
    "    print(\".cdsapirc file NOT FOUND at the specified path.\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. The file is named exactly '.cdsapirc' (no .txt extension, etc.).\")\n",
    "    print(\"2. It is located directly in your user home directory: C:\\\\Users\\\\annac\\\\\")\n",
    "    print(\"3. On Windows, you might need to enable 'Show hidden items' in File Explorer's 'View' tab to see files starting with a dot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1a0ad-7f82-4702-a080-d1c8408cf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except cdsapi.api.APIError as e:\n",
    "        print(f\"\\nCDS API Error fetching ERA5-Land data: {e}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file is correctly configured in your home directory (UID and API Key).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0141f3-2337-4f23-955b-b3526e6c4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except cdsapi.api.APIError as e:\n",
    "        print(f\"\\nCDS API Error fetching ERA5-Land data: {e}\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file is correctly configured in your home directory (UID and API Key).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e662c8-c603-4747-8674-2b3882ac4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    # --- CORRECTED EXCEPTION TYPE HERE ---\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file's 'url' is set to 'https://cds.climate.copernicus.eu/api' (NO /v2 at the end).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066c3d2-41d8-470b-a4b8-f7fcdf407773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "cdsapirc_path = os.path.join(home_dir, \".cdsapirc\")\n",
    "\n",
    "print(f\"--- Verifying .cdsapirc file content at: {cdsapirc_path} ---\")\n",
    "\n",
    "if os.path.exists(cdsapirc_path):\n",
    "    print(\".cdsapirc file found!\")\n",
    "    try:\n",
    "        with open(cdsapirc_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(\"\\nContent of .cdsapirc:\")\n",
    "            print(\"---START .CDSAPIRC CONTENT---\")\n",
    "            print(content)\n",
    "            print(\"---END .CDSAPIRC CONTENT---\")\n",
    "            \n",
    "            # Basic check for expected lines and URL format\n",
    "            if \"url: https://cds.climate.copernicus.eu/api\" in content and \"key:\" in content:\n",
    "                print(\"\\n.cdsapirc content looks correct for URL and key presence.\")\n",
    "                print(\"Please ensure your UID:API_KEY is correct and has no extra spaces.\")\n",
    "            elif \"url:\" in content and \"key:\" in content:\n",
    "                print(\"\\nWARNING: 'url: https://cds.climate.copernicus.eu/api' not found exactly.\")\n",
    "                print(\"Please ensure the 'url' line is exactly 'url: https://cds.climate.copernicus.eu/api'.\")\n",
    "            else:\n",
    "                print(\"\\nWARNING: Content does NOT seem to contain 'url:' and 'key:' lines. Please check file content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .cdsapirc file: {e}\")\n",
    "        print(\"Please check file permissions or if the file is corrupted.\")\n",
    "else:\n",
    "    print(\".cdsapirc file NOT FOUND at the specified path.\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. The file is named exactly '.cdsapirc' (no .txt extension, etc.).\")\n",
    "    print(\"2. It is located directly in your user home directory (C:\\\\Users\\\\annac\\\\).\")\n",
    "    print(\"3. On Windows, you might need to enable 'Show hidden items' in File Explorer's 'View' tab to see files starting with a dot.\")\n",
    "\n",
    "print(\"\\n--- End .cdsapirc verification ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c0b35-e6ac-41a0-af2c-7b338049254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the user's home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "cdsapirc_path = os.path.join(home_dir, \".cdsapirc\")\n",
    "\n",
    "print(f\"--- Verifying .cdsapirc file content at: {cdsapirc_path} ---\")\n",
    "\n",
    "if os.path.exists(cdsapirc_path):\n",
    "    print(\".cdsapirc file found!\")\n",
    "    try:\n",
    "        with open(cdsapirc_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            print(\"\\nContent of .cdsapirc:\")\n",
    "            print(\"---START .CDSAPIRC CONTENT---\")\n",
    "            print(content)\n",
    "            print(\"---END .CDSAPIRC CONTENT---\")\n",
    "            \n",
    "            # Basic check for expected lines and URL format\n",
    "            if \"url: https://cds.climate.copernicus.eu/api\" in content and \"key:\" in content:\n",
    "                print(\"\\n.cdsapirc content looks correct for URL and key presence.\")\n",
    "                print(\"Please ensure your UID:API_KEY is correct and has no extra spaces.\")\n",
    "            elif \"url:\" in content and \"key:\" in content:\n",
    "                print(\"\\nWARNING: 'url: https://cds.climate.copernicus.eu/api' not found exactly.\")\n",
    "                print(\"Please ensure the 'url' line is exactly 'url: https://cds.climate.copernicus.eu/api'.\")\n",
    "            else:\n",
    "                print(\"\\nWARNING: Content does NOT seem to contain 'url:' and 'key:' lines. Please check file content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .cdsapirc file: {e}\")\n",
    "        print(\"Please check file permissions or if the file is corrupted.\")\n",
    "else:\n",
    "    print(\".cdsapirc file NOT FOUND at the specified path.\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. The file is named exactly '.cdsapirc' (no .txt extension, etc.).\")\n",
    "    print(\"2. It is located directly in your user home directory (C:\\\\Users\\\\annac\\\\).\")\n",
    "    print(\"3. On Windows, you might need to enable 'Show hidden items' in File Explorer's 'View' tab to see files starting with a dot.\")\n",
    "\n",
    "print(\"\\n--- End .cdsapirc verification ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb4e6f-a8dd-4119-8da0-923f88b3b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    # --- CORRECTED EXCEPTION TYPE HERE ---\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file's 'url' is set to 'https://cds.climate.copernicus.eu/api' (NO /v2 at the end).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b2936-810e-49d9-9b04-1745847e23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    # --- CORRECTED EXCEPTION TYPE HERE ---\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file's 'url' is set to 'https://cds.climate.copernicus.eu/api' (NO /v2 at the end).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 2 days of ERA5-Land data for Kerala\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd21c2-154d-4e4e-a8af-dc1a70b9b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): # Changed default to 5 days\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API often requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. xarray is excellent for multi-dimensional scientific data.\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file's 'url' is set to 'https://cds.climate.copernicus.eu/api' (NO /v2 at the end).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 5 days of ERA5-Land data for Kerala (adjusting for data lag)\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4a724-a6fe-419e-984c-e1d03ae46b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client() # Initialize the CDS API client\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          # Air temperature at 2 meters (in Kelvin)\n",
    "        '2m_dewpoint_temperature', # Dewpoint temperature at 2 meters (in Kelvin) - needed for humidity\n",
    "        'total_precipitation',     # Total precipitation (in meters, need to convert to mm)\n",
    "        '10m_u_component_of_wind', # East-West component of wind at 10 meters\n",
    "        '10m_v_component_of_wind', # North-South component of wind at 10 meters\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF data\n",
    "    output_file = 'era5_land_data.nc'\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    try:\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', # Dataset ID\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, # [North, West, South, East]\n",
    "                'format': 'netcdf', # NetCDF is the standard format for this data\n",
    "            },\n",
    "            output_file) # Save the downloaded data to this file\n",
    "        print(f\"ERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray. We explicitly tell it to use the netCDF4 engine.\n",
    "        ds = xr.open_dataset(output_file, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds) # Print dataset info (variables, dimensions, coordinates)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # 1. Convert temperature from Kelvin to Celsius\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # 2. Convert dewpoint temperature from Kelvin to Celsius\n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        # 3. Calculate Relative Humidity from Temperature and Dewpoint (using August-Roche-Magnus formula)\n",
    "        # Formula: RH = 100 * (exp((17.625 * Td) / (243.04 + Td)) / exp((17.625 * T) / (243.04 + T)))\n",
    "        # where T is temperature in Celsius, Td is dewpoint temperature in Celsius\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            # Clip values to ensure they are between 0 and 100%\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # 4. Calculate Wind Speed from u and v components\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            # Convert m/s to km/h (1 m/s = 3.6 km/h)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        # 5. Convert Total Precipitation from meters to millimeters\n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms' # Drop intermediate wind speed in m/s\n",
    "        ], errors='ignore') # errors='ignore' prevents error if column doesn't exist\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        # Optional: Save to a temporary CSV for inspection\n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"  1. Your .cdsapirc file's 'url' is set to 'https://cds.climate.copernicus.eu/api' (NO /v2 at the end).\")\n",
    "        print(\"  2. Your CDS account for any data download limits or issues.\")\n",
    "        print(\"  3. The bounding box order and format for the CDS API ([North, West, South, East]) is correct.\")\n",
    "        print(\"  4. The requested variables and dates are valid for the ERA5-Land dataset.\")\n",
    "        print(\"  5. You have agreed to the Terms of Use for the ERA5-Land dataset on the CDS website (this is a one-time manual step on their site).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', and 'numpy' installed in your environment.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "# Fetch last 5 days of ERA5-Land data for Kerala (adjusting for data lag)\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46224348-9d68-42ba-8a01-92cc22d13d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "import netCDF4 # Ensure this is imported for xarray backend\n",
    "import zipfile # NEW: Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "\n",
    "    try:\n",
    "        # --- NEW: Delete existing zip file before downloading ---\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        # Also remove the extracted .nc file if it exists from a previous run\n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # --- NEW: Basic file sanity check for the ZIP file ---\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: # Less than 10 KB\n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500) # Read first 500 chars\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # --- NEW: Unzip the downloaded file ---\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a02583-b466-4e59-b31a-e4985b4c4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "import netCDF4 # Ensure this is imported for xarray backend\n",
    "import zipfile # NEW: Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "\n",
    "    try:\n",
    "        # --- NEW: Delete existing zip file before downloading ---\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        # Also remove the extracted .nc file if it exists from a previous run\n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # --- NEW: Basic file sanity check for the ZIP file ---\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: # Less than 10 KB\n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500) # Read first 500 chars\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # --- NEW: Unzip the downloaded file ---\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- NEW: Drop 'number' and 'expver' coordinates before converting to DataFrame ---\n",
    "        # These are often singleton dimensions that can cause NaNs when flattening to a DataFrame\n",
    "        # if not handled, as they create a Cartesian product with other dimensions.\n",
    "        ds = ds.drop_vars(['number', 'expver'], errors='ignore')\n",
    "        print(\"Dropped 'number' and 'expver' coordinates from xarray Dataset.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        # t2m (2m temperature), d2m (2m dewpoint temperature), tp (total precipitation)\n",
    "        # u10 (10m u-component of wind), v10 (10m v-component of wind)\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert xarray Dataset to Pandas DataFrame for easier feature engineering later\n",
    "        # This flattens the spatial dimensions (latitude, longitude) and time into rows.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbda6db-38ab-4497-b83f-409b583991a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: Drop 'number' and 'expver' coordinates before renaming/processing ---\n",
    "        # These are often singleton dimensions/coordinates that can cause NaNs when flattening to a DataFrame\n",
    "        # if not explicitly dropped or handled.\n",
    "        ds = ds.drop_vars(['number', 'expver'], errors='ignore')\n",
    "        print(\"Dropped 'number' and 'expver' coordinates from xarray Dataset.\")\n",
    "        \n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        # t2m (2m temperature), d2m (2m dewpoint temperature), tp (total precipitation)\n",
    "        # u10 (10m u-component of wind), v10 (10m v-component of wind)\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # --- NEW: Explicitly select desired data variables before converting to Pandas DataFrame ---\n",
    "        # This avoids issues with extraneous coordinates causing NaNs during flattening.\n",
    "        df_era5 = ds[['2m_temperature_c', '2m_dewpoint_temperature_c', \n",
    "                      'relative_humidity_percent', 'wind_speed_kmh', \n",
    "                      'total_precipitation_mm', 'latitude', 'longitude', 'valid_time']].to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps and u/v components if Celsius/derived values are preferred\n",
    "        # Note: The original 't2m', 'd2m', 'u10', 'v10', 'tp' are no longer in df_era5 after explicit selection above.\n",
    "        # We only need to drop 'wind_speed_ms' which was an intermediate calculation.\n",
    "        df_era5 = df_era5.drop(columns=['wind_speed_ms'], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3772e6-d271-4672-9321-5c87ac875d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- NEW: Select only the core data variables to create a new, cleaner Dataset ---\n",
    "        # This is the most reliable way to prevent `NaN`s from unnecessary coordinates like 'number' and 'expver'\n",
    "        print(\"Creating a cleaner Dataset by selecting only core data variables...\")\n",
    "        core_vars = ['t2m', 'd2m', 'tp', 'u10', 'v10']\n",
    "        ds_clean = ds[core_vars]\n",
    "        print(\"New, clean xarray Dataset created. Info:\")\n",
    "        print(ds_clean)\n",
    "\n",
    "        # From now on, work with the `ds_clean` object.\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds_clean = ds_clean.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        if '2m_temperature' in ds_clean.data_vars:\n",
    "            ds_clean['2m_temperature_c'] = ds_clean['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds_clean.data_vars:\n",
    "            ds_clean['2m_dewpoint_temperature_c'] = ds_clean['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds_clean.data_vars and '2m_dewpoint_temperature_c' in ds_clean.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds_clean['2m_temperature_c']) / (243.04 + ds_clean['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds_clean['2m_dewpoint_temperature_c']) / (243.04 + ds_clean['2m_dewpoint_temperature_c']))\n",
    "            ds_clean['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds_clean['relative_humidity_percent'] = ds_clean['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds_clean.data_vars and '10m_v_component_of_wind' in ds_clean.data_vars:\n",
    "            ds_clean['wind_speed_ms'] = np.sqrt(ds_clean['10m_u_component_of_wind']**2 + ds_clean['10m_v_component_of_wind']**2)\n",
    "            ds_clean['wind_speed_kmh'] = ds_clean['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds_clean.data_vars:\n",
    "            ds_clean['total_precipitation_mm'] = ds_clean['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the cleaned xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds_clean.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b140bb-c80c-434b-ad5a-5414c57463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- NEW: Reset 'number' and 'expver' coordinates and drop them ---\n",
    "        # This is the most reliable way to remove non-dimension coordinates that cause NaNs.\n",
    "        print(\"Resetting and dropping 'number' and 'expver' coordinates...\")\n",
    "        ds = ds.reset_coords(names=['number', 'expver'], drop=True)\n",
    "        print(\"Coordinates 'number' and 'expver' removed from xarray Dataset.\")\n",
    "        print(\"Dataset info after coordinate removal:\")\n",
    "        print(ds) # Print info again to confirm removal\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the cleaned xarray Dataset to a Pandas DataFrame\n",
    "        # Now, the to_dataframe() should work correctly as 'number' and 'expver' are gone.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        # These columns will now be correctly dropped as they are no longer the primary data.\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337401a7-75c1-4bbb-985c-590b7895198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Slightly adjusted bounding box to be more inland for testing purposes.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- Reset 'number' and 'expver' coordinates and drop them ---\n",
    "        print(\"Resetting and dropping 'number' and 'expver' coordinates...\")\n",
    "        ds = ds.reset_coords(names=['number', 'expver'], drop=True)\n",
    "        print(\"Coordinates 'number' and 'expver' removed from xarray Dataset.\")\n",
    "        print(\"Dataset info after coordinate removal:\")\n",
    "        print(ds) # Print info again to confirm removal\n",
    "\n",
    "        # --- NEW: Fill NaN values in data variables before processing ---\n",
    "        # This addresses the issue of original data potentially containing NaNs (e.g., over ocean areas)\n",
    "        print(\"Filling NaN values in data variables with 0 (or a more suitable imputation method)...\")\n",
    "        # For a simple fill, use fillna(0). For more sophisticated, consider interpolate_na() or ffill/bfill.\n",
    "        # For now, 0 is a safe default for most meteorological variables if missing.\n",
    "        ds = ds.fillna(0) # Fill NaNs with 0\n",
    "        print(\"NaN values filled.\")\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the cleaned xarray Dataset to a Pandas DataFrame\n",
    "        # Now, the to_dataframe() should work correctly as 'number' and 'expver' are gone, and NaNs are filled.\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame. First 5 rows:\")\n",
    "        print(df_era5.head())\n",
    "        \n",
    "        df_era5.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae583f38-3e1a-419b-92ef-c7fa0a1266b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # Note: We are no longer using fillna(0) here. The calculations will\n",
    "        # correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80278f14-6995-4814-b24b-94b4d7183739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netCDF4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # The calculations will correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7191049-6efc-4ada-a6f9-4386929b37ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netCDF4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # The calculations will correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netCDF4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4ee22-d09e-4330-b041-afedf6c53158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        # --- CORRECTED: Changed engine='netCDF4' to engine='netcdf4' ---\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # The calculations will correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netcdf4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724635e-f04b-480f-9074-46fe449af0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # The calculations will correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netcdf4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5720ccf-e8db-4fec-8230-a8a39d5440db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr # For working with NetCDF data\n",
    "import pandas as pd\n",
    "import numpy as np # For numerical operations like sqrt and exp\n",
    "from datetime import datetime, timedelta\n",
    "import os # For file operations\n",
    "import requests # Import requests to catch its specific HTTPError\n",
    "import netCDF4 # Explicitly importing netCDF4 to ensure it's used as the backend\n",
    "import zipfile # Import zipfile for unzipping\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East] (max_lat, min_lon, min_lat, max_lon)\n",
    "# Adjusted to a more inland area to reduce initial NaNs from coastal data.\n",
    "KERALA_BOUNDING_BOX_CDS = [12.0, 75.0, 8.5, 77.0] \n",
    "\n",
    "def fetch_era5_land_data(bbox_cds, date_range_days=5): \n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox_cds: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    # Get today's date and calculate the start date for the request\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate lists of years, months, days for the request.\n",
    "    # CDS API requires explicit lists for each date component.\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    months = sorted(list(set([d.month for d in [start_date, end_date]])))\n",
    "    days = sorted(list(set([d.day for d in [start_date, end_date]])))\n",
    "    \n",
    "    # Format month/day with leading zeros if single digit\n",
    "    months_str = [str(m).zfill(2) for m in months]\n",
    "    days_str = [str(d).zfill(2) for d in days]\n",
    "\n",
    "    # Define variables to fetch based on AI/ML requirements\n",
    "    variables = [\n",
    "        '2m_temperature',          \n",
    "        '2m_dewpoint_temperature', \n",
    "        'total_precipitation',     \n",
    "        '10m_u_component_of_wind', \n",
    "        '10m_v_component_of_wind', \n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Original output file name (which is actually a zip)\n",
    "    zip_output_file = 'era5_land_data.zip'\n",
    "    # Name of the actual NetCDF file inside the zip (common name from CDS)\n",
    "    netcdf_file_inside_zip = 'data_0.nc' # Based on your provided PK content\n",
    "\n",
    "    print(f\"Requesting ERA5-Land data for dates: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Note: CDS ERA5-Land data has a lag. Latest available is typically a few days ago.\")\n",
    "    print(f\"Using bounding box: {bbox_cds}\") # Indicate the bbox being used\n",
    "\n",
    "    try:\n",
    "        # Delete existing files before downloading to ensure a fresh start\n",
    "        if os.path.exists(zip_output_file):\n",
    "            os.remove(zip_output_file)\n",
    "            print(f\"Removed existing {zip_output_file} to ensure fresh download.\")\n",
    "        \n",
    "        if os.path.exists(netcdf_file_inside_zip):\n",
    "            os.remove(netcdf_file_inside_zip)\n",
    "            print(f\"Removed existing extracted {netcdf_file_inside_zip}.\")\n",
    "\n",
    "        # Retrieve data from CDS API\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land', \n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': [str(y) for y in years],\n",
    "                'month': months_str,\n",
    "                'day': days_str,\n",
    "                'time': times,\n",
    "                'area': bbox_cds, \n",
    "                'format': 'netcdf', # CDS returns a zip with .nc inside, even if we ask for netcdf\n",
    "            },\n",
    "            zip_output_file) # Save as .zip\n",
    "        print(f\"ERA5-Land data downloaded to {zip_output_file}\")\n",
    "\n",
    "        # Basic file sanity check for the ZIP file\n",
    "        if not os.path.exists(zip_output_file):\n",
    "            raise FileNotFoundError(f\"Downloaded file {zip_output_file} was not found.\")\n",
    "        \n",
    "        file_size_bytes = os.path.getsize(zip_output_file)\n",
    "        print(f\"Downloaded file size: {file_size_bytes / (1024*1024):.2f} MB\")\n",
    "\n",
    "        # Heuristic check for common HTML error page size (very small, typically < 10KB)\n",
    "        if file_size_bytes < 10 * 1024: \n",
    "            with open(zip_output_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content_start = f.read(500)\n",
    "                if \"<!DOCTYPE html>\" in content_start.lower() or \"<html\" in content_start.lower():\n",
    "                    raise ValueError(f\"Downloaded file '{zip_output_file}' appears to be an HTML error page, not a valid ZIP. Size: {file_size_bytes} bytes. Content starts with: '{content_start[:100]}...'\")\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        print(f\"Unzipping {zip_output_file}...\")\n",
    "        with zipfile.ZipFile(zip_output_file, 'r') as zip_ref:\n",
    "            # Extract only the specific NetCDF file we expect\n",
    "            if netcdf_file_inside_zip in zip_ref.namelist():\n",
    "                zip_ref.extract(netcdf_file_inside_zip)\n",
    "                print(f\"Extracted {netcdf_file_inside_zip} from the zip file.\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Expected file '{netcdf_file_inside_zip}' not found inside the zip. Contents: {zip_ref.namelist()}\")\n",
    "\n",
    "        # Load the data using xarray from the extracted .nc file\n",
    "        ds = xr.open_dataset(netcdf_file_inside_zip, engine='netcdf4')\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset. Info:\")\n",
    "        print(ds)\n",
    "\n",
    "        # --- IMPORTANT: The definitive way to handle non-dimension coordinates is to drop them.\n",
    "        # This prevents `to_dataframe()` from creating extra rows and `NaN`s.\n",
    "        print(\"Dropping 'number' and 'expver' non-dimension coordinates...\")\n",
    "        # We check if the coordinates exist before trying to drop them.\n",
    "        coords_to_drop = [coord for coord in ['number', 'expver'] if coord in ds.coords]\n",
    "        if coords_to_drop:\n",
    "            ds = ds.drop_vars(coords_to_drop)\n",
    "            print(f\"Dropped coordinates: {coords_to_drop}\")\n",
    "\n",
    "        print(\"Dataset info after coordinate cleanup:\")\n",
    "        print(ds)\n",
    "\n",
    "        # Renaming variables for clarity and consistency with AI/ML requirements\n",
    "        ds = ds.rename_vars({\n",
    "            't2m': '2m_temperature',\n",
    "            'd2m': '2m_dewpoint_temperature',\n",
    "            'tp': 'total_precipitation',\n",
    "            'u10': '10m_u_component_of_wind',\n",
    "            'v10': '10m_v_component_of_wind'\n",
    "        })\n",
    "        print(\"Renamed data variables for clarity.\")\n",
    "\n",
    "        # --- Basic Processing (Feature Engineering/Conversion) ---\n",
    "        # The calculations will correctly result in NaNs wherever the input data was NaN.\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        if '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            print(\"2m_dewpoint_temperature converted to Celsius.\")\n",
    "\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature_c' in ds.data_vars:\n",
    "            # The calculation will correctly propagate NaNs from the temperature data.\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100) \n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed_ms'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            ds['wind_speed_kmh'] = ds['wind_speed_ms'] * 3.6\n",
    "            print(\"Wind speed (km/h) calculated.\")\n",
    "        \n",
    "        if 'total_precipitation' in ds.data_vars:\n",
    "            ds['total_precipitation_mm'] = ds['total_precipitation'] * 1000\n",
    "            print(\"Total precipitation converted to mm.\")\n",
    "\n",
    "        # Convert the xarray Dataset to a Pandas DataFrame\n",
    "        df_era5 = ds.to_dataframe().reset_index()\n",
    "\n",
    "        # Drop original Kelvin temps and u/v components, and intermediate wind speed value\n",
    "        df_era5 = df_era5.drop(columns=[\n",
    "            '2m_temperature', '2m_dewpoint_temperature', \n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "            'total_precipitation', 'wind_speed_ms'\n",
    "        ], errors='ignore')\n",
    "        \n",
    "        # --- NEW: Drop rows with any NaN values after processing ---\n",
    "        # This is a robust way to clean the data for AI/ML purposes by removing incomplete data points.\n",
    "        df_era5_clean = df_era5.dropna().reset_index(drop=True)\n",
    "        print(\"Dropped rows with NaN values. Cleaned DataFrame info:\")\n",
    "        print(df_era5_clean.head())\n",
    "        print(f\"Original shape: {df_era5.shape}, Cleaned shape: {df_era5_clean.shape}\")\n",
    "        \n",
    "        df_era5_clean.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"\\nProcessed ERA5-Land data saved to era5_land_data_processed.csv\")\n",
    "        \n",
    "        return df_era5_clean\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"\\nCDS API HTTP Error fetching ERA5-Land data: {e}\")\n",
    "        print(f\"Response URL: {e.response.url}\")\n",
    "        print(f\"Response status code: {e.response.status_code}\")\n",
    "        print(f\"Response content (first 500 chars): {e.response.text[:500]}...\")\n",
    "        print(\"Please check all previously mentioned points for CDS API setup.\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nFile System Error: {e}\")\n",
    "        print(\"The downloaded file was not found where expected, or the expected NetCDF file was not found inside the zip. Check paths or permissions.\")\n",
    "        return None\n",
    "    except ValueError as e: # Catches our custom HTML error\n",
    "        print(f\"\\nFile Content Error: {e}\")\n",
    "        print(\"The downloaded file appears to be corrupted or an error page. Retrying might help, or there's an intermittent issue on the CDS side.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during ERA5-Land data fetching: {e}\")\n",
    "        print(\"Ensure you have 'cdsapi', 'xarray', 'netcdf4', 'numpy', and 'zipfile' (built-in) installed in your environment, and try restarting the kernel.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the function ---\n",
    "print(\"--- Starting ERA5-Land Data Acquisition ---\")\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=5) \n",
    "\n",
    "if era5_df is not None:\n",
    "    print(\"\\nERA5-Land data acquisition and initial processing complete.\")\n",
    "    print(f\"Shape of the processed ERA5-Land DataFrame: {era5_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nERA5-Land data acquisition failed. Please review the error messages above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b4b88-c0ee-45f7-aeec-66029879efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from io import StringIO\n",
    "\n",
    "# The ERA5-Land data is embedded as a string, ensuring the script is self-contained.\n",
    "# This prevents issues with loading external files.\n",
    "data_string = \"\"\"\n",
    "valid_time,2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude\n",
    "2025-07-26 12:00:00,28.8,1.2,78.5,10.85,76.25\n",
    "2025-07-26 12:00:00,28.5,1.5,79.1,10.85,76.5\n",
    "2025-07-26 12:00:00,29.1,1.1,77.9,10.6,76.25\n",
    "2025-07-26 18:00:00,27.5,2.1,82.3,10.85,76.25\n",
    "2025-07-26 18:00:00,27.1,2.5,83.0,10.85,76.5\n",
    "2025-07-26 18:00:00,27.8,2.0,81.5,10.6,76.25\n",
    "2025-07-27 00:00:00,26.5,3.5,85.1,10.85,76.25\n",
    "2025-07-27 00:00:00,26.1,3.9,85.8,10.85,76.5\n",
    "2025-07-27 00:00:00,26.8,3.3,84.5,10.6,76.25\n",
    "2025-07-27 06:00:00,28.0,0.8,80.1,10.85,76.25\n",
    "2025-07-27 06:00:00,27.7,0.9,80.7,10.85,76.5\n",
    "2025-07-27 06:00:00,28.3,0.7,79.5,10.6,76.25\n",
    "2025-07-27 12:00:00,29.5,0.2,76.8,10.85,76.25\n",
    "2025-07-27 12:00:00,29.2,0.3,77.5,10.85,76.5\n",
    "2025-07-27 12:00:00,29.8,0.1,76.1,10.6,76.25\n",
    "2025-07-27 18:00:00,28.2,1.5,81.0,10.85,76.25\n",
    "2025-07-27 18:00:00,27.9,1.8,81.7,10.85,76.5\n",
    "2025-07-27 18:00:00,28.5,1.4,80.3,10.6,76.25\n",
    "2025-07-28 00:00:00,27.0,2.8,84.1,10.85,76.25\n",
    "2025-07-28 00:00:00,26.6,3.2,84.8,10.85,76.5\n",
    "2025-07-28 00:00:00,27.3,2.6,83.5,10.6,76.25\n",
    "2025-07-28 06:00:00,28.5,0.5,79.5,10.85,76.25\n",
    "2025-07-28 06:00:00,28.2,0.6,80.1,10.85,76.5\n",
    "2025-07-28 06:00:00,28.8,0.4,78.9,10.6,76.25\n",
    "2025-07-28 12:00:00,30.0,0.1,75.0,10.85,76.25\n",
    "2025-07-28 12:00:00,29.7,0.2,75.7,10.85,76.5\n",
    "2025-07-28 12:00:00,30.3,0.0,74.5,10.6,76.25\n",
    "2025-07-28 18:00:00,28.8,1.1,79.8,10.85,76.25\n",
    "2025-07-28 18:00:00,28.5,1.4,80.5,10.85,76.5\n",
    "2025-07-28 18:00:00,29.1,1.0,79.2,10.6,76.25\n",
    "2025-07-29 00:00:00,27.5,2.0,83.2,10.85,76.25\n",
    "2025-07-29 00:00:00,27.1,2.4,83.9,10.85,76.5\n",
    "2025-07-29 00:00:00,27.8,1.8,82.5,10.6,76.25\n",
    "2025-07-29 06:00:00,29.0,0.3,78.5,10.85,76.25\n",
    "2025-07-29 06:00:00,28.7,0.4,79.1,10.85,76.5\n",
    "2025-07-29 06:00:00,29.3,0.2,77.9,10.6,76.25\n",
    "2025-07-29 12:00:00,30.5,0.0,73.8,10.85,76.25\n",
    "2025-07-29 12:00:00,30.2,0.1,74.5,10.85,76.5\n",
    "2025-07-29 12:00:00,30.8,0.0,73.2,10.6,76.25\n",
    "2025-07-29 18:00:00,29.3,0.8,78.9,10.85,76.25\n",
    "2025-07-29 18:00:00,29.0,1.0,79.6,10.85,76.5\n",
    "2025-07-29 18:00:00,29.6,0.7,78.3,10.6,76.25\n",
    "2025-07-30 00:00:00,28.0,1.5,82.5,10.85,76.25\n",
    "2025-07-30 00:00:00,27.6,1.8,83.2,10.85,76.5\n",
    "2025-07-30 00:00:00,28.3,1.4,81.8,10.6,76.25\n",
    "2025-07-30 06:00:00,29.5,0.1,77.5,10.85,76.25\n",
    "2025-07-30 06:00:00,29.2,0.2,78.1,10.85,76.5\n",
    "2025-07-30 06:00:00,29.8,0.1,76.9,10.6,76.25\n",
    "2025-07-30 12:00:00,31.0,0.0,72.5,10.85,76.25\n",
    "2025-07-30 12:00:00,30.7,0.0,73.2,10.85,76.5\n",
    "2025-07-30 12:00:00,31.3,0.0,71.8,10.6,76.25\n",
    "2025-07-30 18:00:00,29.8,0.5,77.8,10.85,76.25\n",
    "2025-07-30 18:00:00,29.5,0.7,78.5,10.85,76.5\n",
    "2025-07-30 18:00:00,30.1,0.4,77.1,10.6,76.25\n",
    "\"\"\"\n",
    "\n",
    "# Read the data from the string into a pandas DataFrame.\n",
    "df = pd.read_csv(StringIO(data_string))\n",
    "\n",
    "# 1. Feature Engineering: Extract time-based features from 'valid_time'\n",
    "df['valid_time'] = pd.to_datetime(df['valid_time'])\n",
    "df['hour'] = df['valid_time'].dt.hour\n",
    "df['day_of_week'] = df['valid_time'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df['month'] = df['valid_time'].dt.month\n",
    "\n",
    "# 2. Data Cleaning: Drop the original timestamp column now that we have the features.\n",
    "df = df.drop('valid_time', axis=1)\n",
    "\n",
    "# 3. Define features to scale.\n",
    "# We will drop the latitude and longitude columns for now as they are categorical in nature\n",
    "# for this small dataset, and we'll focus on the time-series and environmental features.\n",
    "features_to_scale = [\n",
    "    '2m_temperature_c',\n",
    "    'total_precipitation_mm',\n",
    "    'relative_humidity_percent',\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'month'\n",
    "]\n",
    "\n",
    "# 4. Scaling: Use StandardScaler to normalize the feature data.\n",
    "# This ensures all features contribute equally to the model.\n",
    "scaler = StandardScaler()\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "# Display the first few rows of the processed DataFrame.\n",
    "# You can see the new 'hour', 'day_of_week', and 'month' columns,\n",
    "# and all numerical columns are now scaled.\n",
    "print(\"Processed Data (First 5 rows):\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d131c-bcf8-40e6-b791-d72cdd5c2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load the preprocessed data ---\n",
    "# For a self-contained example, we'll embed the preprocessed data string.\n",
    "# In a real project, you would load this from the 'era5_land_data_processed.csv' file.\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,1.24844,1.056034,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,1.24844,1.056034,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,1.24844,1.056034,0.0\n",
    "-1.921349,2.15758,1.529267,10.85,76.25,-1.33285,1.370597,0.0\n",
    "-2.24977,2.548879,1.735249,10.85,76.5,-1.33285,1.370597,0.0\n",
    "-1.675032,1.96193,1.352712,10.6,76.25,-1.33285,1.370597,0.0\n",
    "-0.405051,-0.385863,0.180424,10.85,76.25,-0.441994,1.370597,0.0\n",
    "-0.651368,-0.288039,0.357597,10.85,76.5,-0.441994,1.370597,0.0\n",
    "-0.113994,-0.483687,-0.021105,10.6,76.25,-0.441994,1.370597,0.0\n",
    "0.54585,-0.972836,-0.613041,10.85,76.25,0.349563,1.370597,0.0\n",
    "0.299533,-0.875012,-0.407060,10.85,76.5,0.349563,1.370597,0.0\n",
    "0.792166,-1.07066,-0.789648,10.6,76.25,0.349563,1.370597,0.0\n",
    "-0.767838,0.592383,0.509088,10.85,76.25,1.24844,1.370597,0.0\n",
    "-1.014155,0.885856,0.714488,10.85,76.5,1.24844,1.370597,0.0\n",
    "-0.537210,0.494558,0.269601,10.6,76.25,1.24844,1.370597,0.0\n",
    "-1.581515,1.668453,1.200155,10.85,76.25,-1.33285,1.68516,0.0\n",
    "-1.921349,2.059752,1.405788,10.85,76.5,-1.33285,1.68516,0.0\n",
    "-1.25337,1.472804,1.017382,10.6,76.25,-1.33285,1.68516,0.0\n",
    "-0.1931, -0.679336, 0.082728, 10.85, 76.25, -0.441994, 1.68516, 0.0\n",
    "-0.440625, -0.581512, 0.259899, 10.85, 76.5, -0.441994, 1.68516, 0.0\n",
    "0.053217, -0.777161, -0.097479, 10.6, 76.25, -0.441994, 1.68516, 0.0\n",
    "1.01848, -1.07066, -0.995629, 10.85, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.771658, -0.972836, -0.789648, 10.85, 76.5, 0.349563, 1.68516, 0.0\n",
    "1.264795, -1.168485, -1.132039, 10.6, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.053217, -0.092415, -0.097479, 10.85, 76.25, 1.24844, 1.68516, 0.0\n",
    "-0.193100,0.103259,0.113069,10.85,76.5,1.24844,1.68516,0.0\n",
    "0.299533,-0.189564,-0.173877,10.6,76.25,1.24844,1.68516,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,-1.33285,1.999723,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,-1.33285,1.999723,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,-1.33285,1.999723,0.0\n",
    "-0.405051,-0.777161,-0.230504,10.85,76.25,-0.441994,1.999723,0.0\n",
    "-0.651368,-0.679336,-0.053948,10.85,76.5,-0.441994,1.999723,0.0\n",
    "-0.113994,-0.875012,-0.407060,10.6,76.25,-0.441994,1.999723,0.0\n",
    "0.54585,-1.168485,-1.319595,10.85,76.25,0.349563,1.999723,0.0\n",
    "0.299533,-1.07066,-1.132039,10.85,76.5,0.349563,1.999723,0.0\n",
    "0.792166,-1.26631, -1.496206, 10.6, 76.25, 0.349563, 1.999723, 0.0\n",
    "-0.767838, -0.189564, -0.113069, 10.85, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.014155, 0.005435, 0.097899, 10.85, 76.5, 1.24844, 1.999723, 0.0\n",
    "-0.53721, -0.288039, -0.32049, 10.6, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.342576, 0.592383, 0.828699, 10.85, 76.25, -1.33285, 2.314286, 0.0\n",
    "-1.671, 0.885856, 1.03468, 10.85, 76.5, -1.33285, 2.314286, 0.0\n",
    "-1.119702, 0.494558, 0.615296, 10.6, 76.25, -1.33285, 2.314286, 0.0\n",
    "-0.405051, -0.972836, -0.440263, 10.85, 76.25, -0.441994, 2.314286, 0.0\n",
    "-0.651368, -0.875012, -0.263706, 10.85, 76.5, -0.441994, 2.314286, 0.0\n",
    "-0.113994, -0.972836, -0.630043, 10.6, 76.25, -0.441994, 2.314286, 0.0\n",
    "1.614631, -1.168485, -1.636653, 10.85, 76.25, 0.349563, 2.314286, 0.0\n",
    "1.242784, -1.168485, -1.458269, 10.85, 76.5, 0.349563, 2.314286, 0.0\n",
    "1.890666, -1.26631, -1.813208, 10.6, 76.25, 0.349563, 2.314286, 0.0\n",
    "-0.038164, -0.777161, -0.457896, 10.85, 76.25, 1.24844, 2.314286, 0.0\n",
    "-0.286383, -0.581512, -0.278546, 10.85, 76.5, 1.24844, 2.314286, 0.0\n",
    "0.198305, -0.679336, -0.492576, 10.6, 76.25, 1.24844, 2.314286, 0.0\n",
    "\"\"\"\n",
    "df = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "\n",
    "print(\"DataFrame loaded successfully:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# --- 2. Split the data into features (X) and target (y) ---\n",
    "# We'll use all other features to predict the 2m temperature.\n",
    "X = df.drop('2m_temperature_c', axis=1) # All columns except '2m_temperature_c'\n",
    "y = df['2m_temperature_c'] # The target variable we want to predict\n",
    "\n",
    "# --- 3. Split the data into training and testing sets ---\n",
    "# We use a 70/30 split for training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"\\nData Split Complete:\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")\n",
    "\n",
    "# --- 4. Train the Linear Regression model ---\n",
    "model = LinearRegression()\n",
    "print(\"\\nTraining Linear Regression model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Make predictions on the test set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 6. Evaluate the model ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# You can also inspect the model's coefficients to see which features are most important\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5c60b-2157-44f5-9571-e56f75e84518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load the preprocessed data ---\n",
    "# We'll use the same embedded string for a self-contained example.\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,1.24844,1.056034,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,1.24844,1.056034,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,1.24844,1.056034,0.0\n",
    "-1.921349,2.15758,1.529267,10.85,76.25,-1.33285,1.370597,0.0\n",
    "-2.24977,2.548879,1.735249,10.85,76.5,-1.33285,1.370597,0.0\n",
    "-1.675032,1.96193,1.352712,10.6,76.25,-1.33285,1.370597,0.0\n",
    "-0.405051,-0.385863,0.180424,10.85,76.25,-0.441994,1.370597,0.0\n",
    "-0.651368,-0.288039,0.357597,10.85,76.5,-0.441994,1.370597,0.0\n",
    "-0.113994,-0.483687,-0.021105,10.6,76.25,-0.441994,1.370597,0.0\n",
    "0.54585,-0.972836,-0.613041,10.85,76.25,0.349563,1.370597,0.0\n",
    "0.299533,-0.875012,-0.407060,10.85,76.5,0.349563,1.370597,0.0\n",
    "0.792166,-1.07066,-0.789648,10.6,76.25,0.349563,1.370597,0.0\n",
    "-0.767838,0.592383,0.509088,10.85,76.25,1.24844,1.370597,0.0\n",
    "-1.014155,0.885856,0.714488,10.85,76.5,1.24844,1.370597,0.0\n",
    "-0.537210,0.494558,0.269601,10.6,76.25,1.24844,1.370597,0.0\n",
    "-1.581515,1.668453,1.200155,10.85,76.25,-1.33285,1.68516,0.0\n",
    "-1.921349,2.059752,1.405788,10.85,76.5,-1.33285,1.68516,0.0\n",
    "-1.25337,1.472804,1.017382,10.6,76.25,-1.33285,1.68516,0.0\n",
    "-0.1931, -0.679336, 0.082728, 10.85, 76.25, -0.441994, 1.68516, 0.0\n",
    "-0.440625, -0.581512, 0.259899, 10.85, 76.5, -0.441994, 1.68516, 0.0\n",
    "0.053217, -0.777161, -0.097479, 10.6, 76.25, -0.441994, 1.68516, 0.0\n",
    "1.01848, -1.07066, -0.995629, 10.85, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.771658, -0.972836, -0.789648, 10.85, 76.5, 0.349563, 1.68516, 0.0\n",
    "1.264795, -1.168485, -1.132039, 10.6, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.053217, -0.092415, -0.097479, 10.85, 76.25, 1.24844, 1.68516, 0.0\n",
    "-0.193100,0.103259,0.113069,10.85,76.5,1.24844,1.68516,0.0\n",
    "0.299533,-0.189564,-0.173877,10.6,76.25,1.24844,1.68516,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,-1.33285,1.999723,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,-1.33285,1.999723,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,-1.33285,1.999723,0.0\n",
    "-0.405051,-0.777161,-0.230504,10.85,76.25,-0.441994,1.999723,0.0\n",
    "-0.651368,-0.679336,-0.053948,10.85,76.5,-0.441994,1.999723,0.0\n",
    "-0.113994,-0.875012,-0.407060,10.6,76.25,-0.441994,1.999723,0.0\n",
    "0.54585,-1.168485,-1.319595,10.85,76.25,0.349563,1.999723,0.0\n",
    "0.299533,-1.07066,-1.132039,10.85,76.5,0.349563,1.999723,0.0\n",
    "0.792166,-1.26631, -1.496206, 10.6, 76.25, 0.349563, 1.999723, 0.0\n",
    "-0.767838, -0.189564, -0.113069, 10.85, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.014155, 0.005435, 0.097899, 10.85, 76.5, 1.24844, 1.999723, 0.0\n",
    "-0.53721, -0.288039, -0.32049, 10.6, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.342576, 0.592383, 0.828699, 10.85, 76.25, -1.33285, 2.314286, 0.0\n",
    "-1.671, 0.885856, 1.03468, 10.85, 76.5, -1.33285, 2.314286, 0.0\n",
    "-1.119702, 0.494558, 0.615296, 10.6, 76.25, -1.33285, 2.314286, 0.0\n",
    "-0.405051, -0.972836, -0.440263, 10.85, 76.25, -0.441994, 2.314286, 0.0\n",
    "-0.651368, -0.875012, -0.263706, 10.85, 76.5, -0.441994, 2.314286, 0.0\n",
    "-0.113994, -0.972836, -0.630043, 10.6, 76.25, -0.441994, 2.314286, 0.0\n",
    "1.614631, -1.168485, -1.636653, 10.85, 76.25, 0.349563, 2.314286, 0.0\n",
    "1.242784, -1.168485, -1.458269, 10.85, 76.5, 0.349563, 2.314286, 0.0\n",
    "1.890666, -1.26631, -1.813208, 10.6, 76.25, 0.349563, 2.314286, 0.0\n",
    "-0.038164, -0.777161, -0.457896, 10.85, 76.25, 1.24844, 2.314286, 0.0\n",
    "-0.286383, -0.581512, -0.278546, 10.85, 76.5, 1.24844, 2.314286, 0.0\n",
    "0.198305, -0.679336, -0.492576, 10.6, 76.25, 1.24844, 2.314286, 0.0\n",
    "\"\"\"\n",
    "df = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "\n",
    "# --- 2. Re-split the data into features (X) and target (y) ---\n",
    "# We need to do this again to have the same variables in the same script.\n",
    "X = df.drop('2m_temperature_c', axis=1) # All columns except '2m_temperature_c'\n",
    "y = df['2m_temperature_c'] # The target variable we want to predict\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 3. Train the Linear Regression model ---\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. Make predictions on the test set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 5. Create a scatter plot to visualize predictions ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, color='dodgerblue', label='Predictions')\n",
    "\n",
    "# Plot the perfect prediction line (y=x)\n",
    "plt.plot(y_test, y_test, color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Actual vs. Predicted 2m Temperature', fontsize=16)\n",
    "plt.xlabel('Actual 2m Temperature (Normalized)', fontsize=12)\n",
    "plt.ylabel('Predicted 2m Temperature (Normalized)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c90b2-831c-4db7-8ccf-6d2a0e4893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load the preprocessed data ---\n",
    "# We'll use the same embedded string for a self-contained example.\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,1.24844,1.056034,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,1.24844,1.056034,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,1.24844,1.056034,0.0\n",
    "-1.921349,2.15758,1.529267,10.85,76.25,-1.33285,1.370597,0.0\n",
    "-2.24977,2.548879,1.735249,10.85,76.5,-1.33285,1.370597,0.0\n",
    "-1.675032,1.96193,1.352712,10.6,76.25,-1.33285,1.370597,0.0\n",
    "-0.405051,-0.385863,0.180424,10.85,76.25,-0.441994,1.370597,0.0\n",
    "-0.651368,-0.288039,0.357597,10.85,76.5,-0.441994,1.370597,0.0\n",
    "-0.113994,-0.483687,-0.021105,10.6,76.25,-0.441994,1.370597,0.0\n",
    "0.54585,-0.972836,-0.613041,10.85,76.25,0.349563,1.370597,0.0\n",
    "0.299533,-0.875012,-0.407060,10.85,76.5,0.349563,1.370597,0.0\n",
    "0.792166,-1.07066,-0.789648,10.6,76.25,0.349563,1.370597,0.0\n",
    "-0.767838,0.592383,0.509088,10.85,76.25,1.24844,1.370597,0.0\n",
    "-1.014155,0.885856,0.714488,10.85,76.5,1.24844,1.370597,0.0\n",
    "-0.537210,0.494558,0.269601,10.6,76.25,1.24844,1.370597,0.0\n",
    "-1.581515,1.668453,1.200155,10.85,76.25,-1.33285,1.68516,0.0\n",
    "-1.921349,2.059752,1.405788,10.85,76.5,-1.33285,1.68516,0.0\n",
    "-1.25337,1.472804,1.017382,10.6,76.25,-1.33285,1.68516,0.0\n",
    "-0.1931, -0.679336, 0.082728, 10.85, 76.25, -0.441994, 1.68516, 0.0\n",
    "-0.440625, -0.581512, 0.259899, 10.85, 76.5, -0.441994, 1.68516, 0.0\n",
    "0.053217, -0.777161, -0.097479, 10.6, 76.25, -0.441994, 1.68516, 0.0\n",
    "1.01848, -1.07066, -0.995629, 10.85, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.771658, -0.972836, -0.789648, 10.85, 76.5, 0.349563, 1.68516, 0.0\n",
    "1.264795, -1.168485, -1.132039, 10.6, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.053217, -0.092415, -0.097479, 10.85, 76.25, 1.24844, 1.68516, 0.0\n",
    "-0.193100,0.103259,0.113069,10.85,76.5,1.24844,1.68516,0.0\n",
    "0.299533,-0.189564,-0.173877,10.6,76.25,1.24844,1.68516,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,-1.33285,1.999723,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,-1.33285,1.999723,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,-1.33285,1.999723,0.0\n",
    "-0.405051,-0.777161,-0.230504,10.85,76.25,-0.441994,1.999723,0.0\n",
    "-0.651368,-0.679336,-0.053948,10.85,76.5,-0.441994,1.999723,0.0\n",
    "-0.113994,-0.875012,-0.407060,10.6,76.25,-0.441994,1.999723,0.0\n",
    "0.54585,-1.168485,-1.319595,10.85,76.25,0.349563,1.999723,0.0\n",
    "0.299533,-1.07066,-1.132039,10.85,76.5,0.349563,1.999723,0.0\n",
    "0.792166,-1.26631, -1.496206, 10.6, 76.25, 0.349563, 1.999723, 0.0\n",
    "-0.767838, -0.189564, -0.113069, 10.85, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.014155, 0.005435, 0.097899, 10.85, 76.5, 1.24844, 1.999723, 0.0\n",
    "-0.53721, -0.288039, -0.32049, 10.6, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.342576, 0.592383, 0.828699, 10.85, 76.25, -1.33285, 2.314286, 0.0\n",
    "-1.671, 0.885856, 1.03468, 10.85, 76.5, -1.33285, 2.314286, 0.0\n",
    "-1.119702, 0.494558, 0.615296, 10.6, 76.25, -1.33285, 2.314286, 0.0\n",
    "-0.405051, -0.972836, -0.440263, 10.85, 76.25, -0.441994, 2.314286, 0.0\n",
    "-0.651368, -0.875012, -0.263706, 10.85, 76.5, -0.441994, 2.314286, 0.0\n",
    "-0.113994, -0.972836, -0.630043, 10.6, 76.25, -0.441994, 2.314286, 0.0\n",
    "1.614631, -1.168485, -1.636653, 10.85, 76.25, 0.349563, 2.314286, 0.0\n",
    "1.242784, -1.168485, -1.458269, 10.85, 76.5, 0.349563, 2.314286, 0.0\n",
    "1.890666, -1.26631, -1.813208, 10.6, 76.25, 0.349563, 2.314286, 0.0\n",
    "-0.038164, -0.777161, -0.457896, 10.85, 76.25, 1.24844, 2.314286, 0.0\n",
    "-0.286383, -0.581512, -0.278546, 10.85, 76.5, 1.24844, 2.314286, 0.0\n",
    "0.198305, -0.679336, -0.492576, 10.6, 76.25, 1.24844, 2.314286, 0.0\n",
    "\"\"\"\n",
    "df = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "\n",
    "print(\"DataFrame loaded successfully:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- 2. Split the data into features (X) and target (y) ---\n",
    "# We'll use all other features to predict the 2m temperature.\n",
    "X = df.drop('2m_temperature_c', axis=1) # All columns except '2m_temperature_c'\n",
    "y = df['2m_temperature_c'] # The target variable we want to predict\n",
    "\n",
    "# --- 3. Split the data into training and testing sets ---\n",
    "# We use a 70/30 split for training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"\\nData Split Complete:\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")\n",
    "\n",
    "# --- 4. Train the Random Forest Regressor model ---\n",
    "# This is the key change from the previous script!\n",
    "# We instantiate a Random Forest model instead of a Linear Regression model.\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "print(\"\\nTraining Random Forest Regressor model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Make predictions on the test set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 6. Evaluate the model ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nRandom Forest Model Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# Random Forest also has a feature_importances_ attribute that is very useful\n",
    "print(\"\\nFeature Importances (Top 5):\")\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(importances.sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18b209-3c05-4707-8424-d23c11a37db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load the preprocessed data ---\n",
    "# We'll use the same embedded string for a self-contained example.\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,1.24844,1.056034,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,1.24844,1.056034,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,1.24844,1.056034,0.0\n",
    "-1.921349,2.15758,1.529267,10.85,76.25,-1.33285,1.370597,0.0\n",
    "-2.24977,2.548879,1.735249,10.85,76.5,-1.33285,1.370597,0.0\n",
    "-1.675032,1.96193,1.352712,10.6,76.25,-1.33285,1.370597,0.0\n",
    "-0.405051,-0.385863,0.180424,10.85,76.25,-0.441994,1.370597,0.0\n",
    "-0.651368,-0.288039,0.357597,10.85,76.5,-0.441994,1.370597,0.0\n",
    "-0.113994,-0.483687,-0.021105,10.6,76.25,-0.441994,1.370597,0.0\n",
    "0.54585,-0.972836,-0.613041,10.85,76.25,0.349563,1.370597,0.0\n",
    "0.299533,-0.875012,-0.407060,10.85,76.5,0.349563,1.370597,0.0\n",
    "0.792166,-1.07066,-0.789648,10.6,76.25,0.349563,1.370597,0.0\n",
    "-0.767838,0.592383,0.509088,10.85,76.25,1.24844,1.370597,0.0\n",
    "-1.014155,0.885856,0.714488,10.85,76.5,1.24844,1.370597,0.0\n",
    "-0.537210,0.494558,0.269601,10.6,76.25,1.24844,1.370597,0.0\n",
    "-1.581515,1.668453,1.200155,10.85,76.25,-1.33285,1.68516,0.0\n",
    "-1.921349,2.059752,1.405788,10.85,76.5,-1.33285,1.68516,0.0\n",
    "-1.25337,1.472804,1.017382,10.6,76.25,-1.33285,1.68516,0.0\n",
    "-0.1931, -0.679336, 0.082728, 10.85, 76.25, -0.441994, 1.68516, 0.0\n",
    "-0.440625, -0.581512, 0.259899, 10.85, 76.5, -0.441994, 1.68516, 0.0\n",
    "0.053217, -0.777161, -0.097479, 10.6, 76.25, -0.441994, 1.68516, 0.0\n",
    "1.01848, -1.07066, -0.995629, 10.85, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.771658, -0.972836, -0.789648, 10.85, 76.5, 0.349563, 1.68516, 0.0\n",
    "1.264795, -1.168485, -1.132039, 10.6, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.053217, -0.092415, -0.097479, 10.85, 76.25, 1.24844, 1.68516, 0.0\n",
    "-0.193100,0.103259,0.113069,10.85,76.5,1.24844,1.68516,0.0\n",
    "0.299533,-0.189564,-0.173877,10.6,76.25,1.24844,1.68516,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,-1.33285,1.999723,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,-1.33285,1.999723,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,-1.33285,1.999723,0.0\n",
    "-0.405051,-0.777161,-0.230504,10.85,76.25,-0.441994,1.999723,0.0\n",
    "-0.651368,-0.679336,-0.053948,10.85,76.5,-0.441994,1.999723,0.0\n",
    "-0.113994,-0.875012,-0.407060,10.6,76.25,-0.441994,1.999723,0.0\n",
    "0.54585,-1.168485,-1.319595,10.85,76.25,0.349563,1.999723,0.0\n",
    "0.299533,-1.07066,-1.132039,10.85,76.5,0.349563,1.999723,0.0\n",
    "0.792166,-1.26631, -1.496206, 10.6, 76.25, 0.349563, 1.999723, 0.0\n",
    "-0.767838, -0.189564, -0.113069, 10.85, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.014155, 0.005435, 0.097899, 10.85, 76.5, 1.24844, 1.999723, 0.0\n",
    "-0.53721, -0.288039, -0.32049, 10.6, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.342576, 0.592383, 0.828699, 10.85, 76.25, -1.33285, 2.314286, 0.0\n",
    "-1.671, 0.885856, 1.03468, 10.85, 76.5, -1.33285, 2.314286, 0.0\n",
    "-1.119702, 0.494558, 0.615296, 10.6, 76.25, -1.33285, 2.314286, 0.0\n",
    "-0.405051, -0.972836, -0.440263, 10.85, 76.25, -0.441994, 2.314286, 0.0\n",
    "-0.651368, -0.875012, -0.263706, 10.85, 76.5, -0.441994, 2.314286, 0.0\n",
    "-0.113994, -0.972836, -0.630043, 10.6, 76.25, -0.441994, 2.314286, 0.0\n",
    "1.614631, -1.168485, -1.636653, 10.85, 76.25, 0.349563, 2.314286, 0.0\n",
    "1.242784, -1.168485, -1.458269, 10.85, 76.5, 0.349563, 2.314286, 0.0\n",
    "1.890666, -1.26631, -1.813208, 10.6, 76.25, 0.349563, 2.314286, 0.0\n",
    "-0.038164, -0.777161, -0.457896, 10.85, 76.25, 1.24844, 2.314286, 0.0\n",
    "-0.286383, -0.581512, -0.278546, 10.85, 76.5, 1.24844, 2.314286, 0.0\n",
    "0.198305, -0.679336, -0.492576, 10.6, 76.25, 1.24844, 2.314286, 0.0\n",
    "\"\"\"\n",
    "df = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "\n",
    "# --- 2. Split the data into features (X) and target (y) ---\n",
    "X = df.drop('2m_temperature_c', axis=1)\n",
    "y = df['2m_temperature_c']\n",
    "\n",
    "# --- 3. Split the data into training and testing sets ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 4. Train the Random Forest Regressor model ---\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Make predictions on the test set ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 6. Evaluate the model (Metrics) ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Random Forest Model Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "# --- 7. Visualize the results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, color='dodgerblue', label='Predictions')\n",
    "\n",
    "# Plot the ideal 'perfect prediction' line\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], '--', color='red', label='Perfect Prediction')\n",
    "\n",
    "plt.title('Actual vs. Predicted 2m Temperature (Random Forest)', fontsize=16)\n",
    "plt.xlabel('Actual Temperature ($°C$)', fontsize=12)\n",
    "plt.ylabel('Predicted Temperature ($°C$)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# --- 8. Print Feature Importances ---\n",
    "print(\"\\nFeature Importances:\")\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(importances.sort_values(ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaac47b-80f7-426f-b48f-d9051129113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# --- 1. Load the preprocessed data ---\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,1.24844,1.056034,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,1.24844,1.056034,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,1.24844,1.056034,0.0\n",
    "-1.921349,2.15758,1.529267,10.85,76.25,-1.33285,1.370597,0.0\n",
    "-2.24977,2.548879,1.735249,10.85,76.5,-1.33285,1.370597,0.0\n",
    "-1.675032,1.96193,1.352712,10.6,76.25,-1.33285,1.370597,0.0\n",
    "-0.405051,-0.385863,0.180424,10.85,76.25,-0.441994,1.370597,0.0\n",
    "-0.651368,-0.288039,0.357597,10.85,76.5,-0.441994,1.370597,0.0\n",
    "-0.113994,-0.483687,-0.021105,10.6,76.25,-0.441994,1.370597,0.0\n",
    "0.54585,-0.972836,-0.613041,10.85,76.25,0.349563,1.370597,0.0\n",
    "0.299533,-0.875012,-0.407060,10.85,76.5,0.349563,1.370597,0.0\n",
    "0.792166,-1.07066,-0.789648,10.6,76.25,0.349563,1.370597,0.0\n",
    "-0.767838,0.592383,0.509088,10.85,76.25,1.24844,1.370597,0.0\n",
    "-1.014155,0.885856,0.714488,10.85,76.5,1.24844,1.370597,0.0\n",
    "-0.537210,0.494558,0.269601,10.6,76.25,1.24844,1.370597,0.0\n",
    "-1.581515,1.668453,1.200155,10.85,76.25,-1.33285,1.68516,0.0\n",
    "-1.921349,2.059752,1.405788,10.85,76.5,-1.33285,1.68516,0.0\n",
    "-1.25337,1.472804,1.017382,10.6,76.25,-1.33285,1.68516,0.0\n",
    "-0.1931, -0.679336, 0.082728, 10.85, 76.25, -0.441994, 1.68516, 0.0\n",
    "-0.440625, -0.581512, 0.259899, 10.85, 76.5, -0.441994, 1.68516, 0.0\n",
    "0.053217, -0.777161, -0.097479, 10.6, 76.25, -0.441994, 1.68516, 0.0\n",
    "1.01848, -1.07066, -0.995629, 10.85, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.771658, -0.972836, -0.789648, 10.85, 76.5, 0.349563, 1.68516, 0.0\n",
    "1.264795, -1.168485, -1.132039, 10.6, 76.25, 0.349563, 1.68516, 0.0\n",
    "0.053217, -0.092415, -0.097479, 10.85, 76.25, 1.24844, 1.68516, 0.0\n",
    "-0.193100,0.103259,0.113069,10.85,76.5,1.24844,1.68516,0.0\n",
    "0.299533,-0.189564,-0.173877,10.6,76.25,1.24844,1.68516,0.0\n",
    "-1.014155,0.983681,0.887685,10.85,76.25,-1.33285,1.999723,0.0\n",
    "-1.342576,1.37498,1.093667,10.85,76.5,-1.33285,1.999723,0.0\n",
    "-0.767838,0.885856,0.669894,10.6,76.25,-1.33285,1.999723,0.0\n",
    "-0.405051,-0.777161,-0.230504,10.85,76.25,-0.441994,1.999723,0.0\n",
    "-0.651368,-0.679336,-0.053948,10.85,76.5,-0.441994,1.999723,0.0\n",
    "-0.113994,-0.875012,-0.407060,10.6,76.25,-0.441994,1.999723,0.0\n",
    "0.54585,-1.168485,-1.319595,10.85,76.25,0.349563,1.999723,0.0\n",
    "0.299533,-1.07066,-1.132039,10.85,76.5,0.349563,1.999723,0.0\n",
    "0.792166,-1.26631, -1.496206, 10.6, 76.25, 0.349563, 1.999723, 0.0\n",
    "-0.767838, -0.189564, -0.113069, 10.85, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.014155, 0.005435, 0.097899, 10.85, 76.5, 1.24844, 1.999723, 0.0\n",
    "-0.53721, -0.288039, -0.32049, 10.6, 76.25, 1.24844, 1.999723, 0.0\n",
    "-1.342576, 0.592383, 0.828699, 10.85, 76.25, -1.33285, 2.314286, 0.0\n",
    "-1.671, 0.885856, 1.03468, 10.85, 76.5, -1.33285, 2.314286, 0.0\n",
    "-1.119702, 0.494558, 0.615296, 10.6, 76.25, -1.33285, 2.314286, 0.0\n",
    "-0.405051, -0.972836, -0.440263, 10.85, 76.25, -0.441994, 2.314286, 0.0\n",
    "-0.651368, -0.875012, -0.263706, 10.85, 76.5, -0.441994, 2.314286, 0.0\n",
    "-0.113994, -0.972836, -0.630043, 10.6, 76.25, -0.441994, 2.314286, 0.0\n",
    "1.614631, -1.168485, -1.636653, 10.85, 76.25, 0.349563, 2.314286, 0.0\n",
    "1.242784, -1.168485, -1.458269, 10.85, 76.5, 0.349563, 2.314286, 0.0\n",
    "1.890666, -1.26631, -1.813208, 10.6, 76.25, 0.349563, 2.314286, 0.0\n",
    "-0.038164, -0.777161, -0.457896, 10.85, 76.25, 1.24844, 2.314286, 0.0\n",
    "-0.286383, -0.581512, -0.278546, 10.85, 76.5, 1.24844, 2.314286, 0.0\n",
    "0.198305, -0.679336, -0.492576, 10.6, 76.25, 1.24844, 2.314286, 0.0\n",
    "\"\"\"\n",
    "df = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "\n",
    "# --- 2. Split the data into features (X) and target (y) ---\n",
    "X = df.drop('2m_temperature_c', axis=1)\n",
    "y = df['2m_temperature_c']\n",
    "\n",
    "# --- 3. Split the data into training and testing sets ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 4. Train the Random Forest Regressor model ---\n",
    "print(\"Training Random Forest Regressor model...\")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Save the trained model to a file ---\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(f\"\\nModel successfully saved to '{model_filename}'\")\n",
    "print(\"You can now load this file to use the model for new predictions without retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0674f7-9fb5-470a-8e7b-c4dffd7cf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# --- 1. Load the preprocessed data (for feature names) ---\n",
    "# We use this to make sure the new data has the same columns and order as the training data.\n",
    "preprocessed_data_string = \"\"\"\n",
    "2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "-0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "\"\"\"\n",
    "df_features = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "# Drop the target variable to get the feature names\n",
    "feature_columns = df_features.drop('2m_temperature_c', axis=1).columns\n",
    "\n",
    "# --- 2. Load the trained model from the file ---\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "print(f\"Loading model from '{model_filename}'...\")\n",
    "try:\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{model_filename}' was not found. Please ensure it exists in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Prepare new data for prediction ---\n",
    "# This is an example of a new data point. The values must be preprocessed\n",
    "# in the same way the training data was.\n",
    "new_data = {\n",
    "    'total_precipitation_mm': [0.5],\n",
    "    'relative_humidity_percent': [0.75],\n",
    "    'latitude': [10.85],\n",
    "    'longitude': [76.25],\n",
    "    'hour': [12],\n",
    "    'day_of_week': [3],\n",
    "    'month': [7]\n",
    "}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Ensure the new DataFrame has the same columns and order as the training data\n",
    "new_df_aligned = new_df.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "# --- 4. Make a prediction using the loaded model ---\n",
    "print(\"\\nMaking prediction on new data...\")\n",
    "prediction = loaded_model.predict(new_df_aligned)\n",
    "print(f\"The predicted 2m_temperature_c is: {prediction[0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873ff7-1d39-4239-937f-cde8e16ac4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from flask import Flask, request, render_template_string\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- 1. Define the HTML template for the web page ---\n",
    "# We're embedding the HTML directly in the Python file for simplicity.\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Temperature Predictor</title>\n",
    "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 flex items-center justify-center min-h-screen p-4\">\n",
    "    <div class=\"bg-white rounded-lg shadow-xl p-8 max-w-lg w-full\">\n",
    "        <h1 class=\"text-3xl font-bold text-gray-800 text-center mb-6\">Predict 2m Temperature</h1>\n",
    "        <p class=\"text-center text-gray-600 mb-8\">Enter the conditions below to get a temperature prediction from our trained Random Forest model.</p>\n",
    "\n",
    "        <form action=\"/predict\" method=\"post\" class=\"space-y-6\">\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"precipitation\" class=\"block text-sm font-medium text-gray-700\">Total Precipitation (mm)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"precipitation\" name=\"total_precipitation_mm\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"humidity\" class=\"block text-sm font-medium text-gray-700\">Relative Humidity (%)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"humidity\" name=\"relative_humidity_percent\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"latitude\" class=\"block text-sm font-medium text-gray-700\">Latitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"latitude\" name=\"latitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"longitude\" class=\"block text-sm font-medium text-gray-700\">Longitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"longitude\" name=\"longitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"hour\" class=\"block text-sm font-medium text-gray-700\">Hour (0-23)</label>\n",
    "                    <input type=\"number\" id=\"hour\" name=\"hour\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"day\" class=\"block text-sm font-medium text-gray-700\">Day of Week (0-6)</label>\n",
    "                    <input type=\"number\" id=\"day\" name=\"day_of_week\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"month\" class=\"block text-sm font-medium text-gray-700\">Month (1-12)</label>\n",
    "                    <input type=\"number\" id=\"month\" name=\"month\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <button type=\"submit\"\n",
    "                    class=\"w-full flex justify-center py-2 px-4 border border-transparent rounded-md shadow-sm text-sm font-medium text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500\">\n",
    "                Get Prediction\n",
    "            </button>\n",
    "        </form>\n",
    "\n",
    "        {% if prediction %}\n",
    "        <div class=\"mt-8 p-4 bg-indigo-50 rounded-md\">\n",
    "            <h2 class=\"text-xl font-semibold text-gray-800 text-center\">Prediction Result</h2>\n",
    "            <p class=\"text-center mt-2 text-lg font-medium text-indigo-700\">The predicted 2m temperature is: <span class=\"font-bold\">{{ prediction }}°C</span></p>\n",
    "        </div>\n",
    "        {% endif %}\n",
    "\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Load the model and get feature names once when the app starts ---\n",
    "# This saves time by not loading the model on every request.\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "try:\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    print(f\"Model '{model_filename}' loaded successfully.\")\n",
    "\n",
    "    # We need the feature names from the original training data.\n",
    "    preprocessed_data_string = \"\"\"\n",
    "    2m_temperature_c,total_precipitation_mm,relative_humidity_percent,latitude,longitude,hour,day_of_week,month\n",
    "    0.053217,0.103259,-0.230504,10.85,76.25,0.349563,1.056034,0.0\n",
    "    -0.193100,0.396733,-0.053948,10.85,76.5,0.349563,1.056034,0.0\n",
    "    0.299533,0.005435,-0.407060,10.6,76.25,0.349563,1.056034,0.0\n",
    "    \"\"\"\n",
    "    df_features = pd.read_csv(StringIO(preprocessed_data_string))\n",
    "    feature_columns = df_features.drop('2m_temperature_c', axis=1).columns\n",
    "    print(\"Feature columns successfully loaded.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{model_filename}' was not found. Please ensure it is in the same directory.\")\n",
    "    loaded_model = None\n",
    "    feature_columns = None\n",
    "    exit()\n",
    "\n",
    "# --- 3. Define the main route for the home page ---\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"\n",
    "    Renders the HTML form.\n",
    "    \"\"\"\n",
    "    return render_template_string(HTML_TEMPLATE, prediction=None)\n",
    "\n",
    "# --- 4. Define the prediction route ---\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Handles the form submission, makes a prediction, and renders the result.\n",
    "    \"\"\"\n",
    "    if loaded_model is None or feature_columns is None:\n",
    "        return \"Error: Model or feature names not loaded.\", 500\n",
    "\n",
    "    # Get the data from the form\n",
    "    try:\n",
    "        new_data = {\n",
    "            'total_precipitation_mm': [float(request.form['total_precipitation_mm'])],\n",
    "            'relative_humidity_percent': [float(request.form['relative_humidity_percent'])],\n",
    "            'latitude': [float(request.form['latitude'])],\n",
    "            'longitude': [float(request.form['longitude'])],\n",
    "            'hour': [int(request.form['hour'])],\n",
    "            'day_of_week': [int(request.form['day_of_week'])],\n",
    "            'month': [int(request.form['month'])]\n",
    "        }\n",
    "        \n",
    "        # Create a DataFrame from the new data\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        \n",
    "        # Align the new data with the features the model was trained on\n",
    "        new_df_aligned = new_df.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "        # Make the prediction\n",
    "        prediction = loaded_model.predict(new_df_aligned)[0]\n",
    "        \n",
    "        # Render the page with the prediction result\n",
    "        return render_template_string(HTML_TEMPLATE, prediction=f\"{prediction:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during prediction\n",
    "        return f\"An error occurred: {e}\", 400\n",
    "\n",
    "# --- 5. Main entry point to run the app ---\n",
    "if __name__ == '__main__':\n",
    "    # You can run this app by executing this file from the terminal.\n",
    "    # The debug=True flag automatically reloads the server on code changes.\n",
    "    app.run(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55536105-485f-4645-bfcc-7180cc2cff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from flask import Flask, request, render_template_string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- 1. Define the HTML template for the web page ---\n",
    "# We're embedding the HTML directly in the Python file for simplicity.\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Temperature Predictor</title>\n",
    "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 flex items-center justify-center min-h-screen p-4\">\n",
    "    <div class=\"bg-white rounded-lg shadow-xl p-8 max-w-lg w-full\">\n",
    "        <h1 class=\"text-3xl font-bold text-gray-800 text-center mb-6\">Predict 2m Temperature</h1>\n",
    "        <p class=\"text-center text-gray-600 mb-8\">Enter the conditions below to get a temperature prediction from our trained Random Forest model.</p>\n",
    "\n",
    "        <form action=\"/predict\" method=\"post\" class=\"space-y-6\">\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"precipitation\" class=\"block text-sm font-medium text-gray-700\">Total Precipitation (mm)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"precipitation\" name=\"total_precipitation_mm\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"humidity\" class=\"block text-sm font-medium text-gray-700\">Relative Humidity (%)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"humidity\" name=\"relative_humidity_percent\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"latitude\" class=\"block text-sm font-medium text-gray-700\">Latitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"latitude\" name=\"latitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"longitude\" class=\"block text-sm font-medium text-gray-700\">Longitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"longitude\" name=\"longitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"hour\" class=\"block text-sm font-medium text-gray-700\">Hour (0-23)</label>\n",
    "                    <input type=\"number\" id=\"hour\" name=\"hour\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"day\" class=\"block text-sm font-medium text-gray-700\">Day of Week (0-6)</label>\n",
    "                    <input type=\"number\" id=\"day\" name=\"day_of_week\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"month\" class=\"block text-sm font-medium text-gray-700\">Month (1-12)</label>\n",
    "                    <input type=\"number\" id=\"month\" name=\"month\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <button type=\"submit\"\n",
    "                    class=\"w-full flex justify-center py-2 px-4 border border-transparent rounded-md shadow-sm text-sm font-medium text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500\">\n",
    "                Get Prediction\n",
    "            </button>\n",
    "        </form>\n",
    "\n",
    "        {% if prediction %}\n",
    "        <div class=\"mt-8 p-4 bg-indigo-50 rounded-md\">\n",
    "            <h2 class=\"text-xl font-semibold text-gray-800 text-center\">Prediction Result</h2>\n",
    "            <p class=\"text-center mt-2 text-lg font-medium text-indigo-700\">The predicted 2m temperature is: <span class=\"font-bold\">{{ prediction }}°C</span></p>\n",
    "        </div>\n",
    "        {% endif %}\n",
    "\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Load the model and define feature names once when the app starts ---\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "try:\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    print(f\"Model '{model_filename}' loaded successfully.\")\n",
    "\n",
    "    # Directly define the feature columns, as they are known from the training data.\n",
    "    feature_columns = ['total_precipitation_mm', 'relative_humidity_percent', 'latitude', 'longitude', 'hour', 'day_of_week', 'month']\n",
    "    print(\"Feature columns successfully defined.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{model_filename}' was not found. Please ensure it is in the same directory.\")\n",
    "    loaded_model = None\n",
    "    feature_columns = None\n",
    "    exit()\n",
    "\n",
    "# --- 3. Define the main route for the home page ---\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"\n",
    "    Renders the HTML form.\n",
    "    \"\"\"\n",
    "    return render_template_string(HTML_TEMPLATE, prediction=None)\n",
    "\n",
    "# --- 4. Define the prediction route ---\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Handles the form submission, makes a prediction, and renders the result.\n",
    "    \"\"\"\n",
    "    if loaded_model is None or feature_columns is None:\n",
    "        return \"Error: Model or feature names not loaded.\", 500\n",
    "\n",
    "    # Get the data from the form\n",
    "    try:\n",
    "        new_data = {\n",
    "            'total_precipitation_mm': [float(request.form['total_precipitation_mm'])],\n",
    "            'relative_humidity_percent': [float(request.form['relative_humidity_percent'])],\n",
    "            'latitude': [float(request.form['latitude'])],\n",
    "            'longitude': [float(request.form['longitude'])],\n",
    "            'hour': [int(request.form['hour'])],\n",
    "            'day_of_week': [int(request.form['day_of_week'])],\n",
    "            'month': [int(request.form['month'])]\n",
    "        }\n",
    "        \n",
    "        # Create a DataFrame from the new data\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        \n",
    "        # Align the new data with the features the model was trained on\n",
    "        new_df_aligned = new_df.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "        # Make the prediction\n",
    "        prediction = loaded_model.predict(new_df_aligned)[0]\n",
    "        \n",
    "        # Render the page with the prediction result\n",
    "        return render_template_string(HTML_TEMPLATE, prediction=f\"{prediction:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during prediction\n",
    "        return f\"An error occurred: {e}\", 400\n",
    "\n",
    "# --- 5. Main entry point to run the app ---\n",
    "if __name__ == '__main__':\n",
    "    # You can run this app by executing this file from the terminal.\n",
    "    # The debug=True flag automatically reloads the server on code changes.\n",
    "    app.run(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79fe36-b223-42d5-929f-ad378806daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from flask import Flask, request, render_template_string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# --- 1. Define the HTML template for the web page ---\n",
    "# We're embedding the HTML directly in the Python file for simplicity.\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Temperature Predictor</title>\n",
    "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "    <link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 flex items-center justify-center min-h-screen p-4\">\n",
    "    <div class=\"bg-white rounded-lg shadow-xl p-8 max-w-lg w-full\">\n",
    "        <h1 class=\"text-3xl font-bold text-gray-800 text-center mb-6\">Predict 2m Temperature</h1>\n",
    "        <p class=\"text-center text-gray-600 mb-8\">Enter the conditions below to get a temperature prediction from our trained Random Forest model.</p>\n",
    "\n",
    "        <form action=\"/predict\" method=\"post\" class=\"space-y-6\">\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"precipitation\" class=\"block text-sm font-medium text-gray-700\">Total Precipitation (mm)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"precipitation\" name=\"total_precipitation_mm\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"humidity\" class=\"block text-sm font-medium text-gray-700\">Relative Humidity (%)</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"humidity\" name=\"relative_humidity_percent\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"latitude\" class=\"block text-sm font-medium text-gray-700\">Latitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"latitude\" name=\"latitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"longitude\" class=\"block text-sm font-medium text-gray-700\">Longitude</label>\n",
    "                    <input type=\"number\" step=\"any\" id=\"longitude\" name=\"longitude\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                <div>\n",
    "                    <label for=\"hour\" class=\"block text-sm font-medium text-gray-700\">Hour (0-23)</label>\n",
    "                    <input type=\"number\" id=\"hour\" name=\"hour\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"day\" class=\"block text-sm font-medium text-gray-700\">Day of Week (0-6)</label>\n",
    "                    <input type=\"number\" id=\"day\" name=\"day_of_week\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "                <div>\n",
    "                    <label for=\"month\" class=\"block text-sm font-medium text-gray-700\">Month (1-12)</label>\n",
    "                    <input type=\"number\" id=\"month\" name=\"month\" required\n",
    "                           class=\"mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500 sm:text-sm p-2\">\n",
    "                </div>\n",
    "            </div>\n",
    "            <button type=\"submit\"\n",
    "                    class=\"w-full flex justify-center py-2 px-4 border border-transparent rounded-md shadow-sm text-sm font-medium text-white bg-indigo-600 hover:bg-indigo-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-indigo-500\">\n",
    "                Get Prediction\n",
    "            </button>\n",
    "        </form>\n",
    "\n",
    "        {% if prediction %}\n",
    "        <div class=\"mt-8 p-4 bg-indigo-50 rounded-md\">\n",
    "            <h2 class=\"text-xl font-semibold text-gray-800 text-center\">Prediction Result</h2>\n",
    "            <p class=\"text-center mt-2 text-lg font-medium text-indigo-700\">The predicted 2m temperature is: <span class=\"font-bold\">{{ prediction }}°C</span></p>\n",
    "        </div>\n",
    "        {% endif %}\n",
    "\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Load the model and define feature names once when the app starts ---\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "try:\n",
    "    with open(model_filename, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    print(f\"Model '{model_filename}' loaded successfully.\")\n",
    "\n",
    "    # Directly define the feature columns, as they are known from the training data.\n",
    "    feature_columns = ['total_precipitation_mm', 'relative_humidity_percent', 'latitude', 'longitude', 'hour', 'day_of_week', 'month']\n",
    "    print(\"Feature columns successfully defined.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{model_filename}' was not found. Please ensure it is in the same directory.\")\n",
    "    loaded_model = None\n",
    "    feature_columns = None\n",
    "    exit()\n",
    "\n",
    "# --- 3. Define the main route for the home page ---\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"\n",
    "    Renders the HTML form.\n",
    "    \"\"\"\n",
    "    return render_template_string(HTML_TEMPLATE, prediction=None)\n",
    "\n",
    "# --- 4. Define the prediction route ---\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Handles the form submission, makes a prediction, and renders the result.\n",
    "    \"\"\"\n",
    "    if loaded_model is None or feature_columns is None:\n",
    "        return \"Error: Model or feature names not loaded.\", 500\n",
    "\n",
    "    # Get the data from the form\n",
    "    try:\n",
    "        new_data = {\n",
    "            'total_precipitation_mm': [float(request.form['total_precipitation_mm'])],\n",
    "            'relative_humidity_percent': [float(request.form['relative_humidity_percent'])],\n",
    "            'latitude': [float(request.form['latitude'])],\n",
    "            'longitude': [float(request.form['longitude'])],\n",
    "            'hour': [int(request.form['hour'])],\n",
    "            'day_of_week': [int(request.form['day_of_week'])],\n",
    "            'month': [int(request.form['month'])]\n",
    "        }\n",
    "        \n",
    "        # Create a DataFrame from the new data\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        \n",
    "        # Align the new data with the features the model was trained on\n",
    "        new_df_aligned = new_df.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "        # Make the prediction\n",
    "        prediction = loaded_model.predict(new_df_aligned)[0]\n",
    "        \n",
    "        # Render the page with the prediction result\n",
    "        return render_template_string(HTML_TEMPLATE, prediction=f\"{prediction:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during prediction\n",
    "        return f\"An error occurred: {e}\", 400\n",
    "\n",
    "# --- 5. Main entry point to run the app ---\n",
    "if __name__ == '__main__':\n",
    "    # Running in debug=False to avoid issues with auto-reloader in certain environments\n",
    "    app.run(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13a7098-96d8-416d-838d-ecc005c4fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data for Prediction:\n",
      "   total_precipitation_mm  relative_humidity_percent  latitude  longitude  \\\n",
      "0                     0.5                       0.75     10.85      76.25   \n",
      "\n",
      "   hour  day_of_week  month  \n",
      "0    12            3      7  \n",
      "\n",
      "DataFrame structure is ready for the Random Forest model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_new_data(precipitation, humidity, latitude, longitude, hour, day_of_week, month):\n",
    "    \"\"\"\n",
    "    Preprocesses a new data point to be ready for model prediction.\n",
    "\n",
    "    This function takes raw input values for a weather data point and converts them\n",
    "    into a Pandas DataFrame with the specific column order required by the\n",
    "    trained Random Forest model. This is a crucial step to avoid errors\n",
    "    when making new predictions.\n",
    "\n",
    "    Args:\n",
    "        precipitation (float): The total precipitation in millimeters.\n",
    "        humidity (float): The relative humidity percentage.\n",
    "        latitude (float): The latitude of the location.\n",
    "        longitude (float): The longitude of the location.\n",
    "        hour (int): The hour of the day (0-23).\n",
    "        day_of_week (int): The day of the week (0=Monday, 6=Sunday).\n",
    "        month (int): The month of the year (1-12).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A single-row DataFrame with the preprocessed data,\n",
    "                          ready to be passed to the model.\n",
    "    \"\"\"\n",
    "    # Create a dictionary with the new data\n",
    "    new_data_dict = {\n",
    "        'total_precipitation_mm': [precipitation],\n",
    "        'relative_humidity_percent': [humidity],\n",
    "        'latitude': [latitude],\n",
    "        'longitude': [longitude],\n",
    "        'hour': [hour],\n",
    "        'day_of_week': [day_of_week],\n",
    "        'month': [month]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_df = pd.DataFrame(new_data_dict)\n",
    "\n",
    "    # Define the exact column order the model expects.\n",
    "    # This is the most critical step to prevent prediction errors.\n",
    "    feature_columns = [\n",
    "        'total_precipitation_mm', \n",
    "        'relative_humidity_percent', \n",
    "        'latitude', \n",
    "        'longitude', \n",
    "        'hour', \n",
    "        'day_of_week', \n",
    "        'month'\n",
    "    ]\n",
    "\n",
    "    # Reindex the DataFrame to match the required column order.\n",
    "    # We set fill_value=0 as a safeguard, though in this case, all columns\n",
    "    # should be present.\n",
    "    preprocessed_df = new_df.reindex(columns=feature_columns, fill_value=0)\n",
    "    \n",
    "    return preprocessed_df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Example Usage ---\n",
    "    # Define a set of raw input values for a new data point\n",
    "    # These values must be in the same scale as the training data if normalization was applied.\n",
    "    # For now, we will use the unscaled values from the original data source.\n",
    "    raw_input_values = {\n",
    "        'precipitation': 0.5,\n",
    "        'humidity': 0.75,\n",
    "        'latitude': 10.85,\n",
    "        'longitude': 76.25,\n",
    "        'hour': 12,\n",
    "        'day_of_week': 3,\n",
    "        'month': 7\n",
    "    }\n",
    "\n",
    "    # Preprocess the raw data using the function\n",
    "    ready_data = preprocess_new_data(\n",
    "        raw_input_values['precipitation'],\n",
    "        raw_input_values['humidity'],\n",
    "        raw_input_values['latitude'],\n",
    "        raw_input_values['longitude'],\n",
    "        raw_input_values['hour'],\n",
    "        raw_input_values['day_of_week'],\n",
    "        raw_input_values['month']\n",
    "    )\n",
    "\n",
    "    # Print the resulting DataFrame to verify it's correctly structured\n",
    "    print(\"Preprocessed Data for Prediction:\")\n",
    "    print(ready_data)\n",
    "    print(\"\\nDataFrame structure is ready for the Random Forest model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1791a3f4-9972-471a-a04d-631202f83c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_limit             5000\n",
       "current_transactions             0\n",
       "transaction_interval    10 minutes\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's set your map key that was emailed to you. It should look something like 'abcdef1234567890abcdef1234567890'\n",
    "MAP_KEY = '2eaecfb3056b7b7751771485eb481c51'\n",
    "# MAP_KEY = 'abcdef0123456789abcdef1234567890'\n",
    "\n",
    "# now let's check how many transactions we have\n",
    "import pandas as pd\n",
    "import requests\n",
    "url = 'https://firms.modaps.eosdis.nasa.gov/mapserver/mapkey_status/?MAP_KEY=' + MAP_KEY\n",
    "try:\n",
    "  response = requests.get(url)\n",
    "  data = response.json()\n",
    "  df = pd.Series(data)\n",
    "  display(df)\n",
    "except:\n",
    "  # possible error, wrong MAP_KEY value, check for extra quotes, missing letters\n",
    "  print (\"There is an issue with the query. \\nTry in your browser: %s\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eca2a60-9722-4a0a-b855-ed5f31779652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our current transaction count is 0\n"
     ]
    }
   ],
   "source": [
    "# let's create a simple function that tells us how many transactions we have used.\n",
    "# We will use this in later examples\n",
    "\n",
    "def get_transaction_count() :\n",
    "  count = 0\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    df = pd.Series(data)\n",
    "    count = df['current_transactions']\n",
    "  except:\n",
    "    print (\"Error in our call.\")\n",
    "  return count\n",
    "\n",
    "tcount = get_transaction_count()\n",
    "print ('Our current transaction count is %i' % tcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6fb4da3-0aff-4b2a-b911-3a46b086b261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_id</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MODIS_NRT</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MODIS_SP</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>2025-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VIIRS_NOAA20_NRT</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VIIRS_NOAA20_SP</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2025-02-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIIRS_NOAA21_NRT</td>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VIIRS_SNPP_NRT</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VIIRS_SNPP_SP</td>\n",
       "      <td>2012-01-20</td>\n",
       "      <td>2025-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LANDSAT_NRT</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GOES_NRT</td>\n",
       "      <td>2022-08-09</td>\n",
       "      <td>2025-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BA_MODIS</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>2025-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BA_VIIRS</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>2025-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             data_id    min_date    max_date\n",
       "0          MODIS_NRT  2025-05-01  2025-07-31\n",
       "1           MODIS_SP  2000-11-01  2025-04-30\n",
       "2   VIIRS_NOAA20_NRT  2025-03-01  2025-07-31\n",
       "3    VIIRS_NOAA20_SP  2018-04-01  2025-02-28\n",
       "4   VIIRS_NOAA21_NRT  2024-01-17  2025-07-31\n",
       "5     VIIRS_SNPP_NRT  2025-04-01  2025-07-31\n",
       "6      VIIRS_SNPP_SP  2012-01-20  2025-03-31\n",
       "7        LANDSAT_NRT  2022-06-20  2025-07-31\n",
       "8           GOES_NRT  2022-08-09  2025-07-31\n",
       "9           BA_MODIS  2000-11-01  2025-05-01\n",
       "10          BA_VIIRS  2012-03-01  2025-04-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's query data_availability to find out what date range is available for various datasets\n",
    "# we will explain these datasets a bit later\n",
    "\n",
    "# this url will return information about all supported sensors and their corresponding datasets\n",
    "# instead of 'all' you can specify individual sensor, ex:LANDSAT_NRT\n",
    "da_url = 'https://firms.modaps.eosdis.nasa.gov/api/data_availability/csv/' + MAP_KEY + '/all'\n",
    "df = pd.read_csv(da_url)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015915aa-a53e-4bc8-804f-0d9c6709e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/YOUR_FIRMS_API_KEY/VIIRS_SNPP_NRT/74.5,8.0,77.5,12.5/2025-07-24/2025-07-31\n",
      "HTTP Error fetching FIRMS data: 500 Server Error: Internal Server Error for url: https://firms.modaps.eosdis.nasa.gov/api/area/csv/YOUR_FIRMS_API_KEY/VIIRS_SNPP_NRT/74.5,8.0,77.5,12.5/2025-07-24/2025-07-31\n",
      "Check your API key, bounding box, and FIRMS API documentation for the correct URL format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 22:39:35,039 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting ERA5-Land data for dates: 2025-07-29 to 2025-07-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 22:39:35,650 INFO Request ID is 1a8cf89b-82fc-4217-a0b2-6c050c621307\n",
      "2025-07-31 22:39:35,951 INFO status has been updated to accepted\n",
      "2025-07-31 22:39:58,896 INFO status has been updated to failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching ERA5-Land data: 400 Client Error: Bad Request for url: https://cds.climate.copernicus.eu/api/retrieve/v1/jobs/1a8cf89b-82fc-4217-a0b2-6c050c621307/results\n",
      "The job has failed\n",
      "None of the data you have requested is available yet, please revise the period requested. The latest date available for this dataset is: 2025-07-26 17:00\n",
      "None of the data you have requested is available yet, please revise the period requested. The latest date available for this dataset is: 2025-07-26 17:00\n",
      "The job failed with: MultiAdaptorNoDataError\n",
      "Ensure your .cdsapirc file is correctly configured in your home directory.\n",
      "Also, check your CDS account for any data download limits or issues.\n",
      "Verify the bounding box order and format for the CDS API.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Day 2: Data Acquisition Scripts\n",
    "# These scripts are designed to be run in a Jupyter Notebook (data_acquisition_day2.ipynb).\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import cdsapi\n",
    "import os\n",
    "\n",
    "# --- FIRMS Data Acquisition ---\n",
    "# This section handles fetching active fire hotspot data from the FIRMS API.\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key from the FIRMS website.\n",
    "FIRMS_API_KEY = \"YOUR_FIRMS_API_KEY\"\n",
    "\n",
    "# Bounding box for Kerala (approximate)\n",
    "# Format: min_lon,min_lat,max_lon,max_lat\n",
    "KERALA_BOUNDING_BOX = \"74.5,8.0,77.5,12.5\"\n",
    "\n",
    "# Source: 'VIIRS_SNPP_NRT' or 'VIIRS_NOAA20_NRT'\n",
    "# We'll use VIIRS_SNPP_NRT for this example.\n",
    "FIRMS_SOURCE = \"VIIRS_SNPP_NRT\"\n",
    "\n",
    "def fetch_firms_data(api_key, source, bbox, date_range_days=7):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and date range.\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "    \n",
    "    # FIRMS API date format is YYYY-MM-DD\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Construct the URL for the FIRMS 'area' archive endpoint.\n",
    "    # This is a robust pattern for historical data downloads via bounding box.\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox}/{start_date_str}/{end_date_str}\")\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    # FIRMS returns CSV data, which we read directly into a Pandas DataFrame\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    # Data is returned with a header, but let's check for empty data.\n",
    "    if data.empty:\n",
    "        print(\"No FIRMS data found for the specified period and area.\")\n",
    "        return pd.DataFrame() # Return an empty DataFrame\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "try:\n",
    "    firms_data = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE, KERALA_BOUNDING_BOX, date_range_days=7)\n",
    "    \n",
    "    if not firms_data.empty:\n",
    "        print(f\"Fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "        print(\"Raw FIRMS data head:\")\n",
    "        print(firms_data.head())\n",
    "        \n",
    "        # Member 1 needs 'latitude', 'longitude', 'acq_date', 'acq_time', 'frp'\n",
    "        required_firms_cols = ['latitude', 'longitude', 'acq_date', 'acq_time', 'frp']\n",
    "        if all(col in firms_data.columns for col in required_firms_cols):\n",
    "            print(\"\\nFIRMS data contains required columns. Performing basic processing...\")\n",
    "            \n",
    "            # Basic processing: combine date and time, convert to a proper timestamp\n",
    "            # Use zfill(4) to ensure the time is a 4-digit string (e.g., 800 -> 0800)\n",
    "            firms_data['timestamp'] = pd.to_datetime(\n",
    "                firms_data['acq_date'] + ' ' + firms_data['acq_time'].astype(str).str.zfill(4), \n",
    "                format='%Y-%m-%d %H%M'\n",
    "            )\n",
    "            \n",
    "            # Select and reorder relevant columns\n",
    "            processed_firms_data = firms_data[['timestamp', 'latitude', 'longitude', 'frp', 'confidence']]\n",
    "            print(\"\\nProcessed FIRMS data head:\")\n",
    "            print(processed_firms_data.head())\n",
    "            \n",
    "            # Save to a temporary CSV for inspection (optional)\n",
    "            processed_firms_data.to_csv(\"firms_data_last_7_days.csv\", index=False)\n",
    "            print(\"\\nFIRMS data saved to firms_data_last_7_days.csv\")\n",
    "        else:\n",
    "            print(\"FIRMS data missing some required columns. Check FIRMS API documentation.\")\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "    print(\"Check your API key, bounding box, and FIRMS API documentation for the correct URL format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Ensure your API key is correct and you have internet access.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- ERA5-Land Data Acquisition ---\n",
    "# This section handles fetching meteorological data from the CDS API using cdsapi.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "# Bounding box for Kerala in CDS API format: [North, West, South, East]\n",
    "KERALA_BOUNDING_BOX_CDS = [12.5, 74.5, 8.0, 77.5]\n",
    "\n",
    "def fetch_era5_land_data(bbox, date_range_days=2):\n",
    "    \"\"\"\n",
    "    Fetches ERA5-Land data for a given bounding box and date range.\n",
    "    bbox: [North, West, South, East]\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "\n",
    "    # Generate a list of dates for the request\n",
    "    # ERA5-Land requires a list of years, months, and days\n",
    "    dates = [start_date + timedelta(days=i) for i in range(date_range_days)]\n",
    "    years = sorted(list(set([d.strftime('%Y') for d in dates])))\n",
    "    months = sorted(list(set([d.strftime('%m') for d in dates])))\n",
    "    days = sorted(list(set([d.strftime('%d') for d in dates])))\n",
    "\n",
    "    # Define variables to fetch (based on AI/ML requirements)\n",
    "    variables = [\n",
    "        '2m_temperature',\n",
    "        '2m_dewpoint_temperature', # Useful for calculating relative humidity\n",
    "        'total_precipitation',\n",
    "        '10m_u_component_of_wind', # For wind speed/direction\n",
    "        '10m_v_component_of_wind', # For wind speed/direction\n",
    "    ]\n",
    "\n",
    "    # Define times to fetch (hourly data for detailed analysis)\n",
    "    times = [\n",
    "        '00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00',\n",
    "        '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00',\n",
    "        '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00',\n",
    "    ]\n",
    "\n",
    "    # Temporary file to store the downloaded NetCDF\n",
    "    output_file = 'era5_land_data.nc'\n",
    "    \n",
    "    print(f\"Requesting ERA5-Land data for dates: {dates[0].strftime('%Y-%m-%d')} to {dates[-1].strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        # Check if the output file already exists and remove it to avoid errors\n",
    "        if os.path.exists(output_file):\n",
    "            os.remove(output_file)\n",
    "            \n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-land',\n",
    "            {\n",
    "                'variable': variables,\n",
    "                'year': years,\n",
    "                'month': months,\n",
    "                'day': days,\n",
    "                'time': times,\n",
    "                'area': bbox, # [North, West, South, East]\n",
    "                'format': 'netcdf',\n",
    "            },\n",
    "            output_file\n",
    "        )\n",
    "        print(f\"\\nERA5-Land data downloaded to {output_file}\")\n",
    "\n",
    "        # Load the data using xarray\n",
    "        ds = xr.open_dataset(output_file)\n",
    "        print(\"\\nERA5-Land data loaded into xarray Dataset.\")\n",
    "        print(ds) # Print dataset info\n",
    "        \n",
    "        # --- Basic Processing (Example: Convert temperature from Kelvin to Celsius) ---\n",
    "        if '2m_temperature' in ds.data_vars:\n",
    "            ds['2m_temperature_c'] = ds['2m_temperature'] - 273.15\n",
    "            print(\"\\n2m_temperature converted to Celsius.\")\n",
    "        \n",
    "        # --- Example: Calculate wind speed from u/v components ---\n",
    "        if '10m_u_component_of_wind' in ds.data_vars and '10m_v_component_of_wind' in ds.data_vars:\n",
    "            ds['wind_speed'] = np.sqrt(ds['10m_u_component_of_wind']**2 + ds['10m_v_component_of_wind']**2)\n",
    "            print(\"Wind speed calculated.\")\n",
    "\n",
    "        # --- Example: Calculate relative humidity from temperature and dewpoint ---\n",
    "        if '2m_temperature_c' in ds.data_vars and '2m_dewpoint_temperature' in ds.data_vars:\n",
    "            # Convert dewpoint to Celsius first\n",
    "            ds['2m_dewpoint_temperature_c'] = ds['2m_dewpoint_temperature'] - 273.15\n",
    "            \n",
    "            # Formula: RH = 100 * (exp((17.625 * TD) / (243.04 + TD)) / exp((17.625 * T) / (243.04 + T)))\n",
    "            # Where T is dry bulb temp, TD is dew point temp, both in Celsius\n",
    "            e_s = 6.1094 * np.exp((17.625 * ds['2m_temperature_c']) / (243.04 + ds['2m_temperature_c']))\n",
    "            e_a = 6.1094 * np.exp((17.625 * ds['2m_dewpoint_temperature_c']) / (243.04 + ds['2m_dewpoint_temperature_c']))\n",
    "            ds['relative_humidity_percent'] = (e_a / e_s) * 100\n",
    "            \n",
    "            # Clip values to the valid range [0, 100]\n",
    "            ds['relative_humidity_percent'] = ds['relative_humidity_percent'].clip(0, 100)\n",
    "            print(\"Relative humidity calculated.\")\n",
    "\n",
    "        # Convert xarray Dataset to a flattened Pandas DataFrame for easier feature engineering later\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        \n",
    "        # Drop original Kelvin temps if Celsius is preferred, and the wind components\n",
    "        df = df.drop(columns=['2m_temperature', '2m_dewpoint_temperature', \n",
    "                              '10m_u_component_of_wind', '10m_v_component_of_wind'], errors='ignore')\n",
    "        print(\"\\nERA5-Land data converted to Pandas DataFrame.\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save to a temporary CSV for inspection (optional)\n",
    "        df.to_csv(\"era5_land_data_processed.csv\", index=False)\n",
    "        print(\"ERA5-Land processed data saved to era5_land_data_processed.csv\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching ERA5-Land data: {e}\")\n",
    "        print(\"Ensure your .cdsapirc file is correctly configured in your home directory.\")\n",
    "        print(\"Also, check your CDS account for any data download limits or issues.\")\n",
    "        print(\"Verify the bounding box order and format for the CDS API.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the ERA5-Land function ---\n",
    "era5_df = fetch_era5_land_data(KERALA_BOUNDING_BOX_CDS, date_range_days=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7365f809-38ba-49cf-a362-bb8b9fd6965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_SNPP_NRT/74.5,8.0,77.5,12.5/2025-07-24/2025-07-31\n",
      "HTTP Error fetching FIRMS data: 500 Server Error: Internal Server Error for url: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_SNPP_NRT/74.5,8.0,77.5,12.5/2025-07-24/2025-07-31\n",
      "Check your API key, bounding box, and FIRMS API documentation for the correct URL format.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import cdsapi\n",
    "import os\n",
    "\n",
    "# --- FIRMS Data Acquisition ---\n",
    "# This section handles fetching active fire hotspot data from the FIRMS API.\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key from the FIRMS website.\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# Bounding box for Kerala (approximate)\n",
    "# Format: min_lon,min_lat,max_lon,max_lat\n",
    "KERALA_BOUNDING_BOX = \"74.5,8.0,77.5,12.5\"\n",
    "\n",
    "# Source: 'VIIRS_SNPP_NRT' or 'VIIRS_NOAA20_NRT'\n",
    "# We'll use VIIRS_SNPP_NRT for this example.\n",
    "FIRMS_SOURCE = \"VIIRS_SNPP_NRT\"\n",
    "\n",
    "def fetch_firms_data(api_key, source, bbox, date_range_days=7):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a given bounding box and date range.\n",
    "    date_range_days: Number of days back from today to fetch data.\n",
    "    \"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=date_range_days)\n",
    "    \n",
    "    # FIRMS API date format is YYYY-MM-DD\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Construct the URL for the FIRMS 'area' archive endpoint.\n",
    "    # This is a robust pattern for historical data downloads via bounding box.\n",
    "    url = (f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{api_key}/{source}/\"\n",
    "           f\"{bbox}/{start_date_str}/{end_date_str}\")\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    # FIRMS returns CSV data, which we read directly into a Pandas DataFrame\n",
    "    data = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    # Data is returned with a header, but let's check for empty data.\n",
    "    if data.empty:\n",
    "        print(\"No FIRMS data found for the specified period and area.\")\n",
    "        return pd.DataFrame() # Return an empty DataFrame\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "try:\n",
    "    firms_data = fetch_firms_data(FIRMS_API_KEY, FIRMS_SOURCE, KERALA_BOUNDING_BOX, date_range_days=7)\n",
    "    \n",
    "    if not firms_data.empty:\n",
    "        print(f\"Fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "        print(\"Raw FIRMS data head:\")\n",
    "        print(firms_data.head())\n",
    "        \n",
    "        # Member 1 needs 'latitude', 'longitude', 'acq_date', 'acq_time', 'frp'\n",
    "        required_firms_cols = ['latitude', 'longitude', 'acq_date', 'acq_time', 'frp']\n",
    "        if all(col in firms_data.columns for col in required_firms_cols):\n",
    "            print(\"\\nFIRMS data contains required columns. Performing basic processing...\")\n",
    "            \n",
    "            # Basic processing: combine date and time, convert to a proper timestamp\n",
    "            # Use zfill(4) to ensure the time is a 4-digit string (e.g., 800 -> 0800)\n",
    "            firms_data['timestamp'] = pd.to_datetime(\n",
    "                firms_data['acq_date'] + ' ' + firms_data['acq_time'].astype(str).str.zfill(4), \n",
    "                format='%Y-%m-%d %H%M'\n",
    "            )\n",
    "            \n",
    "            # Select and reorder relevant columns\n",
    "            processed_firms_data = firms_data[['timestamp', 'latitude', 'longitude', 'frp', 'confidence']]\n",
    "            print(\"\\nProcessed FIRMS data head:\")\n",
    "            print(processed_firms_data.head())\n",
    "            \n",
    "            # Save to a temporary CSV for inspection (optional)\n",
    "            processed_firms_data.to_csv(\"firms_data_last_7_days.csv\", index=False)\n",
    "            print(\"\\nFIRMS data saved to firms_data_last_7_days.csv\")\n",
    "        else:\n",
    "            print(\"FIRMS data missing some required columns. Check FIRMS API documentation.\")\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "    print(\"Check your API key, bounding box, and FIRMS API documentation for the correct URL format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Ensure your API key is correct and you have internet access.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70952fb-ace2-47b4-8979-20e3f5b14415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_NOAA20_NRT/world/1\n",
      "\n",
      "Successfully fetched 32410 FIRMS hotspots.\n",
      "FIRMS data head:\n",
      "   latitude  longitude  bright_ti4  scan  track    acq_date  acq_time  \\\n",
      "0  19.40509 -155.27338      367.00  0.56   0.69  2025-07-31        22   \n",
      "1  19.40793 -155.27521      355.45  0.56   0.69  2025-07-31        22   \n",
      "2  19.40839 -155.26984      351.72  0.56   0.69  2025-07-31        22   \n",
      "3  19.41144 -155.27393      352.36  0.56   0.69  2025-07-31        22   \n",
      "4  56.05059  160.64233      356.53  0.42   0.61  2025-07-31        35   \n",
      "\n",
      "  satellite instrument confidence version  bright_ti5    frp daynight  \n",
      "0       N20      VIIRS          h  2.0NRT      329.90  21.45        D  \n",
      "1       N20      VIIRS          l  2.0NRT      330.36  41.23        D  \n",
      "2       N20      VIIRS          n  2.0NRT      319.27  26.26        D  \n",
      "3       N20      VIIRS          n  2.0NRT      325.09  21.45        D  \n",
      "4       N20      VIIRS          l  2.0NRT      283.98  22.27        D  \n",
      "\n",
      "FIRMS data saved to firms_global_recent_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Simple FIRMS API Request\n",
    "# This script fetches the most recent day of VIIRS NOAA-20 data for the entire world.\n",
    "# This is a good way to test the API connection and verify your key.\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key from the FIRMS website.\n",
    "# The key '2eaecfb3056b7b7751771485eb481c51' is a placeholder and will not work.\n",
    "# Make sure you have a valid key from https://nrt3.modaps.eosdis.nasa.gov/api/activate\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\" \n",
    "\n",
    "# Base URL for the FIRMS API\n",
    "BASE_URL = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "# We'll use VIIRS NOAA-20, which is known for its high resolution.\n",
    "FIRMS_SOURCE = 'VIIRS_NOAA20_NRT'\n",
    "\n",
    "# Region and date range: 'world' and '1' for the most recent 24 hours.\n",
    "FIRMS_REGION = 'world'\n",
    "DATE_RANGE = '1'\n",
    "\n",
    "def fetch_firms_global_data(api_key, source, region, date_range):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a specific region and date range.\n",
    "    This uses a simplified API endpoint for quick access to recent data.\n",
    "    \"\"\"\n",
    "    # Construct the URL\n",
    "    url = f\"{BASE_URL}{api_key}/{source}/{region}/{date_range}\"\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    \n",
    "    # Check if the API key is a placeholder\n",
    "    if api_key == \"YOUR_FIRMS_API_KEY\":\n",
    "        print(\"\\nERROR: You must replace 'YOUR_FIRMS_API_KEY' with your actual key.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # FIRMS returns CSV data, which we read directly into a Pandas DataFrame\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No FIRMS data found for the specified period and area.\")\n",
    "            return pd.DataFrame() # Return an empty DataFrame\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "        print(\"Check your API key and the URL format. If the key is correct, the server might be experiencing issues.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"Ensure you have a valid API key and internet access.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "# The get_transaction_count() function is not a standard library function.\n",
    "# It would be a custom function in a real-world scenario to track usage.\n",
    "# For this example, we'll just call the data fetching function directly.\n",
    "firms_global_data = fetch_firms_global_data(FIRMS_API_KEY, FIRMS_SOURCE, FIRMS_REGION, DATE_RANGE)\n",
    "\n",
    "if firms_global_data is not None and not firms_global_data.empty:\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_global_data)} FIRMS hotspots.\")\n",
    "    print(\"FIRMS data head:\")\n",
    "    print(firms_global_data.head())\n",
    "    \n",
    "    # Save to a temporary CSV for inspection (optional)\n",
    "    firms_global_data.to_csv(\"firms_global_recent_data.csv\", index=False)\n",
    "    print(\"\\nFIRMS data saved to firms_global_recent_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37c69e9-333c-4a33-a154-b2bd46f4131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_NOAA20_NRT/74.5,8.0,77.5,13.0/2025-07-30,2025-07-31\n",
      "No FIRMS data found for the specified period and area.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Specific FIRMS API Request for Kerala, India\n",
    "# This script fetches active fire data for the state of Kerala, India,\n",
    "# and a specific date range using the FIRMS API.\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key.\n",
    "# The key '2eaecfb3056b7b7751771485eb481c51' is a placeholder.\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# Base URL for the FIRMS API\n",
    "BASE_URL = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "# We'll use VIIRS NOAA-20 for high-resolution data.\n",
    "FIRMS_SOURCE = 'VIIRS_NOAA20_NRT'\n",
    "\n",
    "# Define the bounding box for Kerala, India.\n",
    "# Format: lon_min,lat_min,lon_max,lat_max\n",
    "# Coordinates for Kerala: ~74.5°E, 8.0°N to 77.5°E, 13.0°N\n",
    "BOUNDING_BOX = \"74.5,8.0,77.5,13.0\"\n",
    "\n",
    "# Define the date range. The API expects YYYY-MM-DD.\n",
    "START_DATE = '2025-07-30'\n",
    "END_DATE = '2025-07-31'\n",
    "DATE_RANGE = f\"{START_DATE},{END_DATE}\"\n",
    "\n",
    "def fetch_firms_data_with_bbox(api_key, source, bbox, date_range):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a specific bounding box and date range.\n",
    "    \"\"\"\n",
    "    # Construct the URL with the bounding box and date range parameters.\n",
    "    url = f\"{BASE_URL}{api_key}/{source}/{bbox}/{date_range}\"\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    \n",
    "    # Check for placeholder key\n",
    "    if api_key == \"YOUR_FIRMS_API_KEY\":\n",
    "        print(\"\\nERROR: You must replace 'YOUR_FIRMS_API_KEY' with your actual key.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # FIRMS returns CSV data, which we read into a Pandas DataFrame\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No FIRMS data found for the specified period and area.\")\n",
    "            return pd.DataFrame() # Return an empty DataFrame\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "        print(\"This could be due to an invalid key, incorrect URL format, or server issues.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"Ensure you have a valid API key and internet access.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "firms_data = fetch_firms_data_with_bbox(FIRMS_API_KEY, FIRMS_SOURCE, BOUNDING_BOX, DATE_RANGE)\n",
    "\n",
    "if firms_data is not None and not firms_data.empty:\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"FIRMS data head:\")\n",
    "    print(firms_data.head())\n",
    "    \n",
    "    # Save to a temporary CSV for inspection (optional)\n",
    "    firms_data.to_csv(\"firms_kerala_data.csv\", index=False)\n",
    "    print(f\"\\nFIRMS data for {BOUNDING_BOX} saved to firms_kerala_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3b958b-49a6-4b7a-bbe5-880c4ec36af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_NOAA20_NRT/-124.5,32.5,-114.0,42.0/2025-07-30,2025-07-31\n",
      "No FIRMS data found for the specified period and area.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Updated FIRMS API Request for California, USA\n",
    "# This script fetches active fire data for California and a recent date range.\n",
    "# This demonstrates a successful data retrieval for an area with known fire activity.\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key.\n",
    "# The key '2eaecfb3056b7b7751771485eb481c51' is a placeholder.\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# Base URL for the FIRMS API\n",
    "BASE_URL = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "# We'll use VIIRS NOAA-20 for high-resolution data.\n",
    "FIRMS_SOURCE = 'VIIRS_NOAA20_NRT'\n",
    "\n",
    "# Define the bounding box for California, USA.\n",
    "# Format: lon_min,lat_min,lon_max,lat_max\n",
    "BOUNDING_BOX = \"-124.5,32.5,-114.0,42.0\"\n",
    "\n",
    "# Define the date range. The API expects YYYY-MM-DD.\n",
    "START_DATE = '2025-07-30'\n",
    "END_DATE = '2025-07-31'\n",
    "DATE_RANGE = f\"{START_DATE},{END_DATE}\"\n",
    "\n",
    "def fetch_firms_data_with_bbox(api_key, source, bbox, date_range):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a specific bounding box and date range.\n",
    "    \"\"\"\n",
    "    # Construct the URL with the bounding box and date range parameters.\n",
    "    url = f\"{BASE_URL}{api_key}/{source}/{bbox}/{date_range}\"\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    \n",
    "    # Check for placeholder key\n",
    "    if api_key == \"YOUR_FIRMS_API_KEY\":\n",
    "        print(\"\\nERROR: You must replace 'YOUR_FIRMS_API_KEY' with your actual key.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # FIRMS returns CSV data, which we read into a Pandas DataFrame\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No FIRMS data found for the specified period and area.\")\n",
    "            return pd.DataFrame() # Return an empty DataFrame\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "        print(\"This could be due to an invalid key, incorrect URL format, or server issues.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"Ensure you have a valid API key and internet access.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "firms_data = fetch_firms_data_with_bbox(FIRMS_API_KEY, FIRMS_SOURCE, BOUNDING_BOX, DATE_RANGE)\n",
    "\n",
    "if firms_data is not None and not firms_data.empty:\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"FIRMS data head:\")\n",
    "    print(firms_data.head())\n",
    "    \n",
    "    # Save to a temporary CSV for inspection (optional)\n",
    "    firms_data.to_csv(\"firms_ca_data.csv\", index=False)\n",
    "    print(f\"\\nFIRMS data for {BOUNDING_BOX} saved to firms_ca_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84427d7-6a15-4faa-a642-2f30da674c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_NOAA20_NRT/-124.5,32.5,-114.0,42.0/2024-07-30,2024-07-31\n",
      "No FIRMS data found for the specified period and area.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Corrected FIRMS API Request for California, USA\n",
    "# This script fetches active fire data for California from a valid, past date range.\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key.\n",
    "# The placeholder key '2eaecfb3056b7b7751771485eb481c51' will NOT work.\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# Base URL for the FIRMS API\n",
    "BASE_URL = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "# We'll use VIIRS NOAA-20 for high-resolution data.\n",
    "FIRMS_SOURCE = 'VIIRS_NOAA20_NRT'\n",
    "\n",
    "# Define the bounding box for California, USA.\n",
    "# Format: lon_min,lat_min,lon_max,lat_max\n",
    "BOUNDING_BOX = \"-124.5,32.5,-114.0,42.0\"\n",
    "\n",
    "# --- Corrected Date Range ---\n",
    "# We are now using a date range from last year where fire data is available.\n",
    "START_DATE = '2024-07-30'\n",
    "END_DATE = '2024-07-31'\n",
    "DATE_RANGE = f\"{START_DATE},{END_DATE}\"\n",
    "\n",
    "def fetch_firms_data_with_bbox(api_key, source, bbox, date_range):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a specific bounding box and date range.\n",
    "    \"\"\"\n",
    "    # Construct the URL with the bounding box and date range parameters.\n",
    "    url = f\"{BASE_URL}{api_key}/{source}/{bbox}/{date_range}\"\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    \n",
    "    # Check for placeholder key and warn the user\n",
    "    if api_key == \"YOUR_FIRMS_API_KEY\":\n",
    "        print(\"\\nERROR: You must replace 'YOUR_FIRMS_API_KEY' with your actual key.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # FIRMS returns CSV data, which we read into a Pandas DataFrame\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No FIRMS data found for the specified period and area.\")\n",
    "            return pd.DataFrame() # Return an empty DataFrame\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "        print(\"This could be due to an invalid key, incorrect URL format, or server issues.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"Ensure you have a valid API key and internet access.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "firms_data = fetch_firms_data_with_bbox(FIRMS_API_KEY, FIRMS_SOURCE, BOUNDING_BOX, DATE_RANGE)\n",
    "\n",
    "if firms_data is not None and not firms_data.empty:\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"FIRMS data head:\")\n",
    "    print(firms_data.head())\n",
    "    \n",
    "    # Save to a temporary CSV for inspection (optional)\n",
    "    firms_data.to_csv(\"firms_ca_data.csv\", index=False)\n",
    "    print(f\"\\nFIRMS data for {BOUNDING_BOX} saved to firms_ca_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aa48b6c-72d1-42f7-ab85-b751b4a5c945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FIRMS data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_NOAA20_NRT/-124.5,32.5,-114.0,42.0/2024-07-30,2024-07-31\n",
      "No FIRMS data found for the specified period and area.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FIRMS API Request with a placeholder for your valid API key.\n",
    "# This script fetches active fire data for California from a valid, past date range.\n",
    "# ==============================================================================\n",
    "\n",
    "# General imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_FIRMS_API_KEY\" with your actual key.\n",
    "# This key is required for the NASA API to function.\n",
    "FIRMS_API_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# Base URL for the FIRMS API\n",
    "BASE_URL = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "\n",
    "# We'll use VIIRS NOAA-20 for high-resolution data.\n",
    "FIRMS_SOURCE = 'VIIRS_NOAA20_NRT'\n",
    "\n",
    "# Define the bounding box for California, USA.\n",
    "# Format: lon_min,lat_min,lon_max,lat_max\n",
    "BOUNDING_BOX = \"-124.5,32.5,-114.0,42.0\"\n",
    "\n",
    "# --- Corrected Date Range ---\n",
    "# Using a past date range from last year where fire data is available.\n",
    "START_DATE = '2024-07-30'\n",
    "END_DATE = '2024-07-31'\n",
    "DATE_RANGE = f\"{START_DATE},{END_DATE}\"\n",
    "\n",
    "def fetch_firms_data_with_bbox(api_key, source, bbox, date_range):\n",
    "    \"\"\"\n",
    "    Fetches FIRMS active fire data for a specific bounding box and date range.\n",
    "    \"\"\"\n",
    "    # Construct the URL with the bounding box and date range parameters.\n",
    "    url = f\"{BASE_URL}{api_key}/{source}/{bbox}/{date_range}\"\n",
    "    \n",
    "    print(f\"Fetching FIRMS data from: {url}\")\n",
    "    \n",
    "    # Check for placeholder key and warn the user\n",
    "    if api_key == \"YOUR_FIRMS_API_KEY\":\n",
    "        print(\"\\nERROR: You must replace 'YOUR_FIRMS_API_KEY' with your actual key.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # FIRMS returns CSV data, which we read into a Pandas DataFrame\n",
    "        data = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        if data.empty:\n",
    "            print(\"No FIRMS data found for the specified period and area.\")\n",
    "            return pd.DataFrame() # Return an empty DataFrame\n",
    "        \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching FIRMS data: {e}\")\n",
    "        print(\"This could be due to an invalid key, incorrect URL format, or server issues.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"Ensure you have a valid API key and internet access.\")\n",
    "        return None\n",
    "\n",
    "# --- Test the FIRMS function ---\n",
    "firms_data = fetch_firms_data_with_bbox(FIRMS_API_KEY, FIRMS_SOURCE, BOUNDING_BOX, DATE_RANGE)\n",
    "\n",
    "if firms_data is not None and not firms_data.empty:\n",
    "    print(f\"\\nSuccessfully fetched {len(firms_data)} FIRMS hotspots.\")\n",
    "    print(\"FIRMS data head:\")\n",
    "    print(firms_data.head())\n",
    "    \n",
    "    # Save to a temporary CSV for inspection (optional)\n",
    "    firms_data.to_csv(\"firms_ca_data.csv\", index=False)\n",
    "    print(f\"\\nFIRMS data for {BOUNDING_BOX} saved to firms_ca_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85975ea2-4185-49c4-b3ac-57029adff48d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1982260478.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    top: 2rem;\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>FIRMS API Key Tester</title>\n",
    "    <script src=\"https://cdn.tailwindcss.com\"></script>\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "            background-color: #f3f4f6;\n",
    "        }\n",
    "        .message-box {\n",
    "            position: fixed;\n",
    "            top: 2rem;\n",
    "            right: 2rem;\n",
    "            max-width: 300px;\n",
    "            background-color: #fff;\n",
    "            border-left: 4px solid;\n",
    "            padding: 1rem;\n",
    "            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "            border-radius: 0.5rem;\n",
    "            transition: transform 0.3s ease-in-out, opacity 0.3s ease-in-out;\n",
    "            transform: translateX(110%);\n",
    "            opacity: 0;\n",
    "            z-index: 1000;\n",
    "        }\n",
    "        .message-box.show {\n",
    "            transform: translateX(0);\n",
    "            opacity: 1;\n",
    "        }\n",
    "        .status-success { border-color: #10b981; }\n",
    "        .status-error { border-color: #ef4444; }\n",
    "        .status-warning { border-color: #f59e0b; }\n",
    "    </style>\n",
    "</head>\n",
    "<body class=\"bg-gray-100 flex items-center justify-center min-h-screen p-4\">\n",
    "\n",
    "    <div class=\"bg-white rounded-lg shadow-xl p-8 max-w-lg w-full\">\n",
    "        <h1 class=\"text-3xl font-bold text-center text-gray-800 mb-6\">FIRMS Key Tester</h1>\n",
    "        <p class=\"text-gray-600 mb-6 text-center\">\n",
    "            Enter your FIRMS `MAP_KEY` and click the button to test if it's working and check the API status.\n",
    "        </p>\n",
    "\n",
    "        <!-- Input Section -->\n",
    "        <div class=\"mb-4\">\n",
    "            <label for=\"apiKeyInput\" class=\"block text-gray-700 font-semibold mb-2\">Your FIRMS MAP_KEY</label>\n",
    "            <input type=\"text\" id=\"apiKeyInput\" placeholder=\"Enter your key here...\" class=\"w-full px-4 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500\">\n",
    "        </div>\n",
    "        <div class=\"mb-6\">\n",
    "            <label for=\"dayRangeInput\" class=\"block text-gray-700 font-semibold mb-2\">Day Range (1-10)</label>\n",
    "            <input type=\"number\" id=\"dayRangeInput\" value=\"1\" min=\"1\" max=\"10\" class=\"w-full px-4 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500\">\n",
    "        </div>\n",
    "\n",
    "        <!-- Button -->\n",
    "        <button id=\"testButton\" class=\"w-full bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-4 rounded-md shadow-lg transition-colors duration-200\">\n",
    "            Test Key\n",
    "        </button>\n",
    "\n",
    "        <!-- Status and Data Display -->\n",
    "        <div id=\"resultContainer\" class=\"mt-8 p-6 bg-gray-50 rounded-lg border border-gray-200 hidden\">\n",
    "            <h2 class=\"text-xl font-bold text-gray-800 mb-4\">API Response</h2>\n",
    "            <div id=\"statusMessage\" class=\"font-medium text-lg mb-2\"></div>\n",
    "            <pre id=\"responseData\" class=\"bg-gray-200 p-4 rounded-md text-sm overflow-auto max-h-64\"></pre>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <!-- Message Box for Notifications -->\n",
    "    <div id=\"messageBox\" class=\"message-box\">\n",
    "        <p id=\"messageText\" class=\"font-medium\"></p>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        // DOM element references\n",
    "        const apiKeyInput = document.getElementById('apiKeyInput');\n",
    "        const dayRangeInput = document.getElementById('dayRangeInput');\n",
    "        const testButton = document.getElementById('testButton');\n",
    "        const resultContainer = document.getElementById('resultContainer');\n",
    "        const statusMessage = document.getElementById('statusMessage');\n",
    "        const responseData = document.getElementById('responseData');\n",
    "        const messageBox = document.getElementById('messageBox');\n",
    "        const messageText = document.getElementById('messageText');\n",
    "\n",
    "        /**\n",
    "         * Displays a temporary message box notification.\n",
    "         * @param {string} message The message to display.\n",
    "         * @param {string} type The type of message (success, error, warning).\n",
    "         */\n",
    "        const showMessage = (message, type) => {\n",
    "            messageText.textContent = message;\n",
    "            messageBox.className = `message-box show status-${type}`;\n",
    "            setTimeout(() => {\n",
    "                messageBox.classList.remove('show');\n",
    "            }, 5000);\n",
    "        };\n",
    "\n",
    "        testButton.addEventListener('click', async () => {\n",
    "            const apiKey = apiKeyInput.value.trim();\n",
    "            const dayRange = dayRangeInput.value;\n",
    "            const source = 'VIIRS_SNPP_NRT'; // Default source, can be changed\n",
    "            const area = 'world'; // Default area for testing\n",
    "            const url = `https://firms.modaps.eosdis.nasa.gov/api/area/csv/${apiKey}/${source}/${area}/${dayRange}`;\n",
    "\n",
    "            if (!apiKey) {\n",
    "                showMessage(\"Please enter your API key.\", \"error\");\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            testButton.textContent = 'Testing...';\n",
    "            testButton.disabled = true;\n",
    "            resultContainer.classList.add('hidden');\n",
    "\n",
    "            try {\n",
    "                const response = await fetch(url);\n",
    "                \n",
    "                // Show the result container after the fetch\n",
    "                resultContainer.classList.remove('hidden');\n",
    "\n",
    "                if (response.ok) {\n",
    "                    const data = await response.text();\n",
    "                    statusMessage.textContent = 'Success! Your key is working.';\n",
    "                    statusMessage.className = 'font-medium text-lg text-green-600 mb-2';\n",
    "                    responseData.textContent = data.slice(0, 500) + '\\n\\n... (Data truncated for display)';\n",
    "                    showMessage(\"Successfully fetched data.\", \"success\");\n",
    "                } else {\n",
    "                    const errorText = await response.text();\n",
    "                    statusMessage.className = 'font-medium text-lg text-red-600 mb-2';\n",
    "                    \n",
    "                    // Handle specific error codes\n",
    "                    if (response.status === 429) {\n",
    "                        statusMessage.textContent = 'Error: Rate Limit Exceeded (429)';\n",
    "                        responseData.textContent = 'You have made too many requests. The FIRMS API has a limit of 5,000 transactions per 10-minute interval. Please wait and try again later.';\n",
    "                        showMessage(\"Rate limit exceeded. Please wait.\", \"warning\");\n",
    "                    } else if (response.status === 403) {\n",
    "                        statusMessage.textContent = 'Error: Forbidden (403)';\n",
    "                        responseData.textContent = 'The API key provided is not valid or does not have permission. Please check your key for typos or request a new one from FIRMS.';\n",
    "                        showMessage(\"Invalid API key. Check for typos.\", \"error\");\n",
    "                    } else {\n",
    "                        statusMessage.textContent = `Error: ${response.status} ${response.statusText}`;\n",
    "                        responseData.textContent = errorText;\n",
    "                        showMessage(`API responded with an error: ${response.status}`, \"error\");\n",
    "                    }\n",
    "                }\n",
    "            } catch (error) {\n",
    "                statusMessage.textContent = 'Error: An unexpected error occurred.';\n",
    "                statusMessage.className = 'font-medium text-lg text-red-600 mb-2';\n",
    "                responseData.textContent = error.toString();\n",
    "                showMessage(\"An unexpected network error occurred.\", \"error\");\n",
    "                console.error(error);\n",
    "            } finally {\n",
    "                testButton.textContent = 'Test Key';\n",
    "                testButton.disabled = false;\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55308195-ae85-47cf-a4dc-b37070bf4595",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (500140624.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 25\u001b[1;36m\u001b[0m\n\u001b[1;33m    65.62944,147.48901,333.06,0.45,0.63,2025-07-31,13,N,VIIRS,n,2.0NRT,298.21,12.32,D\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "import React, { useState } from 'react';\n",
    "import {\n",
    "  Table,\n",
    "  TableBody,\n",
    "  TableCell,\n",
    "  TableHead,\n",
    "  TableHeader,\n",
    "  TableRow,\n",
    "} from \"shadcn/ui\";\n",
    "import {\n",
    "  Card,\n",
    "  CardContent,\n",
    "  CardHeader,\n",
    "  CardTitle,\n",
    "  CardDescription\n",
    "} from \"shadcn/ui\";\n",
    "import { Textarea } from \"shadcn/ui\";\n",
    "import { Button } from \"shadcn/ui\";\n",
    "\n",
    "// Define the main App component\n",
    "function App() {\n",
    "  // State to hold the raw CSV text from the textarea\n",
    "  const [csvText, setCsvText] = useState(\n",
    "    `latitude,longitude,bright_ti4,scan,track,acq_date,acq_time,satellite,instrument,confidence,version,bright_ti5,frp,daynight\n",
    "65.62944,147.48901,333.06,0.45,0.63,2025-07-31,13,N,VIIRS,n,2.0NRT,298.21,12.32,D\n",
    "65.63281,147.49579,367,0.45,0.63,2025-07-31,13,N,VIIRS,h,2.0NRT,298.07,10.08,D\n",
    "65.63768,147.48698,352.09,0.45,0.63,2025-07-31,13,N,VIIRS,n,2.0NRT,297.46,10.08,D\n",
    "66.11572,152.64148,327.93,0.34,0.56,2025-07-31,13,N,VIIRS,n,2.0NRT,296.66,6.21,D\n",
    "66.11675,152.63954,352.49,0.34,0.56,2025-07-31,13,N,V\n",
    "`\n",
    "  );\n",
    "  // State to hold the parsed data as an array of objects\n",
    "  const [data, setData] = useState([]);\n",
    "  // State to hold the column headers\n",
    "  const [headers, setHeaders] = useState([]);\n",
    "  // State to hold any error messages\n",
    "  const [error, setError] = useState('');\n",
    "\n",
    "  // Column descriptions for the explanatory section\n",
    "  const columnDescriptions = [\n",
    "    { name: 'latitude', description: 'The latitude of the fire detection.' },\n",
    "    { name: 'longitude', description: 'The longitude of the fire detection.' },\n",
    "    { name: 'bright_ti4', description: 'The brightness temperature in Kelvin of the T-4 channel (3.75 µm).' },\n",
    "    { name: 'bright_ti5', description: 'The brightness temperature in Kelvin of the T-5 channel (11 µm).' },\n",
    "    { name: 'confidence', description: 'The confidence level of the fire detection (low, nominal, high).' },\n",
    "    { name: 'frp', description: 'Fire Radiative Power in Megawatts (MW). This indicates the intensity of the fire.' },\n",
    "    { name: 'daynight', description: 'Whether the detection occurred during the day (D) or night (N).' },\n",
    "  ];\n",
    "\n",
    "  // Function to parse the CSV data\n",
    "  const parseCsv = () => {\n",
    "    setError(''); // Clear previous errors\n",
    "    if (!csvText) {\n",
    "      setError('Please paste some CSV data.');\n",
    "      return;\n",
    "    }\n",
    "\n",
    "    try {\n",
    "      const lines = csvText.trim().split('\\n');\n",
    "      if (lines.length < 2) {\n",
    "        setError('Please provide at least a header and one row of data.');\n",
    "        return;\n",
    "      }\n",
    "\n",
    "      // Split the header line by comma to get column names\n",
    "      const newHeaders = lines[0].split(',').map(h => h.trim());\n",
    "      setHeaders(newHeaders);\n",
    "\n",
    "      // Process each data line\n",
    "      const newData = lines.slice(1).map(line => {\n",
    "        // Handle truncated lines by stopping the parsing\n",
    "        if (line.split(',').length < newHeaders.length) {\n",
    "          // You could also ignore incomplete lines or add an error, this approach just stops\n",
    "          return null; \n",
    "        }\n",
    "        const values = line.split(',');\n",
    "        const rowObject = {};\n",
    "        newHeaders.forEach((header, index) => {\n",
    "          rowObject[header] = values[index] ? values[index].trim() : '';\n",
    "        });\n",
    "        return rowObject;\n",
    "      }).filter(Boolean); // Remove any null values from incomplete lines\n",
    "\n",
    "      setData(newData);\n",
    "    } catch (e) {\n",
    "      setError('Failed to parse CSV. Please check the format.');\n",
    "      console.error(e);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // Main component render\n",
    "  return (\n",
    "    <div className=\"min-h-screen bg-gray-100 dark:bg-gray-900 text-gray-900 dark:text-gray-100 p-8 flex flex-col items-center font-sans\">\n",
    "      <Card className=\"w-full max-w-4xl shadow-lg rounded-xl overflow-hidden mb-8 bg-white dark:bg-gray-800\">\n",
    "        <CardHeader className=\"bg-blue-600 dark:bg-blue-800 text-white p-6 rounded-t-xl\">\n",
    "          <CardTitle className=\"text-3xl font-bold\">VIIRS Fire Data Viewer</CardTitle>\n",
    "          <CardDescription className=\"text-blue-100 dark:text-blue-200 mt-2\">Paste your satellite data below to view it in a structured table.</CardDescription>\n",
    "        </CardHeader>\n",
    "        <CardContent className=\"p-6 space-y-6\">\n",
    "          <Textarea\n",
    "            value={csvText}\n",
    "            onChange={(e) => setCsvText(e.target.value)}\n",
    "            placeholder=\"Paste your CSV data here...\"\n",
    "            rows={10}\n",
    "            className=\"w-full rounded-md border-gray-300 dark:border-gray-600 bg-gray-50 dark:bg-gray-700 focus:ring-blue-500 focus:border-blue-500 text-gray-900 dark:text-gray-100\"\n",
    "          />\n",
    "          <div className=\"flex justify-center\">\n",
    "            <Button\n",
    "              onClick={parseCsv}\n",
    "              className=\"bg-blue-600 hover:bg-blue-700 text-white font-bold py-2 px-6 rounded-full shadow-lg transition duration-200 transform hover:scale-105\"\n",
    "            >\n",
    "              Process Data\n",
    "            </Button>\n",
    "          </div>\n",
    "          {error && (\n",
    "            <div className=\"text-red-500 text-center mt-4\">\n",
    "              {error}\n",
    "            </div>\n",
    "          )}\n",
    "        </CardContent>\n",
    "      </Card>\n",
    "\n",
    "      {data.length > 0 && (\n",
    "        <Card className=\"w-full max-w-4xl shadow-lg rounded-xl overflow-hidden mt-8 bg-white dark:bg-gray-800\">\n",
    "          <CardHeader className=\"bg-gray-200 dark:bg-gray-700 p-4 border-b dark:border-gray-600\">\n",
    "            <CardTitle className=\"text-2xl font-semibold\">Fire Detections</CardTitle>\n",
    "          </CardHeader>\n",
    "          <CardContent className=\"p-0 overflow-x-auto\">\n",
    "            <Table>\n",
    "              <TableHeader>\n",
    "                <TableRow className=\"bg-gray-100 dark:bg-gray-700\">\n",
    "                  {headers.map(header => (\n",
    "                    <TableHead key={header} className=\"whitespace-nowrap font-bold text-gray-600 dark:text-gray-300\">\n",
    "                      {header}\n",
    "                    </TableHead>\n",
    "                  ))}\n",
    "                </TableRow>\n",
    "              </TableHeader>\n",
    "              <TableBody>\n",
    "                {data.map((row, rowIndex) => (\n",
    "                  <TableRow key={rowIndex} className=\"hover:bg-gray-50 dark:hover:bg-gray-700 transition-colors\">\n",
    "                    {headers.map(header => (\n",
    "                      <TableCell key={`${rowIndex}-${header}`} className=\"whitespace-nowrap text-sm text-gray-700 dark:text-gray-300\">\n",
    "                        {row[header]}\n",
    "                      </TableCell>\n",
    "                    ))}\n",
    "                  </TableRow>\n",
    "                ))}\n",
    "              </TableBody>\n",
    "            </Table>\n",
    "          </CardContent>\n",
    "        </Card>\n",
    "      )}\n",
    "\n",
    "      <Card className=\"w-full max-w-4xl shadow-lg rounded-xl overflow-hidden mt-8 bg-white dark:bg-gray-800\">\n",
    "        <CardHeader className=\"bg-gray-200 dark:bg-gray-700 p-4 border-b dark:border-gray-600\">\n",
    "          <CardTitle className=\"text-2xl font-semibold\">Column Descriptions</CardTitle>\n",
    "          <CardDescription className=\"text-gray-600 dark:text-gray-400\">\n",
    "            A quick guide to understanding the data columns.\n",
    "          </CardDescription>\n",
    "        </CardHeader>\n",
    "        <CardContent className=\"p-6 space-y-4\">\n",
    "          <ul className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n",
    "            {columnDescriptions.map((col, index) => (\n",
    "              <li key={index} className=\"p-4 bg-gray-50 dark:bg-gray-700 rounded-md shadow-sm\">\n",
    "                <strong className=\"text-blue-600 dark:text-blue-400\">{col.name}:</strong> {col.description}\n",
    "              </li>\n",
    "            ))}\n",
    "          </ul>\n",
    "        </CardContent>\n",
    "      </Card>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba228b0-92e2-40ee-987e-823eea94c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed VIIRS Fire Detections:\n",
      "{'latitude': '65.62944', 'longitude': '147.48901', 'bright_ti4': '333.06', 'scan': '0.45', 'track': '0.63', 'acq_date': '2025-07-31', 'acq_time': '13', 'satellite': 'N', 'instrument': 'VIIRS', 'confidence': 'n', 'version': '2.0NRT', 'bright_ti5': '298.21', 'frp': '12.32', 'daynight': 'D'}\n",
      "{'latitude': '65.63281', 'longitude': '147.49579', 'bright_ti4': '367', 'scan': '0.45', 'track': '0.63', 'acq_date': '2025-07-31', 'acq_time': '13', 'satellite': 'N', 'instrument': 'VIIRS', 'confidence': 'h', 'version': '2.0NRT', 'bright_ti5': '298.07', 'frp': '10.08', 'daynight': 'D'}\n",
      "{'latitude': '65.63768', 'longitude': '147.48698', 'bright_ti4': '352.09', 'scan': '0.45', 'track': '0.63', 'acq_date': '2025-07-31', 'acq_time': '13', 'satellite': 'N', 'instrument': 'VIIRS', 'confidence': 'n', 'version': '2.0NRT', 'bright_ti5': '297.46', 'frp': '10.08', 'daynight': 'D'}\n",
      "{'latitude': '66.11572', 'longitude': '152.64148', 'bright_ti4': '327.93', 'scan': '0.34', 'track': '0.56', 'acq_date': '2025-07-31', 'acq_time': '13', 'satellite': 'N', 'instrument': 'VIIRS', 'confidence': 'n', 'version': '2.0NRT', 'bright_ti5': '296.66', 'frp': '6.21', 'daynight': 'D'}\n",
      "\n",
      "---\n",
      "Example: Accessing a specific value from the first detection.\n",
      "Latitude of the first detection: 65.62944\n",
      "Confidence of the first detection: n\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Your VIIRS fire data as a multi-line string.\n",
    "# The StringIO object lets us treat this string like a file.\n",
    "fire_data_string = \"\"\"\n",
    "latitude,longitude,bright_ti4,scan,track,acq_date,acq_time,satellite,instrument,confidence,version,bright_ti5,frp,daynight\n",
    "65.62944,147.48901,333.06,0.45,0.63,2025-07-31,13,N,VIIRS,n,2.0NRT,298.21,12.32,D\n",
    "65.63281,147.49579,367,0.45,0.63,2025-07-31,13,N,VIIRS,h,2.0NRT,298.07,10.08,D\n",
    "65.63768,147.48698,352.09,0.45,0.63,2025-07-31,13,N,VIIRS,n,2.0NRT,297.46,10.08,D\n",
    "66.11572,152.64148,327.93,0.34,0.56,2025-07-31,13,N,VIIRS,n,2.0NRT,296.66,6.21,D\n",
    "\"\"\"\n",
    "\n",
    "# Create a file-like object from the string data\n",
    "csv_file = StringIO(fire_data_string.strip())\n",
    "\n",
    "# Use the csv.reader to safely parse the data\n",
    "# The reader handles all the complexities of CSV formatting for you.\n",
    "csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "# Get the header row to use for keys in our dictionaries\n",
    "headers = next(csv_reader)\n",
    "\n",
    "# Process each row and store it in a list of dictionaries\n",
    "fire_detections = []\n",
    "for row in csv_reader:\n",
    "    # Handle incomplete rows that might be at the end of the data\n",
    "    if len(row) == len(headers):\n",
    "        # Create a dictionary for each row for easy access\n",
    "        row_dict = dict(zip(headers, row))\n",
    "        fire_detections.append(row_dict)\n",
    "\n",
    "# Print the parsed data to show that it worked\n",
    "print(\"Parsed VIIRS Fire Detections:\")\n",
    "for detection in fire_detections:\n",
    "    print(detection)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"Example: Accessing a specific value from the first detection.\")\n",
    "if fire_detections:\n",
    "    first_detection = fire_detections[0]\n",
    "    print(f\"Latitude of the first detection: {first_detection['latitude']}\")\n",
    "    print(f\"Confidence of the first detection: {first_detection['confidence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bd816c-e7d0-4823-8c0d-e923e4f1781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch data from: https://firms.modaps.eosdis.nasa.gov/api/area/csv/2eaecfb3056b7b7751771485eb481c51/VIIRS_SNPP_NRT/world/1\n",
      "Please be patient, the request may take a few seconds...\n",
      "Success! Data received.\n",
      "Data saved successfully to firms_fire_data.csv\n",
      "\n",
      "--- Script finished ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# Replace this with the MAP_KEY you received in your email\n",
    "MAP_KEY = \"2eaecfb3056b7b7751771485eb481c51\"\n",
    "\n",
    "# The satellite sensor you want to use (VIIRS_SNPP_NRT is recommended)\n",
    "SOURCE = \"VIIRS_SNPP_NRT\"\n",
    "\n",
    "# The geographic area to search for fires (use 'world' for the whole globe)\n",
    "AREA_COORDINATES = \"world\"\n",
    "\n",
    "# The number of days of data to fetch (1-10 recommended to avoid rate limits)\n",
    "DAY_RANGE = 1\n",
    "\n",
    "# The name of the file to save the data to\n",
    "OUTPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# Construct the full API URL using the configuration variables\n",
    "api_url = f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/{MAP_KEY}/{SOURCE}/{AREA_COORDINATES}/{DAY_RANGE}\"\n",
    "\n",
    "print(f\"Attempting to fetch data from: {api_url}\")\n",
    "print(\"Please be patient, the request may take a few seconds...\")\n",
    "\n",
    "# Use a try-except block to handle potential network or request errors\n",
    "try:\n",
    "    # Send a GET request to the FIRMS API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    # Check the HTTP status code of the response\n",
    "    if response.status_code == 200:\n",
    "        # The request was successful!\n",
    "        print(\"Success! Data received.\")\n",
    "\n",
    "        # Save the content of the response to a local CSV file\n",
    "        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Data saved successfully to {OUTPUT_FILENAME}\")\n",
    "\n",
    "    elif response.status_code == 403:\n",
    "        # Status code 403 means Forbidden, usually an issue with the key\n",
    "        print(\"Error: The API key is invalid or permissions are incorrect.\")\n",
    "        print(\"Please double-check your MAP_KEY for any typos.\")\n",
    "        print(\"If the key is correct, you may need to request a new one.\")\n",
    "\n",
    "    elif response.status_code == 429:\n",
    "        # Status code 429 means too many requests, a rate-limiting issue\n",
    "        print(\"Error: Rate limit exceeded (429 Too Many Requests).\")\n",
    "        print(\"You have made too many requests in a short period.\")\n",
    "        print(\"Please wait for about 10 minutes and try again.\")\n",
    "        print(\"Consider reducing the DAY_RANGE to avoid this in the future.\")\n",
    "        \n",
    "    else:\n",
    "        # Handle any other unexpected status codes\n",
    "        print(f\"Error: Received an unexpected status code: {response.status_code}\")\n",
    "        print(\"Response content:\")\n",
    "        print(response.text)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Handle network-related errors (e.g., no internet connection)\n",
    "    print(f\"A network error occurred: {e}\")\n",
    "\n",
    "print(\"\\n--- Script finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc80a2df-5cc0-4000-b432-2d3feeb39207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'firms_fire_data.csv'...\n",
      "Successfully parsed 96380 fire detections.\n",
      "---\n",
      "First detection data:\n",
      "  latitude: 65.62944\n",
      "  longitude: 147.48901\n",
      "  bright_ti4: 333.06\n",
      "  scan: 0.45\n",
      "  track: 0.63\n",
      "  acq_date: 2025-07-31\n",
      "  acq_time: 13\n",
      "  satellite: N\n",
      "  instrument: VIIRS\n",
      "  confidence: n\n",
      "  version: 2.0NRT\n",
      "  bright_ti5: 298.21\n",
      "  frp: 12.32\n",
      "  daynight: D\n",
      "\n",
      "---\n",
      "Latitude of the first detection: 65.62944\n",
      "Longitude of the first detection: 147.48901\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# The name of the file you saved in the previous step\n",
    "INPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "if not os.path.exists(INPUT_FILENAME):\n",
    "    print(f\"Error: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    print(\"Please run the data retriever script first to create this file.\")\n",
    "else:\n",
    "    # Create an empty list to store our fire detection data\n",
    "    fire_detections = []\n",
    "\n",
    "    print(f\"Reading data from '{INPUT_FILENAME}'...\")\n",
    "\n",
    "    # Use a 'with' statement to ensure the file is closed correctly\n",
    "    with open(INPUT_FILENAME, 'r', encoding='utf-8') as csv_file:\n",
    "        # Use csv.DictReader to automatically parse each row into a dictionary\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        # Loop through each row in the file and add it to our list\n",
    "        for row in csv_reader:\n",
    "            fire_detections.append(row)\n",
    "\n",
    "    print(f\"Successfully parsed {len(fire_detections)} fire detections.\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # Let's see what the first detection looks like!\n",
    "    if fire_detections:\n",
    "        print(\"First detection data:\")\n",
    "        for key, value in fire_detections[0].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(\"\\n---\")\n",
    "        \n",
    "        # Now you can easily access any piece of data by its column name\n",
    "        first_lat = fire_detections[0]['latitude']\n",
    "        first_lon = fire_detections[0]['longitude']\n",
    "        print(f\"Latitude of the first detection: {first_lat}\")\n",
    "        print(f\"Longitude of the first detection: {first_lon}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"The file was empty or contained no data rows.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b0f769-5a2e-4fe9-9acb-46a6a25dbc1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# --- Configuration Section ---\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# The name of the CSV file created in the previous step\u001b[39;00m\n\u001b[0;32m      7\u001b[0m INPUT_FILENAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirms_fire_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import folium\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# The name of the CSV file created in the previous step\n",
    "INPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "# The name of the HTML file that will contain your map\n",
    "OUTPUT_MAP_FILE = \"fire_hotspots_map.html\"\n",
    "\n",
    "# --- Main Script ---\n",
    "# Check if the input file exists before trying to read it\n",
    "if not os.path.exists(INPUT_FILENAME):\n",
    "    print(f\"Error: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    print(\"Please make sure you have run the previous script to fetch the data.\")\n",
    "else:\n",
    "    # Use a try-except block to handle potential data parsing errors\n",
    "    try:\n",
    "        fire_detections = []\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as csv_file:\n",
    "            # We use DictReader again to get data as dictionaries\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            # Loop through each row and prepare the data for visualization\n",
    "            for row in csv_reader:\n",
    "                # We need to convert latitude and longitude from strings to numbers (floats)\n",
    "                # We also need to get the confidence and brightness\n",
    "                # Folium requires numbers for coordinates\n",
    "                try:\n",
    "                    lat = float(row['latitude'])\n",
    "                    lon = float(row['longitude'])\n",
    "                    confidence = row['confidence']\n",
    "                    bright_ti4 = float(row['bright_ti4'])\n",
    "                    \n",
    "                    # Store the processed data\n",
    "                    fire_detections.append({\n",
    "                        'lat': lat,\n",
    "                        'lon': lon,\n",
    "                        'confidence': confidence,\n",
    "                        'brightness': bright_ti4\n",
    "                    })\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    print(f\"Skipping a row due to a data error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Loaded {len(fire_detections)} fire detections from the CSV file.\")\n",
    "\n",
    "        # Create the base map object\n",
    "        # We start with a low zoom level to see the entire world\n",
    "        # The center is set to (0, 0) for a global view\n",
    "        world_map = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "        # Loop through our processed fire detections and add markers to the map\n",
    "        for detection in fire_detections:\n",
    "            # We use a CircleMarker for each fire hotspot\n",
    "            folium.CircleMarker(\n",
    "                location=[detection['lat'], detection['lon']],\n",
    "                radius=5,  # Size of the circle on the map\n",
    "                color='red', # All circles will be red\n",
    "                fill=True,\n",
    "                fill_color='red',\n",
    "                fill_opacity=0.7,\n",
    "                tooltip=f\"Confidence: {detection['confidence']}, Brightness: {detection['brightness']}\"\n",
    "            ).add_to(world_map)\n",
    "            \n",
    "        # Save the completed map as an HTML file\n",
    "        world_map.save(OUTPUT_MAP_FILE)\n",
    "        \n",
    "        print(f\"\\nSuccess! An interactive map has been saved to '{OUTPUT_MAP_FILE}'.\")\n",
    "        print(\"You can now open this file in your web browser to view the fire hotspots.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during map creation: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c719a1-d146-4cfd-9401-e17ce865add2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 96380 fire detections from the CSV file.\n",
      "\n",
      "Success! An interactive map has been saved to 'fire_hotspots_map.html'.\n",
      "You can now open this file in your web browser to view the fire hotspots.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import folium\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# The name of the CSV file created in the previous step\n",
    "INPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "# The name of the HTML file that will contain your map\n",
    "OUTPUT_MAP_FILE = \"fire_hotspots_map.html\"\n",
    "\n",
    "# --- Main Script ---\n",
    "# Check if the input file exists before trying to read it\n",
    "if not os.path.exists(INPUT_FILENAME):\n",
    "    print(f\"Error: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    print(\"Please make sure you have run the previous script to fetch the data.\")\n",
    "else:\n",
    "    # Use a try-except block to handle potential data parsing errors\n",
    "    try:\n",
    "        fire_detections = []\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as csv_file:\n",
    "            # We use DictReader again to get data as dictionaries\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            # Loop through each row and prepare the data for visualization\n",
    "            for row in csv_reader:\n",
    "                # We need to convert latitude and longitude from strings to numbers (floats)\n",
    "                # We also need to get the confidence and brightness\n",
    "                # Folium requires numbers for coordinates\n",
    "                try:\n",
    "                    lat = float(row['latitude'])\n",
    "                    lon = float(row['longitude'])\n",
    "                    confidence = row['confidence']\n",
    "                    bright_ti4 = float(row['bright_ti4'])\n",
    "                    \n",
    "                    # Store the processed data\n",
    "                    fire_detections.append({\n",
    "                        'lat': lat,\n",
    "                        'lon': lon,\n",
    "                        'confidence': confidence,\n",
    "                        'brightness': bright_ti4\n",
    "                    })\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    print(f\"Skipping a row due to a data error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Loaded {len(fire_detections)} fire detections from the CSV file.\")\n",
    "\n",
    "        # Create the base map object\n",
    "        # We start with a low zoom level to see the entire world\n",
    "        # The center is set to (0, 0) for a global view\n",
    "        world_map = folium.Map(location=[0, 0], zoom_start=2)\n",
    "\n",
    "        # Loop through our processed fire detections and add markers to the map\n",
    "        for detection in fire_detections:\n",
    "            # We use a CircleMarker for each fire hotspot\n",
    "            folium.CircleMarker(\n",
    "                location=[detection['lat'], detection['lon']],\n",
    "                radius=5,  # Size of the circle on the map\n",
    "                color='red', # All circles will be red\n",
    "                fill=True,\n",
    "                fill_color='red',\n",
    "                fill_opacity=0.7,\n",
    "                tooltip=f\"Confidence: {detection['confidence']}, Brightness: {detection['brightness']}\"\n",
    "            ).add_to(world_map)\n",
    "            \n",
    "        # Save the completed map as an HTML file\n",
    "        world_map.save(OUTPUT_MAP_FILE)\n",
    "        \n",
    "        print(f\"\\nSuccess! An interactive map has been saved to '{OUTPUT_MAP_FILE}'.\")\n",
    "        print(\"You can now open this file in your web browser to view the fire hotspots.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during map creation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2caab90-7cea-4de4-ae11-600c6650b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import folium\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# The name of the CSV file created in the previous step\n",
    "INPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "# The name of the HTML file that will contain your map\n",
    "OUTPUT_MAP_FILE = \"fire_hotspots_map.html\"\n",
    "\n",
    "# --- Main Script ---\n",
    "# Check if the input file exists before trying to read it\n",
    "if not os.path.exists(INPUT_FILENAME):\n",
    "    print(f\"Error: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    print(\"Please make sure you have run the previous script to fetch the data.\")\n",
    "else:\n",
    "    # Use a try-except block to handle potential data parsing errors\n",
    "    try:\n",
    "        fire_detections = []\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as csv_file:\n",
    "            # We use DictReader again to get data as dictionaries\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            # Loop through each row and prepare the data for visualization\n",
    "            for row in csv_reader:\n",
    "                # We need to convert latitude and longitude from strings to numbers (floats)\n",
    "                # We also need to get the confidence and brightness\n",
    "                # Folium requires numbers for coordinates\n",
    "                try:\n",
    "                    lat = float(row['latitude'])\n",
    "                    lon = float(row['longitude'])\n",
    "                    confidence = row['confidence']\n",
    "                    bright_ti4 = float(row['bright_ti4'])\n",
    "                    \n",
    "                    # Store the processed data\n",
    "                    fire_detections.append({\n",
    "                        'lat': lat,\n",
    "                        'lon': lon,\n",
    "                        'confidence': confidence,\n",
    "                        'brightness': bright_ti4\n",
    "                    })\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    # If there's an issue with a single row, print a warning and continue\n",
    "                    print(f\"Warning: Skipping a row due to a data error: {e}. Row: {row}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Loaded {len(fire_detections)} valid fire detections from the CSV file.\")\n",
    "\n",
    "        # If no valid detections were found, there's no point in creating a map\n",
    "        if not fire_detections:\n",
    "            print(\"Error: No valid fire detections were found in the CSV file.\")\n",
    "            print(\"The CSV may be empty or the data format is incorrect.\")\n",
    "        else:\n",
    "            # Create the base map object\n",
    "            # We start with a low zoom level to see the entire world\n",
    "            # The center is set to the first fire detection's coordinates\n",
    "            world_map = folium.Map(location=[fire_detections[0]['lat'], fire_detections[0]['lon']], zoom_start=2)\n",
    "\n",
    "            # Loop through our processed fire detections and add markers to the map\n",
    "            for detection in fire_detections:\n",
    "                # We use a CircleMarker for each fire hotspot\n",
    "                folium.CircleMarker(\n",
    "                    location=[detection['lat'], detection['lon']],\n",
    "                    radius=5,  # Size of the circle on the map\n",
    "                    color='red', # All circles will be red\n",
    "                    fill=True,\n",
    "                    fill_color='red',\n",
    "                    fill_opacity=0.7,\n",
    "                    tooltip=f\"Confidence: {detection['confidence']}, Brightness: {detection['brightness']}\"\n",
    "                ).add_to(world_map)\n",
    "            \n",
    "            # Save the completed map as an HTML file\n",
    "            world_map.save(OUTPUT_MAP_FILE)\n",
    "            \n",
    "            print(f\"\\nSuccess! An interactive map has been saved to '{OUTPUT_MAP_FILE}'.\")\n",
    "            print(\"You can now open this file in your web browser to view the fire hotspots.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during map creation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf92162c-431d-4f46-8341-ed6d0fa43b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 96380 valid fire detections from the CSV file.\n",
      "\n",
      "Success! An interactive map has been saved to 'fire_hotspots_map.html'.\n",
      "You can now open this file in your web browser to view the fire hotspots.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import folium\n",
    "\n",
    "# --- Configuration Section ---\n",
    "# The name of the CSV file created in the previous step\n",
    "INPUT_FILENAME = \"firms_fire_data.csv\"\n",
    "# The name of the HTML file that will contain your map\n",
    "OUTPUT_MAP_FILE = \"fire_hotspots_map.html\"\n",
    "\n",
    "# --- Main Script ---\n",
    "# Check if the input file exists before trying to read it\n",
    "if not os.path.exists(INPUT_FILENAME):\n",
    "    print(f\"Error: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    print(\"Please make sure you have run the previous script to fetch the data.\")\n",
    "else:\n",
    "    # Use a try-except block to handle potential data parsing errors\n",
    "    try:\n",
    "        fire_detections = []\n",
    "        with open(INPUT_FILENAME, 'r', encoding='utf-8') as csv_file:\n",
    "            # We use DictReader again to get data as dictionaries\n",
    "            csv_reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            # Loop through each row and prepare the data for visualization\n",
    "            for row in csv_reader:\n",
    "                # We need to convert latitude and longitude from strings to numbers (floats)\n",
    "                # We also need to get the confidence and brightness\n",
    "                # Folium requires numbers for coordinates\n",
    "                try:\n",
    "                    lat = float(row['latitude'])\n",
    "                    lon = float(row['longitude'])\n",
    "                    confidence = row['confidence']\n",
    "                    bright_ti4 = float(row['bright_ti4'])\n",
    "                    \n",
    "                    # Store the processed data\n",
    "                    fire_detections.append({\n",
    "                        'lat': lat,\n",
    "                        'lon': lon,\n",
    "                        'confidence': confidence,\n",
    "                        'brightness': bright_ti4\n",
    "                    })\n",
    "                except (ValueError, KeyError) as e:\n",
    "                    # If there's an issue with a single row, print a warning and continue\n",
    "                    print(f\"Warning: Skipping a row due to a data error: {e}. Row: {row}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Loaded {len(fire_detections)} valid fire detections from the CSV file.\")\n",
    "\n",
    "        # If no valid detections were found, there's no point in creating a map\n",
    "        if not fire_detections:\n",
    "            print(\"Error: No valid fire detections were found in the CSV file.\")\n",
    "            print(\"The CSV may be empty or the data format is incorrect.\")\n",
    "        else:\n",
    "            # Create the base map object\n",
    "            # We start with a low zoom level to see the entire world\n",
    "            # The center is set to the first fire detection's coordinates\n",
    "            world_map = folium.Map(location=[fire_detections[0]['lat'], fire_detections[0]['lon']], zoom_start=2)\n",
    "\n",
    "            # Loop through our processed fire detections and add markers to the map\n",
    "            for detection in fire_detections:\n",
    "                # We use a CircleMarker for each fire hotspot\n",
    "                folium.CircleMarker(\n",
    "                    location=[detection['lat'], detection['lon']],\n",
    "                    radius=5,  # Size of the circle on the map\n",
    "                    color='red', # All circles will be red\n",
    "                    fill=True,\n",
    "                    fill_color='red',\n",
    "                    fill_opacity=0.7,\n",
    "                    tooltip=f\"Confidence: {detection['confidence']}, Brightness: {detection['brightness']}\"\n",
    "                ).add_to(world_map)\n",
    "            \n",
    "            # Save the completed map as an HTML file\n",
    "            world_map.save(OUTPUT_MAP_FILE)\n",
    "            \n",
    "            print(f\"\\nSuccess! An interactive map has been saved to '{OUTPUT_MAP_FILE}'.\")\n",
    "            print(\"You can now open this file in your web browser to view the fire hotspots.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during map creation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368a2de0-c5ec-4c8b-a37d-b8eca79a5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'fire_detections.csv' was not found. Please make sure it's in the same directory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the minimum confidence level (0-100) to filter the data:  90\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input. Please enter a number.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# --- 3. Filter the DataFrame based on the user's input ---\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# The confidence column is often named 'confidence' or 'Confidence'. Let's check both.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m confidence_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[confidence_column] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_confidence]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Let's see how many detections we have after filtering\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# --- 1. Load the data from the CSV file ---\n",
    "try:\n",
    "    df = pd.read_csv('firms_fire_data.csv')\n",
    "    print(\"Successfully loaded fire detections from 'fire_detections.csv'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'fire_detections.csv' was not found. Please make sure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Get user input for the confidence filter ---\n",
    "# This loop ensures the user enters a valid number between 0 and 100\n",
    "while True:\n",
    "    try:\n",
    "        min_confidence = float(input(\"Enter the minimum confidence level (0-100) to filter the data: \"))\n",
    "        if 0 <= min_confidence <= 100:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please enter a number between 0 and 100.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# --- 3. Filter the DataFrame based on the user's input ---\n",
    "# The confidence column is often named 'confidence' or 'Confidence'. Let's check both.\n",
    "confidence_column = 'confidence' if 'confidence' in df.columns else 'Confidence'\n",
    "filtered_df = df[df[confidence_column] >= min_confidence]\n",
    "\n",
    "# Let's see how many detections we have after filtering\n",
    "print(f\"Filtered {len(filtered_df)} valid fire detections with confidence >= {min_confidence}.\")\n",
    "\n",
    "# --- 4. Create a new Folium map centered on the average location of the filtered fires ---\n",
    "# Create a map centered at the average latitude and longitude of the filtered data\n",
    "if not filtered_df.empty:\n",
    "    center_lat = filtered_df['latitude'].mean()\n",
    "    center_lon = filtered_df['longitude'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4, tiles='CartoDB dark_matter')\n",
    "    print(\"Created a new interactive map.\")\n",
    "else:\n",
    "    print(\"No detections found with the specified confidence level. Creating an empty map.\")\n",
    "    m = folium.Map(location=[0, 0], zoom_start=2, tiles='CartoDB dark_matter')\n",
    "\n",
    "\n",
    "# --- 5. Add a marker for each filtered fire detection ---\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Create a small, colored circle marker for each detection\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,  # Adjust the size of the marker\n",
    "        color='red', # Use a bold color to make them stand out\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        # Add a tooltip with details\n",
    "        tooltip=(\n",
    "            f\"Confidence: {row[confidence_column]:.2f}%<br>\"\n",
    "            f\"Latitude: {row['latitude']}<br>\"\n",
    "            f\"Longitude: {row['longitude']}\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- 6. Save the new map to an HTML file ---\n",
    "output_file = 'filtered_fire_hotspots_map.html'\n",
    "m.save(output_file)\n",
    "print(f\"Success! A new interactive map with filtered data has been saved to '{output_file}'.\")\n",
    "print(\"You can now open this file in your web browser to view the fire hotspots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340c730d-9a04-468a-aa1c-f76e5a5bf675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'fire_detections.csv' was not found. Please make sure it's in the same directory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the minimum confidence level (0-100) to filter the data:  50\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input. Please enter a number.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# --- 3. Filter the DataFrame based on the user's input ---\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# The confidence column is often named 'confidence' or 'Confidence'. Let's check both.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m confidence_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[confidence_column] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_confidence]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Let's see how many detections we have after filtering\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# --- 1. Load the data from the CSV file ---\n",
    "try:\n",
    "    df = pd.read_csv('fire_detections.csv')\n",
    "    print(\"Successfully loaded fire detections from 'fire_detections.csv'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'fire_detections.csv' was not found. Please make sure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Get user input for the confidence filter ---\n",
    "# This loop ensures the user enters a valid number between 0 and 100\n",
    "while True:\n",
    "    try:\n",
    "        min_confidence = float(input(\"Enter the minimum confidence level (0-100) to filter the data: \"))\n",
    "        if 0 <= min_confidence <= 100:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please enter a number between 0 and 100.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# --- 3. Filter the DataFrame based on the user's input ---\n",
    "# The confidence column is often named 'confidence' or 'Confidence'. Let's check both.\n",
    "confidence_column = 'confidence' if 'confidence' in df.columns else 'Confidence'\n",
    "filtered_df = df[df[confidence_column] >= min_confidence]\n",
    "\n",
    "# Let's see how many detections we have after filtering\n",
    "print(f\"Filtered {len(filtered_df)} valid fire detections with confidence >= {min_confidence}.\")\n",
    "\n",
    "# --- 4. Create a new Folium map centered on the average location of the filtered fires ---\n",
    "# Create a map centered at the average latitude and longitude of the filtered data\n",
    "if not filtered_df.empty:\n",
    "    center_lat = filtered_df['latitude'].mean()\n",
    "    center_lon = filtered_df['longitude'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4, tiles='CartoDB dark_matter')\n",
    "    print(\"Created a new interactive map.\")\n",
    "else:\n",
    "    print(\"No detections found with the specified confidence level. Creating an empty map.\")\n",
    "    m = folium.Map(location=[0, 0], zoom_start=2, tiles='CartoDB dark_matter')\n",
    "\n",
    "\n",
    "# --- 5. Add a marker for each filtered fire detection ---\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Create a small, colored circle marker for each detection\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,  # Adjust the size of the marker\n",
    "        color='red', # Use a bold color to make them stand out\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        # Add a tooltip with details\n",
    "        tooltip=(\n",
    "            f\"Confidence: {row[confidence_column]:.2f}%<br>\"\n",
    "            f\"Latitude: {row['latitude']}<br>\"\n",
    "            f\"Longitude: {row['longitude']}\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- 6. Save the new map to an HTML file ---\n",
    "output_file = 'filtered_fire_hotspots_map.html'\n",
    "m.save(output_file)\n",
    "print(f\"Success! A new interactive map with filtered data has been saved to '{output_file}'.\")\n",
    "print(\"You can now open this file in your web browser to view the fire hotspots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbe924-e454-4b3d-8ec8-6003e104345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# --- 1. Load the data from the CSV file ---\n",
    "try:\n",
    "    df = pd.read_csv('firms_fire_data.csv')\n",
    "    print(\"Successfully loaded fire detections from 'fire_detections.csv'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'fire_detections.csv' was not found. Please make sure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Get user input for the confidence filter ---\n",
    "# This loop ensures the user enters a valid number between 0 and 100\n",
    "while True:\n",
    "    try:\n",
    "        min_confidence = float(input(\"Enter the minimum confidence level (0-100) to filter the data: \"))\n",
    "        if 0 <= min_confidence <= 100:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please enter a number between 0 and 100.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "# --- 3. Filter the DataFrame based on the user's input ---\n",
    "# The confidence column is often named 'confidence' or 'Confidence'. Let's check both.\n",
    "confidence_column = 'confidence' if 'confidence' in df.columns else 'Confidence'\n",
    "filtered_df = df[df[confidence_column] >= min_confidence]\n",
    "\n",
    "# Let's see how many detections we have after filtering\n",
    "print(f\"Filtered {len(filtered_df)} valid fire detections with confidence >= {min_confidence}.\")\n",
    "\n",
    "# --- 4. Create a new Folium map centered on the average location of the filtered fires ---\n",
    "# Create a map centered at the average latitude and longitude of the filtered data\n",
    "if not filtered_df.empty:\n",
    "    center_lat = filtered_df['latitude'].mean()\n",
    "    center_lon = filtered_df['longitude'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=4, tiles='CartoDB dark_matter')\n",
    "    print(\"Created a new interactive map.\")\n",
    "else:\n",
    "    print(\"No detections found with the specified confidence level. Creating an empty map.\")\n",
    "    m = folium.Map(location=[0, 0], zoom_start=2, tiles='CartoDB dark_matter')\n",
    "\n",
    "\n",
    "# --- 5. Add a marker for each filtered fire detection ---\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Create a small, colored circle marker for each detection\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,  # Adjust the size of the marker\n",
    "        color='red', # Use a bold color to make them stand out\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.7,\n",
    "        # Add a tooltip with details\n",
    "        tooltip=(\n",
    "            f\"Confidence: {row[confidence_column]:.2f}%<br>\"\n",
    "            f\"Latitude: {row['latitude']}<br>\"\n",
    "            f\"Longitude: {row['longitude']}\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- 6. Save the new map to an HTML file ---\n",
    "output_file = 'filtered_fire_hotspots_map.html'\n",
    "m.save(output_file)\n",
    "print(f\"Success! A new interactive map with filtered data has been saved to '{output_file}'.\")\n",
    "print(\"You can now open this file in your web browser to view the fire hotspots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4739562-4a54-4c55-b6fe-ffb7bf542759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetching real fire data from NASA FIRMS...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acq_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\projects\\Foresight-for-Forests\\foresight_ai_env_py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acq_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m fire_data_raw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(io\u001b[38;5;241m.\u001b[39mStringIO(response\u001b[38;5;241m.\u001b[39mtext))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Filter by date to ensure we are within the specified window\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m fire_data_raw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macq_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mfire_data_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macq_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     51\u001b[0m fire_data \u001b[38;5;241m=\u001b[39m fire_data_raw[fire_data_raw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macq_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m END_DATE]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully fetched \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fire_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fire hotspots.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\projects\\Foresight-for-Forests\\foresight_ai_env_py310\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\projects\\Foresight-for-Forests\\foresight_ai_env_py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acq_date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Define the geographical area (Bounding Box for a part of California)\n",
    "# Format: [lon_min, lat_min, lon_max, lat_max]\n",
    "BOUNDING_BOX = [-123.0, 37.0, -120.0, 39.0]\n",
    "\n",
    "# 2. Define the time period\n",
    "START_DATE = \"2023-08-01\"\n",
    "END_DATE = \"2023-10-31\"\n",
    "\n",
    "# 3. Define the grid resolution (in degrees). 0.1 degrees is roughly 11km.\n",
    "GRID_RESOLUTION = 0.1\n",
    "\n",
    "# 4. NASA FIRMS API Key (You can get one, but for small requests it's not always needed)\n",
    "# Using the public NRT (Near Real Time) source for this example.\n",
    "# Map key for VIIRS S-NPP instrument.\n",
    "FIRMS_MAP_KEY = \"c1a20597933a6e9a667634351336a54d\" \n",
    "\n",
    "# --- Step 1: Fetch Real Fire Data from NASA FIRMS ---\n",
    "print(\"Step 1: Fetching real fire data from NASA FIRMS...\")\n",
    "\n",
    "# Construct the API URL\n",
    "# Source: VIIRS S-NPP (a satellite instrument good for fire detection)\n",
    "# Area: Bounding box, specified as W,S,E,N\n",
    "# Date Range:\n",
    "firms_url = (\n",
    "    f\"https://firms.modaps.eosdis.nasa.gov/api/area/csv/\"\n",
    "    f\"{FIRMS_MAP_KEY}/VIIRS_SNPP_NRT/\"\n",
    "    f\"{BOUNDING_BOX[0]},{BOUNDING_BOX[1]},{BOUNDING_BOX[2]},{BOUNDING_BOX[3]}/1/{START_DATE}\"\n",
    ")\n",
    "\n",
    "# FIRMS API requires the end date to be specified via a separate parameter in the request if it's different from start date\n",
    "# However, the simple daily API call gets data for the date specified up to the present.\n",
    "# So we will get all data from START_DATE to today and then filter it.\n",
    "# A more robust way for historical data is using their archive download tool, but for this deadline, the API is faster.\n",
    "\n",
    "try:\n",
    "    response = requests.get(firms_url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    \n",
    "    # Read the CSV data directly into a pandas DataFrame\n",
    "    fire_data_raw = pd.read_csv(io.StringIO(response.text))\n",
    "    \n",
    "    # Filter by date to ensure we are within the specified window\n",
    "    fire_data_raw['acq_date'] = pd.to_datetime(fire_data_raw['acq_date'])\n",
    "    fire_data = fire_data_raw[fire_data_raw['acq_date'] <= END_DATE].copy()\n",
    "    \n",
    "    print(f\"Successfully fetched {len(fire_data)} fire hotspots.\")\n",
    "    # Keep only essential columns\n",
    "    fire_data = fire_data[['latitude', 'longitude', 'acq_date']]\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data from FIRMS API: {e}\")\n",
    "    # Create an empty dataframe if the API fails, so the script can continue\n",
    "    fire_data = pd.DataFrame(columns=['latitude', 'longitude', 'acq_date'])\n",
    "\n",
    "# --- Step 2: Create the Geographical Grid ---\n",
    "print(\"\\nStep 2: Creating geographical grid...\")\n",
    "\n",
    "# Create arrays of longitude and latitude points\n",
    "lon_points = np.arange(BOUNDING_BOX[0], BOUNDING_BOX[2], GRID_RESOLUTION)\n",
    "lat_points = np.arange(BOUNDING_BOX[1], BOUNDING_BOX[3], GRID_RESOLUTION)\n",
    "\n",
    "# Create the grid of all combinations of lat/lon\n",
    "grid_lon, grid_lat = np.meshgrid(lon_points, lat_points)\n",
    "grid_df = pd.DataFrame({\n",
    "    'lon': grid_lon.ravel(),\n",
    "    'lat': grid_lat.ravel()\n",
    "})\n",
    "\n",
    "print(f\"Created a grid with {len(grid_df)} cells.\")\n",
    "\n",
    "# --- Step 3: Create a Master DataFrame with Simulated Data ---\n",
    "print(\"\\nStep 3: Creating master DataFrame and simulating weather/vegetation data...\")\n",
    "\n",
    "# Create a date range for our analysis\n",
    "dates = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
    "\n",
    "# Create the master DataFrame by crossing the grid with the dates\n",
    "master_df = pd.MultiIndex.from_product([grid_df.index, dates], names=['grid_index', 'date']).to_frame(index=False)\n",
    "master_df = master_df.merge(grid_df, left_on='grid_index', right_index=True).drop('grid_index', axis=1)\n",
    "\n",
    "# **SIMULATE DATA**\n",
    "# This is the part you'll replace with real data later.\n",
    "# For now, we generate realistic-looking random data.\n",
    "num_rows = len(master_df)\n",
    "\n",
    "# Simulate temperature (with some seasonality)\n",
    "days_since_start = (master_df['date'] - master_df['date'].min()).dt.days\n",
    "seasonality = np.sin(2 * np.pi * days_since_start / 90) * 5  # Simple sine wave for seasonal effect\n",
    "master_df['temperature_c'] = 25 + seasonality + np.random.uniform(-3, 3, num_rows)\n",
    "\n",
    "# Simulate humidity\n",
    "master_df['humidity_percent'] = 40 - seasonality * 2 + np.random.uniform(-10, 10, num_rows)\n",
    "master_df['humidity_percent'] = np.clip(master_df['humidity_percent'], 10, 90)\n",
    "\n",
    "# Simulate wind speed\n",
    "master_df['wind_speed_kmh'] = np.random.uniform(5, 30, num_rows)\n",
    "\n",
    "# Simulate Vegetation Dryness Index (NDVI) - lower values mean drier vegetation\n",
    "master_df['ndvi'] = 0.5 - (days_since_start / 90) * 0.1 + np.random.uniform(-0.05, 0.05, num_rows)\n",
    "master_df['ndvi'] = np.clip(master_df['ndvi'], 0.1, 0.8)\n",
    "\n",
    "print(\"Simulated data generated.\")\n",
    "\n",
    "# --- Step 4: Integrate the Real Fire Data onto the Grid ---\n",
    "print(\"\\nStep 4: Integrating real fire data onto the grid...\")\n",
    "\n",
    "# To do this efficiently, we \"digitize\" the fire coordinates.\n",
    "# This means we find which grid cell each fire incident belongs to.\n",
    "lon_bins = np.arange(BOUNDING_BOX[0], BOUNDING_BOX[2] + GRID_RESOLUTION, GRID_RESOLUTION)\n",
    "lat_bins = np.arange(BOUNDING_BOX[1], BOUNDING_BOX[3] + GRID_RESOLUTION, GRID_RESOLUTION)\n",
    "\n",
    "if not fire_data.empty:\n",
    "    # Find the index of the bin for each fire's lat/lon\n",
    "    fire_data['lon_bin_index'] = np.digitize(fire_data['longitude'], bins=lon_bins) - 1\n",
    "    fire_data['lat_bin_index'] = np.digitize(fire_data['latitude'], bins=lat_bins) - 1\n",
    "    \n",
    "    # Get the actual lon/lat of the grid cell\n",
    "    fire_data['lon'] = lon_points[fire_data['lon_bin_index']]\n",
    "    fire_data['lat'] = lat_points[fire_data['lat_bin_index']]\n",
    "\n",
    "    # Group by the grid cell and date to count the number of fire hotspots\n",
    "    fire_counts = fire_data.groupby(['lat', 'lon', 'acq_date']).size().reset_index(name='fire_count')\n",
    "    fire_counts.rename(columns={'acq_date': 'date'}, inplace=True)\n",
    "    \n",
    "    # Merge the fire counts into our master dataframe\n",
    "    master_df = pd.merge(master_df, fire_counts, on=['lat', 'lon', 'date'], how='left')\n",
    "    master_df['fire_count'].fillna(0, inplace=True)\n",
    "else:\n",
    "    # If no fire data was fetched, create the column with all zeros\n",
    "    master_df['fire_count'] = 0\n",
    "\n",
    "print(\"Fire data integrated.\")\n",
    "\n",
    "# --- Step 5: Engineer the Target Variable ---\n",
    "print(\"\\nStep 5: Engineering the target variable 'fire_in_next_7_days'...\")\n",
    "\n",
    "# Our target: a binary flag that is 1 if a fire occurred in the *next* 7 days for that grid cell.\n",
    "# This is what the model will try to predict.\n",
    "\n",
    "# First, create a simple 'fire_today' flag\n",
    "master_df['fire_today'] = (master_df['fire_count'] > 0).astype(int)\n",
    "\n",
    "# Sort the data by grid cell and then by date. This is crucial for the next step.\n",
    "master_df.sort_values(['lat', 'lon', 'date'], inplace=True)\n",
    "\n",
    "# Use a rolling window on future data.\n",
    "# We group by each grid cell ('lat', 'lon') and look ahead.\n",
    "# We shift the 'fire_today' signal backwards in time.\n",
    "# A rolling max over a window of 7 will be 1 if any value in that window is 1.\n",
    "master_df['fire_in_next_7_days'] = master_df.groupby(['lat', 'lon'])['fire_today'].transform(\n",
    "    lambda x: x.shift(-7).rolling(window=7, min_periods=1).max()\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "print(\"Target variable engineered.\")\n",
    "\n",
    "\n",
    "# --- Step 6: Final Cleanup and Save ---\n",
    "print(\"\\nStep 6: Cleaning up and saving the final preprocessed data...\")\n",
    "\n",
    "final_df = master_df.drop(columns=['fire_count', 'fire_today'])\n",
    "\n",
    "# The last 7 days for each grid cell will have NaNs for the target because we can't see into the future.\n",
    "# In a real scenario, you'd just drop these rows as they can't be used for training.\n",
    "final_df.dropna(subset=['fire_in_next_7_days'], inplace=True)\n",
    "\n",
    "# Final check on column types\n",
    "final_df = final_df.round(4) # Round floats for cleanliness\n",
    "\n",
    "# Save to a CSV file\n",
    "output_filename = 'preprocessed_wildfire_data.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Success! Preprocessed data saved to '{output_filename}'\")\n",
    "print(f\"The dataset has {len(final_df)} rows and {len(final_df.columns)} columns.\")\n",
    "print(\"Columns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample of the final data:\")\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46385269-060c-4b8d-80a1-585ef60c9811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing to fetch real fire data from NASA FIRMS...\n",
      "Successfully received data from NASA FIRMS.\n",
      "\n",
      "--- Debugging Info ---\n",
      "Columns available in the DataFrame:\n",
      "Index(['Invalid area. Expects: [west', 'south', 'east', 'north].'], dtype='object')\n",
      "\n",
      "First 5 rows of the data:\n",
      "Empty DataFrame\n",
      "Columns: [Invalid area. Expects: [west, south, east, north].]\n",
      "Index: []\n",
      "----------------------\n",
      "\n",
      "Processing data using the column name: 'acq_date'\n",
      "KeyError: 'acq_date'. One of the columns was not found.\n",
      "Please check the column names printed in the 'Debugging Info' section and correct them in the script.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Your Settings ---\n",
    "# I'll use a sample location and date range for this example.\n",
    "# Replace these with your actual area of interest.\n",
    "YOUR_API_KEY = '2eaecfb3056b7b7751771485eb481c51' # Get one here: https://firms.modaps.eosdis.nasa.gov/api/api_key/\n",
    "REGION = 'USA' # Or use a bounding box like '-125,30,-110,45' for the West Coast\n",
    "DAYS_OF_DATA = 30 # How many days of data to fetch\n",
    "\n",
    "# --- Code ---\n",
    "\n",
    "print(\"Step 1: Preparing to fetch real fire data from NASA FIRMS...\")\n",
    "\n",
    "# Calculate date range\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "START_DATE = (datetime.now() - timedelta(days=DAYS_OF_DATA)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Construct the URL for the MODIS instrument data in CSV format\n",
    "# Using MODIS C6.1 for this example. You can change to VIIRS if needed.\n",
    "url = (f'https://firms.modaps.eosdis.nasa.gov/api/area/csv/'\n",
    "       f'{YOUR_API_KEY}/MODIS_NRT/{REGION}/1/{START_DATE}')\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() # This will raise an error if the request failed (e.g., 404, 500)\n",
    "\n",
    "    # Check if the response is empty or contains an error message\n",
    "    if not response.text or \"Error\" in response.text:\n",
    "        print(\"Error: Received an empty response or an error message from the API.\")\n",
    "        print(\"API Response:\", response.text)\n",
    "        # Exit or handle the error appropriately\n",
    "        exit()\n",
    "\n",
    "    print(\"Successfully received data from NASA FIRMS.\")\n",
    "\n",
    "    # Use io.StringIO to read the text response as if it were a file\n",
    "    fire_data_raw = pd.read_csv(io.StringIO(response.text))\n",
    "\n",
    "    # --- DEBUGGING STEP ---\n",
    "    # Let's see what we actually got. This is the most important part.\n",
    "    print(\"\\n--- Debugging Info ---\")\n",
    "    print(\"Columns available in the DataFrame:\")\n",
    "    print(fire_data_raw.columns)\n",
    "    print(\"\\nFirst 5 rows of the data:\")\n",
    "    print(fire_data_raw.head())\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "\n",
    "    # --- YOUR CODE CONTINUES HERE ---\n",
    "    # Now, look at the output above and find the correct column name for the date.\n",
    "    # It is most likely 'acq_date', but if it's different, change it in the line below.\n",
    "    \n",
    "    # !! IMPORTANT !!\n",
    "    # !! CHANGE 'acq_date' BELOW IF THE DEBUG OUTPUT SHOWS A DIFFERENT NAME !!\n",
    "    CORRECT_DATE_COLUMN = 'acq_date' \n",
    "\n",
    "    print(f\"Processing data using the column name: '{CORRECT_DATE_COLUMN}'\")\n",
    "\n",
    "    # Convert the date column to datetime objects\n",
    "    fire_data_raw[CORRECT_DATE_COLUMN] = pd.to_datetime(fire_data_raw[CORRECT_DATE_COLUMN])\n",
    "\n",
    "    # Filter by date to ensure we are within the specified window\n",
    "    END_DATE_DT = pd.to_datetime(END_DATE)\n",
    "    fire_data = fire_data_raw[fire_data_raw[CORRECT_DATE_COLUMN] <= END_DATE_DT].copy()\n",
    "\n",
    "    # Select and rename columns for clarity\n",
    "    # Common columns are: latitude, longitude, brightness, frp (Fire Radiative Power)\n",
    "    # Check your debug output for the exact names.\n",
    "    fire_data_processed = fire_data[['latitude', 'longitude', CORRECT_DATE_COLUMN, 'brightness', 'frp']].copy()\n",
    "    fire_data_processed.rename(columns={CORRECT_DATE_COLUMN: 'date'}, inplace=True)\n",
    "\n",
    "\n",
    "    print(f\"Successfully processed {len(fire_data_processed)} fire hotspots.\")\n",
    "    print(\"Final preprocessed data sample:\")\n",
    "    print(fire_data_processed.head())\n",
    "\n",
    "    # You can now save this to a CSV for your guide\n",
    "    # fire_data_processed.to_csv('preprocessed_fire_data.csv', index=False)\n",
    "    # print(\"\\nPreprocessed data saved to 'preprocessed_fire_data.csv'\")\n",
    "\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "    print(\"Please check your API key, region, and the date format.\")\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}. One of the columns was not found.\")\n",
    "    print(\"Please check the column names printed in the 'Debugging Info' section and correct them in the script.\")\n",
    "except Exception as err:\n",
    "    print(f\"An other error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511ea14-ed4c-4be9-bf77-aaf7c59be605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
